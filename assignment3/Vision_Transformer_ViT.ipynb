{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Attentional Networks in Computer Vision\n",
    "Prepared by comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
    "\n",
    "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
    "\n",
    "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_ENABLE_MPS_FALLBACK=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=False,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=False,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=False, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Barebones Transformers: Self-Attentional Layer\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
    "\n",
    "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
    "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
    "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
    "\n",
    "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
    "\n",
    "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
    "\n",
    "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
    "\n",
    "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
    "\n",
    "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
    "\n",
    "5. Reassemble heads into one flat vector and return the output.\n",
    "\n",
    "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        ## initialize module's instance variables\n",
    "        self.input_dims = input_dims\n",
    "        self.head_dims = head_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dims = head_dims * num_heads\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Input of shape, [B, N, D] where:\n",
    "        ## - B denotes the batch size\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D corresponds to model dimensionality\n",
    "        b,n,d = x.shape\n",
    "        \n",
    "        ## Construct queries,keys,values\n",
    "        q_ = self.W_Q(x)\n",
    "        k_ = self.W_K(x)\n",
    "        v_ = self.W_V(x)\n",
    "        \n",
    "        ## Seperate q,k,v into their corresponding heads,\n",
    "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
    "        ## - B denotes the batch size\n",
    "        ## - H denotes number of heads\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D//H corresponds to per head dimensionality\n",
    "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
    "       \n",
    "        #########################################################################################\n",
    "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
    "        #########################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        ## Compute attention logits. Note that this operation is conducted as a\n",
    "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
    "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
    "        ## Output Attention logits should have the size: [B,H,N,N]\n",
    "        \n",
    "        alignment_scores = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(self.head_dims)\n",
    "    \n",
    "        ## Compute attention Weights. Note that this operation is conducted as a\n",
    "        ## Softmax Normalization across the keys dimension. \n",
    "        ## Hint: You can apply the Softmax operation across the final dimension\n",
    "\n",
    "        attention_scores = F.softmax(alignment_scores, dim=2)\n",
    "        \n",
    "        ## Compute output values. Note that this operation is conducted as a \n",
    "        ## batched matrix multiplication between the Attention Weights matrix and \n",
    "        ## the values tensor. After computing output values, the output should be reshaped\n",
    "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H]\n",
    "        ## Output should be of size [B, N, D]\n",
    "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
    "        \n",
    "        attn_out = torch.matmul(attention_scores, v)\n",
    "        attn_out = attn_out.permute(0,2,1,3)\n",
    "        attn_out = torch.reshape(attn_out,(b,n,self.proj_dims))\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             \n",
    "        ################################################################################\n",
    "    \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "def test_self_attn_layer():\n",
    "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
    "    layer = SelfAttention(32,64,4)\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,256]\n",
    "test_self_attn_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Barebones Transformers: Transformer Encoder Block\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
    "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = F.relu(self.fc_1(x))\n",
    "        o = self.fc_2(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
    "## module follows a simple computational pipeline:\n",
    "## input --> layernorm --> SelfAttention --> skip connection \n",
    "##       --> layernorm --> MLP ---> skip connection ---> output\n",
    "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
    "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
    "## to the SelfAttention block.\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "###############################################################\n",
    "# TODO: Complete the consturctor of  TransformerBlock module  #\n",
    "###############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "                \n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dims = self.hidden_dims // self.num_heads\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(self.hidden_dims) \n",
    "        self.attention = SelfAttention(self.hidden_dims,head_dims=self.head_dims,num_heads=self.num_heads)\n",
    "        self.norm2 = nn.LayerNorm(self.hidden_dims)\n",
    "        self.mlp = MLP(hidden_dims, hidden_dims, hidden_dims)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "##############################################################\n",
    "# TODO: Complete the forward of TransformerBlock module      #\n",
    "##############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        norm1_out = self.norm1(x)\n",
    "        attention_out = self.attention(norm1_out) + x\n",
    "        norm2_out = self.norm2(attention_out)\n",
    "        mlp_out = self.mlp(norm2_out)\n",
    "        out = mlp_out + attention_out\n",
    "\n",
    "        return out\n",
    "        \n",
    " # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_transfomerblock_layer():\n",
    "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
    "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
    "    out = layer(x)\n",
    "    print(out.size()) \n",
    "test_transfomerblock_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV The Vision Transformer (ViT)\n",
    "\n",
    "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
    "        super(ViT, self).__init__()\n",
    "                \n",
    "        ## initialize module's instance variables\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_trans_layers = num_trans_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.image_k = image_k\n",
    "        self.patch_k = patch_k\n",
    "        \n",
    "        self.image_height = self.image_width = image_k\n",
    "        self.patch_height = self.patch_width = patch_k\n",
    "        \n",
    "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
    "                'Image size must be divisible by the patch size.'\n",
    "\n",
    "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        self.patch_flat_len = self.patch_height * self.patch_width\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        \n",
    "        ## ViT's flattened patch embedding projection:\n",
    "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
    "        \n",
    "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
    "        \n",
    "        ## Learnable classt token and its index among attention sequence elements.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
    "        self.cls_index = torch.LongTensor([0])\n",
    "        \n",
    "        ## Declare cascaded Transformer blocks:\n",
    "        transformer_encoder_list = []\n",
    "        for _ in range(self.num_trans_layers):\n",
    "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
    "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
    "        \n",
    "        ## Declare the output mlp:\n",
    "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
    "         \n",
    "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
    "        ## Create sliding window pathes using nn.Functional.unfold\n",
    "        ## Input dimensions: [B,D,H,W] where\n",
    "        ## --B : input batch size\n",
    "        ## --D : input channels\n",
    "        ## --H, W: input height and width\n",
    "        ## Output dimensions: [B,N,H*W,D]\n",
    "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
    "        ##      sliding window stride and padding.\n",
    "        b,d,h,w = x.shape\n",
    "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
    "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
    "        n = x_unf.size(1)\n",
    "        return x_unf,n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        ## create sliding window patches from the input image\n",
    "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
    "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
    "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
    "        ## linearly embed each flattened patch\n",
    "        x_embed = self.linear_embed(x_patch_flat)\n",
    "        \n",
    "        ## retrieve class token \n",
    "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
    "        ## concatanate class token to input patches\n",
    "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
    "        \n",
    "        ## add positional embedding to input patches + class token \n",
    "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
    "        \n",
    "        ## pass through the transformer encoder\n",
    "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
    "        \n",
    "        ## select the class token \n",
    "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
    "        \n",
    "        ## create output\n",
    "        out = self.out_mlp(out_cls_token)\n",
    "        \n",
    "        return out.squeeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def test_vit():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
    "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
    "    out = model(x)\n",
    "    print(out.size()) \n",
    "test_vit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. Train the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Accuracy\n",
    "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
    "\n",
    "The check_batch_accuracy function is provided for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def check_batch_accuracy(out, target,eps=1e-7):\n",
    "    b, c = out.shape\n",
    "    with torch.no_grad():\n",
    "        _, pred = out.max(-1) \n",
    "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
    "    return correct, float(correct) / (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer, trainloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall training accuracy for the epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    network.train()  # put model to training mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
    "            \n",
    "        outputs = network(inputs)\n",
    "        loss =  F.cross_entropy(outputs, targets)\n",
    "            \n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "            \n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.detach()\n",
    "        train_loss += loss.item()\n",
    "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
    "        correct += correct_p\n",
    "        total += targets.size(0)\n",
    "\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Loop\n",
    "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, evalloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall evaluation accuracy for the epoch\n",
    "    \"\"\"\n",
    "    network.eval() # put model to evaluation mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('\\n---- Evaluation in process ----')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
    "            outputs = network(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
    "            correct += correct_p\n",
    "            total += targets.size(0)\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit a ViT\n",
    "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
    "\n",
    "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
    "\n",
    "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
    "\n",
    "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
      "==> Data ready, batchsize = 25\n"
     ]
    }
   ],
   "source": [
    "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
    "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
    "\n",
    "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
    "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
    "\n",
    "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
    "\n",
    "batch_size_sub = 25\n",
    "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
    "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
    "\n",
    "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 3.518 | Acc: 16.000% (4/25)\n",
      "Loss: 6.079 | Acc: 10.000% (5/50)\n",
      "Loss: 6.755 | Acc: 13.333% (10/75)\n",
      "Loss: 6.064 | Acc: 13.000% (13/100)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 13.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.575 | Acc: 12.000% (3/25)\n",
      "Loss: 2.899 | Acc: 12.000% (6/50)\n",
      "Loss: 3.174 | Acc: 9.333% (7/75)\n",
      "Loss: 3.084 | Acc: 10.000% (10/100)\n",
      "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 10.0\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 2.655 | Acc: 12.000% (3/25)\n",
      "Loss: 3.430 | Acc: 14.000% (7/50)\n",
      "Loss: 3.006 | Acc: 18.667% (14/75)\n",
      "Loss: 2.880 | Acc: 20.000% (20/100)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 20.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.160 | Acc: 4.000% (1/25)\n",
      "Loss: 2.802 | Acc: 16.000% (8/50)\n",
      "Loss: 2.783 | Acc: 14.667% (11/75)\n",
      "Loss: 2.762 | Acc: 16.000% (16/100)\n",
      "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 16.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 2.731 | Acc: 12.000% (3/25)\n",
      "Loss: 2.467 | Acc: 18.000% (9/50)\n",
      "Loss: 2.440 | Acc: 16.000% (12/75)\n",
      "Loss: 2.368 | Acc: 17.000% (17/100)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 17.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.121 | Acc: 24.000% (6/25)\n",
      "Loss: 2.376 | Acc: 14.000% (7/50)\n",
      "Loss: 2.462 | Acc: 16.000% (12/75)\n",
      "Loss: 2.454 | Acc: 15.000% (15/100)\n",
      "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 15.0\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 1.976 | Acc: 36.000% (9/25)\n",
      "Loss: 2.228 | Acc: 20.000% (10/50)\n",
      "Loss: 2.239 | Acc: 18.667% (14/75)\n",
      "Loss: 2.220 | Acc: 22.000% (22/100)\n",
      "Epoch 3 of training is completed, Training accuracy for this epoch is 22.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.370 | Acc: 20.000% (5/25)\n",
      "Loss: 2.501 | Acc: 16.000% (8/50)\n",
      "Loss: 2.634 | Acc: 13.333% (10/75)\n",
      "Loss: 2.659 | Acc: 15.000% (15/100)\n",
      "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 15.0\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 2.273 | Acc: 20.000% (5/25)\n",
      "Loss: 2.004 | Acc: 32.000% (16/50)\n",
      "Loss: 1.979 | Acc: 29.333% (22/75)\n",
      "Loss: 2.006 | Acc: 27.000% (27/100)\n",
      "Epoch 4 of training is completed, Training accuracy for this epoch is 27.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.139 | Acc: 16.000% (4/25)\n",
      "Loss: 2.080 | Acc: 22.000% (11/50)\n",
      "Loss: 2.169 | Acc: 20.000% (15/75)\n",
      "Loss: 2.186 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 2.035 | Acc: 20.000% (5/25)\n",
      "Loss: 1.923 | Acc: 20.000% (10/50)\n",
      "Loss: 1.921 | Acc: 18.667% (14/75)\n",
      "Loss: 1.913 | Acc: 20.000% (20/100)\n",
      "Epoch 5 of training is completed, Training accuracy for this epoch is 20.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.254 | Acc: 8.000% (2/25)\n",
      "Loss: 2.314 | Acc: 8.000% (4/50)\n",
      "Loss: 2.358 | Acc: 10.667% (8/75)\n",
      "Loss: 2.314 | Acc: 11.000% (11/100)\n",
      "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 11.0\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 1.579 | Acc: 40.000% (10/25)\n",
      "Loss: 1.581 | Acc: 38.000% (19/50)\n",
      "Loss: 1.680 | Acc: 34.667% (26/75)\n",
      "Loss: 1.659 | Acc: 37.000% (37/100)\n",
      "Epoch 6 of training is completed, Training accuracy for this epoch is 37.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.390 | Acc: 20.000% (5/25)\n",
      "Loss: 2.370 | Acc: 18.000% (9/50)\n",
      "Loss: 2.437 | Acc: 14.667% (11/75)\n",
      "Loss: 2.458 | Acc: 15.000% (15/100)\n",
      "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 15.0\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 1.538 | Acc: 40.000% (10/25)\n",
      "Loss: 1.640 | Acc: 36.000% (18/50)\n",
      "Loss: 1.556 | Acc: 40.000% (30/75)\n",
      "Loss: 1.642 | Acc: 37.000% (37/100)\n",
      "Epoch 7 of training is completed, Training accuracy for this epoch is 37.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.268 | Acc: 20.000% (5/25)\n",
      "Loss: 2.306 | Acc: 16.000% (8/50)\n",
      "Loss: 2.404 | Acc: 17.333% (13/75)\n",
      "Loss: 2.345 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 1.524 | Acc: 48.000% (12/25)\n",
      "Loss: 1.491 | Acc: 52.000% (26/50)\n",
      "Loss: 1.386 | Acc: 52.000% (39/75)\n",
      "Loss: 1.392 | Acc: 52.000% (52/100)\n",
      "Epoch 8 of training is completed, Training accuracy for this epoch is 52.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.843 | Acc: 12.000% (3/25)\n",
      "Loss: 2.794 | Acc: 12.000% (6/50)\n",
      "Loss: 2.816 | Acc: 10.667% (8/75)\n",
      "Loss: 2.797 | Acc: 11.000% (11/100)\n",
      "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 11.0\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 1.320 | Acc: 56.000% (14/25)\n",
      "Loss: 1.275 | Acc: 56.000% (28/50)\n",
      "Loss: 1.189 | Acc: 58.667% (44/75)\n",
      "Loss: 1.212 | Acc: 59.000% (59/100)\n",
      "Epoch 9 of training is completed, Training accuracy for this epoch is 59.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.023 | Acc: 20.000% (5/25)\n",
      "Loss: 2.197 | Acc: 18.000% (9/50)\n",
      "Loss: 2.366 | Acc: 14.667% (11/75)\n",
      "Loss: 2.379 | Acc: 16.000% (16/100)\n",
      "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 16.0\n",
      "\n",
      "Epoch: 10\n",
      "Loss: 1.006 | Acc: 64.000% (16/25)\n",
      "Loss: 1.034 | Acc: 60.000% (30/50)\n",
      "Loss: 1.027 | Acc: 60.000% (45/75)\n",
      "Loss: 1.046 | Acc: 60.000% (60/100)\n",
      "Epoch 10 of training is completed, Training accuracy for this epoch is 60.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.462 | Acc: 20.000% (5/25)\n",
      "Loss: 2.711 | Acc: 16.000% (8/50)\n",
      "Loss: 2.798 | Acc: 17.333% (13/75)\n",
      "Loss: 2.726 | Acc: 18.000% (18/100)\n",
      "Evaluation of Epoch 10 is completed, Validation accuracy for this epoch is 18.0\n",
      "\n",
      "Epoch: 11\n",
      "Loss: 0.819 | Acc: 68.000% (17/25)\n",
      "Loss: 0.733 | Acc: 76.000% (38/50)\n",
      "Loss: 0.837 | Acc: 73.333% (55/75)\n",
      "Loss: 0.826 | Acc: 75.000% (75/100)\n",
      "Epoch 11 of training is completed, Training accuracy for this epoch is 75.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.236 | Acc: 24.000% (6/25)\n",
      "Loss: 2.522 | Acc: 18.000% (9/50)\n",
      "Loss: 2.772 | Acc: 17.333% (13/75)\n",
      "Loss: 2.773 | Acc: 17.000% (17/100)\n",
      "Evaluation of Epoch 11 is completed, Validation accuracy for this epoch is 17.0\n",
      "\n",
      "Epoch: 12\n",
      "Loss: 0.811 | Acc: 80.000% (20/25)\n",
      "Loss: 0.765 | Acc: 78.000% (39/50)\n",
      "Loss: 0.745 | Acc: 80.000% (60/75)\n",
      "Loss: 0.702 | Acc: 81.000% (81/100)\n",
      "Epoch 12 of training is completed, Training accuracy for this epoch is 81.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.821 | Acc: 20.000% (5/25)\n",
      "Loss: 2.882 | Acc: 20.000% (10/50)\n",
      "Loss: 3.112 | Acc: 20.000% (15/75)\n",
      "Loss: 3.032 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 12 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Epoch: 13\n",
      "Loss: 0.720 | Acc: 80.000% (20/25)\n",
      "Loss: 0.603 | Acc: 84.000% (42/50)\n",
      "Loss: 0.568 | Acc: 80.000% (60/75)\n",
      "Loss: 0.581 | Acc: 79.000% (79/100)\n",
      "Epoch 13 of training is completed, Training accuracy for this epoch is 79.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.702 | Acc: 20.000% (5/25)\n",
      "Loss: 2.866 | Acc: 14.000% (7/50)\n",
      "Loss: 3.209 | Acc: 16.000% (12/75)\n",
      "Loss: 3.127 | Acc: 16.000% (16/100)\n",
      "Evaluation of Epoch 13 is completed, Validation accuracy for this epoch is 16.0\n",
      "\n",
      "Epoch: 14\n",
      "Loss: 0.453 | Acc: 84.000% (21/25)\n",
      "Loss: 0.424 | Acc: 82.000% (41/50)\n",
      "Loss: 0.384 | Acc: 86.667% (65/75)\n",
      "Loss: 0.421 | Acc: 84.000% (84/100)\n",
      "Epoch 14 of training is completed, Training accuracy for this epoch is 84.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.017 | Acc: 20.000% (5/25)\n",
      "Loss: 3.170 | Acc: 18.000% (9/50)\n",
      "Loss: 3.541 | Acc: 17.333% (13/75)\n",
      "Loss: 3.469 | Acc: 16.000% (16/100)\n",
      "Evaluation of Epoch 14 is completed, Validation accuracy for this epoch is 16.0\n",
      "\n",
      "Final train set accuracy is 84.0\n",
      "Final val set accuracy is 16.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "regularization_val = 1e-6\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "network= ViT(hidden_dims,input_dims,output_dims, num_trans_layers,num_heads,image_k,patch_k)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay=regularization_val)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "eval_accs=[]\n",
    "for epoch in range(15):\n",
    "    tr_acc = train(network, optimizer, trainloader_sub)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    eval_acc = evaluate(network, valloader_sub)\n",
    "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
    "              .format(epoch, eval_acc))  \n",
    "    tr_accs.append(tr_acc)\n",
    "    eval_accs.append(eval_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 3.630 | Acc: 4.688% (3/64)\n",
      "Loss: 4.320 | Acc: 6.250% (8/128)\n",
      "Loss: 4.282 | Acc: 6.250% (12/192)\n",
      "Loss: 4.032 | Acc: 7.422% (19/256)\n",
      "Loss: 3.826 | Acc: 8.750% (28/320)\n",
      "Loss: 3.685 | Acc: 10.417% (40/384)\n",
      "Loss: 3.547 | Acc: 11.384% (51/448)\n",
      "Loss: 3.439 | Acc: 11.328% (58/512)\n",
      "Loss: 3.334 | Acc: 12.153% (70/576)\n",
      "Loss: 3.242 | Acc: 12.188% (78/640)\n",
      "Loss: 3.182 | Acc: 12.074% (85/704)\n",
      "Loss: 3.107 | Acc: 12.240% (94/768)\n",
      "Loss: 3.065 | Acc: 12.260% (102/832)\n",
      "Loss: 3.023 | Acc: 12.612% (113/896)\n",
      "Loss: 2.966 | Acc: 13.125% (126/960)\n",
      "Loss: 2.922 | Acc: 13.379% (137/1024)\n",
      "Loss: 2.889 | Acc: 13.143% (143/1088)\n",
      "Loss: 2.851 | Acc: 13.628% (157/1152)\n",
      "Loss: 2.814 | Acc: 14.062% (171/1216)\n",
      "Loss: 2.785 | Acc: 14.297% (183/1280)\n",
      "Loss: 2.771 | Acc: 14.509% (195/1344)\n",
      "Loss: 2.746 | Acc: 15.128% (213/1408)\n",
      "Loss: 2.734 | Acc: 15.217% (224/1472)\n",
      "Loss: 2.710 | Acc: 15.690% (241/1536)\n",
      "Loss: 2.695 | Acc: 15.625% (250/1600)\n",
      "Loss: 2.677 | Acc: 15.685% (261/1664)\n",
      "Loss: 2.662 | Acc: 15.741% (272/1728)\n",
      "Loss: 2.648 | Acc: 15.904% (285/1792)\n",
      "Loss: 2.628 | Acc: 15.787% (293/1856)\n",
      "Loss: 2.606 | Acc: 16.198% (311/1920)\n",
      "Loss: 2.589 | Acc: 16.381% (325/1984)\n",
      "Loss: 2.575 | Acc: 16.650% (341/2048)\n",
      "Loss: 2.562 | Acc: 16.809% (355/2112)\n",
      "Loss: 2.546 | Acc: 17.050% (371/2176)\n",
      "Loss: 2.536 | Acc: 17.143% (384/2240)\n",
      "Loss: 2.523 | Acc: 17.144% (395/2304)\n",
      "Loss: 2.511 | Acc: 17.061% (404/2368)\n",
      "Loss: 2.502 | Acc: 17.229% (419/2432)\n",
      "Loss: 2.494 | Acc: 17.188% (429/2496)\n",
      "Loss: 2.486 | Acc: 17.305% (443/2560)\n",
      "Loss: 2.477 | Acc: 17.569% (461/2624)\n",
      "Loss: 2.467 | Acc: 17.783% (478/2688)\n",
      "Loss: 2.460 | Acc: 17.842% (491/2752)\n",
      "Loss: 2.453 | Acc: 17.969% (506/2816)\n",
      "Loss: 2.441 | Acc: 18.056% (520/2880)\n",
      "Loss: 2.435 | Acc: 18.105% (533/2944)\n",
      "Loss: 2.427 | Acc: 18.218% (548/3008)\n",
      "Loss: 2.422 | Acc: 18.262% (561/3072)\n",
      "Loss: 2.413 | Acc: 18.559% (582/3136)\n",
      "Loss: 2.406 | Acc: 18.625% (596/3200)\n",
      "Loss: 2.399 | Acc: 18.658% (609/3264)\n",
      "Loss: 2.393 | Acc: 18.630% (620/3328)\n",
      "Loss: 2.387 | Acc: 18.662% (633/3392)\n",
      "Loss: 2.382 | Acc: 18.721% (647/3456)\n",
      "Loss: 2.374 | Acc: 19.006% (669/3520)\n",
      "Loss: 2.367 | Acc: 19.001% (681/3584)\n",
      "Loss: 2.362 | Acc: 19.134% (698/3648)\n",
      "Loss: 2.358 | Acc: 19.208% (713/3712)\n",
      "Loss: 2.351 | Acc: 19.359% (731/3776)\n",
      "Loss: 2.347 | Acc: 19.401% (745/3840)\n",
      "Loss: 2.339 | Acc: 19.621% (766/3904)\n",
      "Loss: 2.337 | Acc: 19.582% (777/3968)\n",
      "Loss: 2.329 | Acc: 19.792% (798/4032)\n",
      "Loss: 2.324 | Acc: 19.751% (809/4096)\n",
      "Loss: 2.322 | Acc: 19.639% (817/4160)\n",
      "Loss: 2.320 | Acc: 19.531% (825/4224)\n",
      "Loss: 2.315 | Acc: 19.543% (838/4288)\n",
      "Loss: 2.312 | Acc: 19.485% (848/4352)\n",
      "Loss: 2.310 | Acc: 19.452% (859/4416)\n",
      "Loss: 2.305 | Acc: 19.554% (876/4480)\n",
      "Loss: 2.304 | Acc: 19.520% (887/4544)\n",
      "Loss: 2.301 | Acc: 19.661% (906/4608)\n",
      "Loss: 2.297 | Acc: 19.713% (921/4672)\n",
      "Loss: 2.296 | Acc: 19.700% (933/4736)\n",
      "Loss: 2.293 | Acc: 19.729% (947/4800)\n",
      "Loss: 2.290 | Acc: 19.881% (967/4864)\n",
      "Loss: 2.287 | Acc: 19.846% (978/4928)\n",
      "Loss: 2.282 | Acc: 19.992% (998/4992)\n",
      "Loss: 2.278 | Acc: 20.036% (1013/5056)\n",
      "Loss: 2.275 | Acc: 20.039% (1026/5120)\n",
      "Loss: 2.272 | Acc: 20.120% (1043/5184)\n",
      "Loss: 2.268 | Acc: 20.198% (1060/5248)\n",
      "Loss: 2.264 | Acc: 20.312% (1079/5312)\n",
      "Loss: 2.259 | Acc: 20.387% (1096/5376)\n",
      "Loss: 2.255 | Acc: 20.625% (1122/5440)\n",
      "Loss: 2.253 | Acc: 20.567% (1132/5504)\n",
      "Loss: 2.251 | Acc: 20.564% (1145/5568)\n",
      "Loss: 2.247 | Acc: 20.614% (1161/5632)\n",
      "Loss: 2.244 | Acc: 20.716% (1180/5696)\n",
      "Loss: 2.240 | Acc: 20.781% (1197/5760)\n",
      "Loss: 2.237 | Acc: 20.913% (1218/5824)\n",
      "Loss: 2.235 | Acc: 21.077% (1241/5888)\n",
      "Loss: 2.234 | Acc: 21.035% (1252/5952)\n",
      "Loss: 2.231 | Acc: 21.177% (1274/6016)\n",
      "Loss: 2.229 | Acc: 21.250% (1292/6080)\n",
      "Loss: 2.227 | Acc: 21.289% (1308/6144)\n",
      "Loss: 2.224 | Acc: 21.392% (1328/6208)\n",
      "Loss: 2.220 | Acc: 21.476% (1347/6272)\n",
      "Loss: 2.217 | Acc: 21.512% (1363/6336)\n",
      "Loss: 2.215 | Acc: 21.609% (1383/6400)\n",
      "Loss: 2.213 | Acc: 21.612% (1397/6464)\n",
      "Loss: 2.210 | Acc: 21.737% (1419/6528)\n",
      "Loss: 2.206 | Acc: 21.860% (1441/6592)\n",
      "Loss: 2.204 | Acc: 21.845% (1454/6656)\n",
      "Loss: 2.205 | Acc: 21.860% (1469/6720)\n",
      "Loss: 2.202 | Acc: 21.978% (1491/6784)\n",
      "Loss: 2.200 | Acc: 22.050% (1510/6848)\n",
      "Loss: 2.196 | Acc: 22.179% (1533/6912)\n",
      "Loss: 2.194 | Acc: 22.219% (1550/6976)\n",
      "Loss: 2.190 | Acc: 22.344% (1573/7040)\n",
      "Loss: 2.190 | Acc: 22.340% (1587/7104)\n",
      "Loss: 2.192 | Acc: 22.294% (1598/7168)\n",
      "Loss: 2.188 | Acc: 22.442% (1623/7232)\n",
      "Loss: 2.185 | Acc: 22.505% (1642/7296)\n",
      "Loss: 2.186 | Acc: 22.527% (1658/7360)\n",
      "Loss: 2.184 | Acc: 22.589% (1677/7424)\n",
      "Loss: 2.181 | Acc: 22.610% (1693/7488)\n",
      "Loss: 2.180 | Acc: 22.630% (1709/7552)\n",
      "Loss: 2.178 | Acc: 22.689% (1728/7616)\n",
      "Loss: 2.176 | Acc: 22.760% (1748/7680)\n",
      "Loss: 2.173 | Acc: 22.818% (1767/7744)\n",
      "Loss: 2.171 | Acc: 22.964% (1793/7808)\n",
      "Loss: 2.169 | Acc: 23.018% (1812/7872)\n",
      "Loss: 2.166 | Acc: 23.085% (1832/7936)\n",
      "Loss: 2.165 | Acc: 23.062% (1845/8000)\n",
      "Loss: 2.163 | Acc: 23.053% (1859/8064)\n",
      "Loss: 2.160 | Acc: 23.118% (1879/8128)\n",
      "Loss: 2.158 | Acc: 23.108% (1893/8192)\n",
      "Loss: 2.157 | Acc: 23.098% (1907/8256)\n",
      "Loss: 2.155 | Acc: 23.101% (1922/8320)\n",
      "Loss: 2.151 | Acc: 23.223% (1947/8384)\n",
      "Loss: 2.150 | Acc: 23.224% (1962/8448)\n",
      "Loss: 2.148 | Acc: 23.320% (1985/8512)\n",
      "Loss: 2.145 | Acc: 23.321% (2000/8576)\n",
      "Loss: 2.143 | Acc: 23.345% (2017/8640)\n",
      "Loss: 2.140 | Acc: 23.460% (2042/8704)\n",
      "Loss: 2.138 | Acc: 23.540% (2064/8768)\n",
      "Loss: 2.136 | Acc: 23.551% (2080/8832)\n",
      "Loss: 2.135 | Acc: 23.572% (2097/8896)\n",
      "Loss: 2.133 | Acc: 23.627% (2117/8960)\n",
      "Loss: 2.132 | Acc: 23.670% (2136/9024)\n",
      "Loss: 2.131 | Acc: 23.713% (2155/9088)\n",
      "Loss: 2.129 | Acc: 23.733% (2172/9152)\n",
      "Loss: 2.128 | Acc: 23.698% (2184/9216)\n",
      "Loss: 2.129 | Acc: 23.685% (2198/9280)\n",
      "Loss: 2.127 | Acc: 23.737% (2218/9344)\n",
      "Loss: 2.124 | Acc: 23.788% (2238/9408)\n",
      "Loss: 2.121 | Acc: 23.891% (2263/9472)\n",
      "Loss: 2.120 | Acc: 23.878% (2277/9536)\n",
      "Loss: 2.119 | Acc: 23.875% (2292/9600)\n",
      "Loss: 2.118 | Acc: 23.903% (2310/9664)\n",
      "Loss: 2.116 | Acc: 23.982% (2333/9728)\n",
      "Loss: 2.114 | Acc: 24.030% (2353/9792)\n",
      "Loss: 2.112 | Acc: 24.097% (2375/9856)\n",
      "Loss: 2.110 | Acc: 24.163% (2397/9920)\n",
      "Loss: 2.108 | Acc: 24.149% (2411/9984)\n",
      "Loss: 2.106 | Acc: 24.144% (2426/10048)\n",
      "Loss: 2.105 | Acc: 24.179% (2445/10112)\n",
      "Loss: 2.105 | Acc: 24.155% (2458/10176)\n",
      "Loss: 2.105 | Acc: 24.199% (2478/10240)\n",
      "Loss: 2.104 | Acc: 24.243% (2498/10304)\n",
      "Loss: 2.102 | Acc: 24.257% (2515/10368)\n",
      "Loss: 2.100 | Acc: 24.367% (2542/10432)\n",
      "Loss: 2.098 | Acc: 24.409% (2562/10496)\n",
      "Loss: 2.097 | Acc: 24.403% (2577/10560)\n",
      "Loss: 2.096 | Acc: 24.435% (2596/10624)\n",
      "Loss: 2.094 | Acc: 24.457% (2614/10688)\n",
      "Loss: 2.093 | Acc: 24.516% (2636/10752)\n",
      "Loss: 2.091 | Acc: 24.602% (2661/10816)\n",
      "Loss: 2.090 | Acc: 24.669% (2684/10880)\n",
      "Loss: 2.088 | Acc: 24.735% (2707/10944)\n",
      "Loss: 2.086 | Acc: 24.782% (2728/11008)\n",
      "Loss: 2.085 | Acc: 24.828% (2749/11072)\n",
      "Loss: 2.083 | Acc: 24.874% (2770/11136)\n",
      "Loss: 2.081 | Acc: 24.973% (2797/11200)\n",
      "Loss: 2.080 | Acc: 24.991% (2815/11264)\n",
      "Loss: 2.079 | Acc: 25.044% (2837/11328)\n",
      "Loss: 2.077 | Acc: 25.114% (2861/11392)\n",
      "Loss: 2.074 | Acc: 25.192% (2886/11456)\n",
      "Loss: 2.072 | Acc: 25.269% (2911/11520)\n",
      "Loss: 2.070 | Acc: 25.319% (2933/11584)\n",
      "Loss: 2.067 | Acc: 25.455% (2965/11648)\n",
      "Loss: 2.065 | Acc: 25.478% (2984/11712)\n",
      "Loss: 2.063 | Acc: 25.543% (3008/11776)\n",
      "Loss: 2.060 | Acc: 25.625% (3034/11840)\n",
      "Loss: 2.058 | Acc: 25.697% (3059/11904)\n",
      "Loss: 2.057 | Acc: 25.685% (3074/11968)\n",
      "Loss: 2.057 | Acc: 25.682% (3090/12032)\n",
      "Loss: 2.055 | Acc: 25.711% (3110/12096)\n",
      "Loss: 2.054 | Acc: 25.699% (3125/12160)\n",
      "Loss: 2.053 | Acc: 25.736% (3146/12224)\n",
      "Loss: 2.052 | Acc: 25.749% (3164/12288)\n",
      "Loss: 2.051 | Acc: 25.818% (3189/12352)\n",
      "Loss: 2.050 | Acc: 25.838% (3208/12416)\n",
      "Loss: 2.048 | Acc: 25.865% (3228/12480)\n",
      "Loss: 2.046 | Acc: 25.933% (3253/12544)\n",
      "Loss: 2.045 | Acc: 25.928% (3269/12608)\n",
      "Loss: 2.043 | Acc: 25.986% (3293/12672)\n",
      "Loss: 2.042 | Acc: 26.029% (3315/12736)\n",
      "Loss: 2.041 | Acc: 26.031% (3332/12800)\n",
      "Loss: 2.040 | Acc: 26.034% (3349/12864)\n",
      "Loss: 2.038 | Acc: 26.114% (3376/12928)\n",
      "Loss: 2.037 | Acc: 26.147% (3397/12992)\n",
      "Loss: 2.035 | Acc: 26.187% (3419/13056)\n",
      "Loss: 2.034 | Acc: 26.265% (3446/13120)\n",
      "Loss: 2.033 | Acc: 26.282% (3465/13184)\n",
      "Loss: 2.032 | Acc: 26.321% (3487/13248)\n",
      "Loss: 2.031 | Acc: 26.292% (3500/13312)\n",
      "Loss: 2.030 | Acc: 26.323% (3521/13376)\n",
      "Loss: 2.029 | Acc: 26.406% (3549/13440)\n",
      "Loss: 2.028 | Acc: 26.474% (3575/13504)\n",
      "Loss: 2.027 | Acc: 26.504% (3596/13568)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.025 | Acc: 26.570% (3622/13632)\n",
      "Loss: 2.023 | Acc: 26.643% (3649/13696)\n",
      "Loss: 2.020 | Acc: 26.722% (3677/13760)\n",
      "Loss: 2.019 | Acc: 26.751% (3698/13824)\n",
      "Loss: 2.017 | Acc: 26.815% (3724/13888)\n",
      "Loss: 2.016 | Acc: 26.856% (3747/13952)\n",
      "Loss: 2.015 | Acc: 26.898% (3770/14016)\n",
      "Loss: 2.013 | Acc: 26.953% (3795/14080)\n",
      "Loss: 2.011 | Acc: 27.029% (3823/14144)\n",
      "Loss: 2.011 | Acc: 27.062% (3845/14208)\n",
      "Loss: 2.010 | Acc: 27.067% (3863/14272)\n",
      "Loss: 2.009 | Acc: 27.121% (3888/14336)\n",
      "Loss: 2.007 | Acc: 27.188% (3915/14400)\n",
      "Loss: 2.007 | Acc: 27.212% (3936/14464)\n",
      "Loss: 2.005 | Acc: 27.251% (3959/14528)\n",
      "Loss: 2.005 | Acc: 27.275% (3980/14592)\n",
      "Loss: 2.004 | Acc: 27.306% (4002/14656)\n",
      "Loss: 2.004 | Acc: 27.344% (4025/14720)\n",
      "Loss: 2.003 | Acc: 27.415% (4053/14784)\n",
      "Loss: 2.002 | Acc: 27.425% (4072/14848)\n",
      "Loss: 2.001 | Acc: 27.468% (4096/14912)\n",
      "Loss: 2.000 | Acc: 27.511% (4120/14976)\n",
      "Loss: 1.998 | Acc: 27.553% (4144/15040)\n",
      "Loss: 1.996 | Acc: 27.609% (4170/15104)\n",
      "Loss: 1.994 | Acc: 27.657% (4195/15168)\n",
      "Loss: 1.993 | Acc: 27.705% (4220/15232)\n",
      "Loss: 1.993 | Acc: 27.674% (4233/15296)\n",
      "Loss: 1.992 | Acc: 27.728% (4259/15360)\n",
      "Loss: 1.992 | Acc: 27.723% (4276/15424)\n",
      "Loss: 1.991 | Acc: 27.757% (4299/15488)\n",
      "Loss: 1.990 | Acc: 27.791% (4322/15552)\n",
      "Loss: 1.989 | Acc: 27.805% (4342/15616)\n",
      "Loss: 1.989 | Acc: 27.812% (4361/15680)\n",
      "Loss: 1.988 | Acc: 27.788% (4375/15744)\n",
      "Loss: 1.987 | Acc: 27.834% (4400/15808)\n",
      "Loss: 1.986 | Acc: 27.867% (4423/15872)\n",
      "Loss: 1.985 | Acc: 27.899% (4446/15936)\n",
      "Loss: 1.984 | Acc: 27.900% (4464/16000)\n",
      "Loss: 1.984 | Acc: 27.951% (4490/16064)\n",
      "Loss: 1.983 | Acc: 27.995% (4515/16128)\n",
      "Loss: 1.982 | Acc: 28.008% (4535/16192)\n",
      "Loss: 1.980 | Acc: 28.076% (4564/16256)\n",
      "Loss: 1.979 | Acc: 28.064% (4580/16320)\n",
      "Loss: 1.978 | Acc: 28.052% (4596/16384)\n",
      "Loss: 1.978 | Acc: 28.058% (4615/16448)\n",
      "Loss: 1.978 | Acc: 28.022% (4627/16512)\n",
      "Loss: 1.978 | Acc: 28.071% (4653/16576)\n",
      "Loss: 1.976 | Acc: 28.119% (4679/16640)\n",
      "Loss: 1.977 | Acc: 28.089% (4692/16704)\n",
      "Loss: 1.976 | Acc: 28.107% (4713/16768)\n",
      "Loss: 1.975 | Acc: 28.131% (4735/16832)\n",
      "Loss: 1.974 | Acc: 28.184% (4762/16896)\n",
      "Loss: 1.974 | Acc: 28.160% (4776/16960)\n",
      "Loss: 1.974 | Acc: 28.166% (4795/17024)\n",
      "Loss: 1.973 | Acc: 28.184% (4816/17088)\n",
      "Loss: 1.972 | Acc: 28.218% (4840/17152)\n",
      "Loss: 1.971 | Acc: 28.235% (4861/17216)\n",
      "Loss: 1.970 | Acc: 28.287% (4888/17280)\n",
      "Loss: 1.968 | Acc: 28.338% (4915/17344)\n",
      "Loss: 1.967 | Acc: 28.395% (4943/17408)\n",
      "Loss: 1.966 | Acc: 28.457% (4972/17472)\n",
      "Loss: 1.965 | Acc: 28.490% (4996/17536)\n",
      "Loss: 1.964 | Acc: 28.540% (5023/17600)\n",
      "Loss: 1.963 | Acc: 28.578% (5048/17664)\n",
      "Loss: 1.963 | Acc: 28.610% (5072/17728)\n",
      "Loss: 1.961 | Acc: 28.698% (5106/17792)\n",
      "Loss: 1.960 | Acc: 28.702% (5125/17856)\n",
      "Loss: 1.959 | Acc: 28.761% (5154/17920)\n",
      "Loss: 1.958 | Acc: 28.787% (5177/17984)\n",
      "Loss: 1.957 | Acc: 28.829% (5203/18048)\n",
      "Loss: 1.956 | Acc: 28.859% (5227/18112)\n",
      "Loss: 1.955 | Acc: 28.879% (5249/18176)\n",
      "Loss: 1.955 | Acc: 28.898% (5271/18240)\n",
      "Loss: 1.954 | Acc: 28.906% (5291/18304)\n",
      "Loss: 1.954 | Acc: 28.942% (5316/18368)\n",
      "Loss: 1.952 | Acc: 28.977% (5341/18432)\n",
      "Loss: 1.952 | Acc: 29.017% (5367/18496)\n",
      "Loss: 1.951 | Acc: 29.062% (5394/18560)\n",
      "Loss: 1.950 | Acc: 29.097% (5419/18624)\n",
      "Loss: 1.949 | Acc: 29.115% (5441/18688)\n",
      "Loss: 1.949 | Acc: 29.149% (5466/18752)\n",
      "Loss: 1.948 | Acc: 29.177% (5490/18816)\n",
      "Loss: 1.947 | Acc: 29.211% (5515/18880)\n",
      "Loss: 1.946 | Acc: 29.260% (5543/18944)\n",
      "Loss: 1.945 | Acc: 29.319% (5573/19008)\n",
      "Loss: 1.944 | Acc: 29.347% (5597/19072)\n",
      "Loss: 1.943 | Acc: 29.400% (5626/19136)\n",
      "Loss: 1.943 | Acc: 29.406% (5646/19200)\n",
      "Loss: 1.942 | Acc: 29.438% (5671/19264)\n",
      "Loss: 1.941 | Acc: 29.501% (5702/19328)\n",
      "Loss: 1.941 | Acc: 29.507% (5722/19392)\n",
      "Loss: 1.940 | Acc: 29.508% (5741/19456)\n",
      "Loss: 1.939 | Acc: 29.529% (5764/19520)\n",
      "Loss: 1.938 | Acc: 29.539% (5785/19584)\n",
      "Loss: 1.937 | Acc: 29.535% (5803/19648)\n",
      "Loss: 1.936 | Acc: 29.571% (5829/19712)\n",
      "Loss: 1.935 | Acc: 29.607% (5855/19776)\n",
      "Loss: 1.934 | Acc: 29.647% (5882/19840)\n",
      "Loss: 1.933 | Acc: 29.667% (5905/19904)\n",
      "Loss: 1.932 | Acc: 29.713% (5933/19968)\n",
      "Loss: 1.932 | Acc: 29.747% (5959/20032)\n",
      "Loss: 1.931 | Acc: 29.777% (5984/20096)\n",
      "Loss: 1.930 | Acc: 29.787% (6005/20160)\n",
      "Loss: 1.929 | Acc: 29.821% (6031/20224)\n",
      "Loss: 1.928 | Acc: 29.865% (6059/20288)\n",
      "Loss: 1.928 | Acc: 29.884% (6082/20352)\n",
      "Loss: 1.927 | Acc: 29.893% (6103/20416)\n",
      "Loss: 1.928 | Acc: 29.873% (6118/20480)\n",
      "Loss: 1.927 | Acc: 29.916% (6146/20544)\n",
      "Loss: 1.926 | Acc: 29.916% (6165/20608)\n",
      "Loss: 1.925 | Acc: 29.920% (6185/20672)\n",
      "Loss: 1.925 | Acc: 29.924% (6205/20736)\n",
      "Loss: 1.925 | Acc: 29.938% (6227/20800)\n",
      "Loss: 1.924 | Acc: 29.989% (6257/20864)\n",
      "Loss: 1.923 | Acc: 30.017% (6282/20928)\n",
      "Loss: 1.922 | Acc: 30.097% (6318/20992)\n",
      "Loss: 1.921 | Acc: 30.096% (6337/21056)\n",
      "Loss: 1.921 | Acc: 30.071% (6351/21120)\n",
      "Loss: 1.920 | Acc: 30.098% (6376/21184)\n",
      "Loss: 1.920 | Acc: 30.111% (6398/21248)\n",
      "Loss: 1.919 | Acc: 30.147% (6425/21312)\n",
      "Loss: 1.918 | Acc: 30.174% (6450/21376)\n",
      "Loss: 1.917 | Acc: 30.205% (6476/21440)\n",
      "Loss: 1.916 | Acc: 30.208% (6496/21504)\n",
      "Loss: 1.915 | Acc: 30.221% (6518/21568)\n",
      "Loss: 1.915 | Acc: 30.228% (6539/21632)\n",
      "Loss: 1.915 | Acc: 30.264% (6566/21696)\n",
      "Loss: 1.913 | Acc: 30.331% (6600/21760)\n",
      "Loss: 1.912 | Acc: 30.366% (6627/21824)\n",
      "Loss: 1.911 | Acc: 30.418% (6658/21888)\n",
      "Loss: 1.911 | Acc: 30.448% (6684/21952)\n",
      "Loss: 1.910 | Acc: 30.478% (6710/22016)\n",
      "Loss: 1.910 | Acc: 30.485% (6731/22080)\n",
      "Loss: 1.909 | Acc: 30.487% (6751/22144)\n",
      "Loss: 1.908 | Acc: 30.512% (6776/22208)\n",
      "Loss: 1.908 | Acc: 30.509% (6795/22272)\n",
      "Loss: 1.907 | Acc: 30.525% (6818/22336)\n",
      "Loss: 1.906 | Acc: 30.576% (6849/22400)\n",
      "Loss: 1.906 | Acc: 30.587% (6871/22464)\n",
      "Loss: 1.906 | Acc: 30.584% (6890/22528)\n",
      "Loss: 1.905 | Acc: 30.586% (6910/22592)\n",
      "Loss: 1.904 | Acc: 30.610% (6935/22656)\n",
      "Loss: 1.903 | Acc: 30.647% (6963/22720)\n",
      "Loss: 1.902 | Acc: 30.666% (6987/22784)\n",
      "Loss: 1.902 | Acc: 30.668% (7007/22848)\n",
      "Loss: 1.901 | Acc: 30.670% (7027/22912)\n",
      "Loss: 1.899 | Acc: 30.710% (7056/22976)\n",
      "Loss: 1.898 | Acc: 30.734% (7081/23040)\n",
      "Loss: 1.897 | Acc: 30.770% (7109/23104)\n",
      "Loss: 1.897 | Acc: 30.775% (7130/23168)\n",
      "Loss: 1.896 | Acc: 30.794% (7154/23232)\n",
      "Loss: 1.896 | Acc: 30.782% (7171/23296)\n",
      "Loss: 1.895 | Acc: 30.835% (7203/23360)\n",
      "Loss: 1.896 | Acc: 30.823% (7220/23424)\n",
      "Loss: 1.896 | Acc: 30.811% (7237/23488)\n",
      "Loss: 1.895 | Acc: 30.817% (7258/23552)\n",
      "Loss: 1.894 | Acc: 30.843% (7284/23616)\n",
      "Loss: 1.894 | Acc: 30.866% (7309/23680)\n",
      "Loss: 1.894 | Acc: 30.888% (7334/23744)\n",
      "Loss: 1.893 | Acc: 30.897% (7356/23808)\n",
      "Loss: 1.893 | Acc: 30.911% (7379/23872)\n",
      "Loss: 1.893 | Acc: 30.924% (7402/23936)\n",
      "Loss: 1.892 | Acc: 30.946% (7427/24000)\n",
      "Loss: 1.892 | Acc: 30.918% (7440/24064)\n",
      "Loss: 1.892 | Acc: 30.927% (7462/24128)\n",
      "Loss: 1.892 | Acc: 30.940% (7485/24192)\n",
      "Loss: 1.890 | Acc: 30.994% (7518/24256)\n",
      "Loss: 1.890 | Acc: 31.028% (7546/24320)\n",
      "Loss: 1.889 | Acc: 31.037% (7568/24384)\n",
      "Loss: 1.889 | Acc: 31.029% (7586/24448)\n",
      "Loss: 1.889 | Acc: 31.034% (7607/24512)\n",
      "Loss: 1.887 | Acc: 31.087% (7640/24576)\n",
      "Loss: 1.887 | Acc: 31.112% (7666/24640)\n",
      "Loss: 1.886 | Acc: 31.145% (7694/24704)\n",
      "Loss: 1.886 | Acc: 31.161% (7718/24768)\n",
      "Loss: 1.885 | Acc: 31.178% (7742/24832)\n",
      "Loss: 1.885 | Acc: 31.190% (7765/24896)\n",
      "Loss: 1.885 | Acc: 31.170% (7780/24960)\n",
      "Loss: 1.884 | Acc: 31.182% (7803/25024)\n",
      "Loss: 1.884 | Acc: 31.206% (7829/25088)\n",
      "Loss: 1.883 | Acc: 31.230% (7855/25152)\n",
      "Loss: 1.882 | Acc: 31.246% (7879/25216)\n",
      "Loss: 1.882 | Acc: 31.250% (7900/25280)\n",
      "Loss: 1.881 | Acc: 31.274% (7926/25344)\n",
      "Loss: 1.881 | Acc: 31.281% (7948/25408)\n",
      "Loss: 1.881 | Acc: 31.301% (7973/25472)\n",
      "Loss: 1.881 | Acc: 31.301% (7993/25536)\n",
      "Loss: 1.880 | Acc: 31.324% (8019/25600)\n",
      "Loss: 1.879 | Acc: 31.344% (8044/25664)\n",
      "Loss: 1.879 | Acc: 31.320% (8058/25728)\n",
      "Loss: 1.879 | Acc: 31.316% (8077/25792)\n",
      "Loss: 1.879 | Acc: 31.335% (8102/25856)\n",
      "Loss: 1.878 | Acc: 31.358% (8128/25920)\n",
      "Loss: 1.878 | Acc: 31.389% (8156/25984)\n",
      "Loss: 1.877 | Acc: 31.388% (8176/26048)\n",
      "Loss: 1.877 | Acc: 31.407% (8201/26112)\n",
      "Loss: 1.877 | Acc: 31.414% (8223/26176)\n",
      "Loss: 1.876 | Acc: 31.418% (8244/26240)\n",
      "Loss: 1.876 | Acc: 31.444% (8271/26304)\n",
      "Loss: 1.875 | Acc: 31.462% (8296/26368)\n",
      "Loss: 1.875 | Acc: 31.481% (8321/26432)\n",
      "Loss: 1.875 | Acc: 31.469% (8338/26496)\n",
      "Loss: 1.874 | Acc: 31.476% (8360/26560)\n",
      "Loss: 1.874 | Acc: 31.487% (8383/26624)\n",
      "Loss: 1.874 | Acc: 31.475% (8400/26688)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.874 | Acc: 31.482% (8422/26752)\n",
      "Loss: 1.874 | Acc: 31.481% (8442/26816)\n",
      "Loss: 1.873 | Acc: 31.499% (8467/26880)\n",
      "Loss: 1.872 | Acc: 31.499% (8487/26944)\n",
      "Loss: 1.872 | Acc: 31.550% (8521/27008)\n",
      "Loss: 1.871 | Acc: 31.575% (8548/27072)\n",
      "Loss: 1.870 | Acc: 31.604% (8576/27136)\n",
      "Loss: 1.870 | Acc: 31.607% (8597/27200)\n",
      "Loss: 1.869 | Acc: 31.591% (8613/27264)\n",
      "Loss: 1.869 | Acc: 31.598% (8635/27328)\n",
      "Loss: 1.869 | Acc: 31.615% (8660/27392)\n",
      "Loss: 1.868 | Acc: 31.629% (8684/27456)\n",
      "Loss: 1.868 | Acc: 31.617% (8701/27520)\n",
      "Loss: 1.867 | Acc: 31.623% (8723/27584)\n",
      "Loss: 1.866 | Acc: 31.651% (8751/27648)\n",
      "Loss: 1.866 | Acc: 31.669% (8776/27712)\n",
      "Loss: 1.866 | Acc: 31.678% (8799/27776)\n",
      "Loss: 1.866 | Acc: 31.659% (8814/27840)\n",
      "Loss: 1.866 | Acc: 31.676% (8839/27904)\n",
      "Loss: 1.865 | Acc: 31.690% (8863/27968)\n",
      "Loss: 1.865 | Acc: 31.710% (8889/28032)\n",
      "Loss: 1.864 | Acc: 31.730% (8915/28096)\n",
      "Loss: 1.863 | Acc: 31.747% (8940/28160)\n",
      "Loss: 1.863 | Acc: 31.767% (8966/28224)\n",
      "Loss: 1.862 | Acc: 31.752% (8982/28288)\n",
      "Loss: 1.862 | Acc: 31.758% (9004/28352)\n",
      "Loss: 1.862 | Acc: 31.774% (9029/28416)\n",
      "Loss: 1.861 | Acc: 31.791% (9054/28480)\n",
      "Loss: 1.861 | Acc: 31.800% (9077/28544)\n",
      "Loss: 1.860 | Acc: 31.816% (9102/28608)\n",
      "Loss: 1.860 | Acc: 31.815% (9122/28672)\n",
      "Loss: 1.859 | Acc: 31.859% (9155/28736)\n",
      "Loss: 1.859 | Acc: 31.861% (9176/28800)\n",
      "Loss: 1.858 | Acc: 31.860% (9196/28864)\n",
      "Loss: 1.857 | Acc: 31.872% (9220/28928)\n",
      "Loss: 1.857 | Acc: 31.895% (9247/28992)\n",
      "Loss: 1.857 | Acc: 31.911% (9272/29056)\n",
      "Loss: 1.856 | Acc: 31.937% (9300/29120)\n",
      "Loss: 1.856 | Acc: 31.973% (9331/29184)\n",
      "Loss: 1.855 | Acc: 31.995% (9358/29248)\n",
      "Loss: 1.855 | Acc: 31.987% (9376/29312)\n",
      "Loss: 1.855 | Acc: 32.009% (9403/29376)\n",
      "Loss: 1.854 | Acc: 32.035% (9431/29440)\n",
      "Loss: 1.853 | Acc: 32.070% (9462/29504)\n",
      "Loss: 1.852 | Acc: 32.085% (9487/29568)\n",
      "Loss: 1.852 | Acc: 32.111% (9515/29632)\n",
      "Loss: 1.851 | Acc: 32.119% (9538/29696)\n",
      "Loss: 1.851 | Acc: 32.124% (9560/29760)\n",
      "Loss: 1.850 | Acc: 32.159% (9591/29824)\n",
      "Loss: 1.850 | Acc: 32.180% (9618/29888)\n",
      "Loss: 1.849 | Acc: 32.198% (9644/29952)\n",
      "Loss: 1.848 | Acc: 32.226% (9673/30016)\n",
      "Loss: 1.848 | Acc: 32.267% (9706/30080)\n",
      "Loss: 1.847 | Acc: 32.298% (9736/30144)\n",
      "Loss: 1.846 | Acc: 32.329% (9766/30208)\n",
      "Loss: 1.846 | Acc: 32.343% (9791/30272)\n",
      "Loss: 1.845 | Acc: 32.384% (9824/30336)\n",
      "Loss: 1.844 | Acc: 32.418% (9855/30400)\n",
      "Loss: 1.843 | Acc: 32.442% (9883/30464)\n",
      "Loss: 1.844 | Acc: 32.442% (9904/30528)\n",
      "Loss: 1.844 | Acc: 32.430% (9921/30592)\n",
      "Loss: 1.843 | Acc: 32.467% (9953/30656)\n",
      "Loss: 1.843 | Acc: 32.464% (9973/30720)\n",
      "Loss: 1.843 | Acc: 32.471% (9996/30784)\n",
      "Loss: 1.842 | Acc: 32.492% (10023/30848)\n",
      "Loss: 1.842 | Acc: 32.505% (10048/30912)\n",
      "Loss: 1.842 | Acc: 32.496% (10066/30976)\n",
      "Loss: 1.841 | Acc: 32.529% (10097/31040)\n",
      "Loss: 1.841 | Acc: 32.549% (10124/31104)\n",
      "Loss: 1.840 | Acc: 32.562% (10149/31168)\n",
      "Loss: 1.840 | Acc: 32.566% (10171/31232)\n",
      "Loss: 1.840 | Acc: 32.582% (10197/31296)\n",
      "Loss: 1.839 | Acc: 32.592% (10221/31360)\n",
      "Loss: 1.839 | Acc: 32.618% (10250/31424)\n",
      "Loss: 1.838 | Acc: 32.651% (10281/31488)\n",
      "Loss: 1.838 | Acc: 32.683% (10312/31552)\n",
      "Loss: 1.837 | Acc: 32.692% (10336/31616)\n",
      "Loss: 1.837 | Acc: 32.730% (10369/31680)\n",
      "Loss: 1.836 | Acc: 32.740% (10393/31744)\n",
      "Loss: 1.836 | Acc: 32.740% (10414/31808)\n",
      "Loss: 1.836 | Acc: 32.737% (10434/31872)\n",
      "Loss: 1.836 | Acc: 32.744% (10457/31936)\n",
      "Loss: 1.835 | Acc: 32.759% (10483/32000)\n",
      "Loss: 1.835 | Acc: 32.769% (10507/32064)\n",
      "Loss: 1.834 | Acc: 32.784% (10533/32128)\n",
      "Loss: 1.834 | Acc: 32.785% (10554/32192)\n",
      "Loss: 1.834 | Acc: 32.797% (10579/32256)\n",
      "Loss: 1.834 | Acc: 32.812% (10605/32320)\n",
      "Loss: 1.833 | Acc: 32.834% (10633/32384)\n",
      "Loss: 1.833 | Acc: 32.859% (10662/32448)\n",
      "Loss: 1.832 | Acc: 32.889% (10693/32512)\n",
      "Loss: 1.832 | Acc: 32.926% (10726/32576)\n",
      "Loss: 1.831 | Acc: 32.941% (10752/32640)\n",
      "Loss: 1.831 | Acc: 32.947% (10775/32704)\n",
      "Loss: 1.831 | Acc: 32.962% (10801/32768)\n",
      "Loss: 1.830 | Acc: 32.977% (10827/32832)\n",
      "Loss: 1.830 | Acc: 33.007% (10858/32896)\n",
      "Loss: 1.829 | Acc: 33.019% (10883/32960)\n",
      "Loss: 1.829 | Acc: 33.031% (10908/33024)\n",
      "Loss: 1.828 | Acc: 33.024% (10927/33088)\n",
      "Loss: 1.828 | Acc: 33.042% (10954/33152)\n",
      "Loss: 1.828 | Acc: 33.050% (10978/33216)\n",
      "Loss: 1.828 | Acc: 33.053% (11000/33280)\n",
      "Loss: 1.827 | Acc: 33.064% (11025/33344)\n",
      "Loss: 1.828 | Acc: 33.076% (11050/33408)\n",
      "Loss: 1.827 | Acc: 33.108% (11082/33472)\n",
      "Loss: 1.826 | Acc: 33.114% (11105/33536)\n",
      "Loss: 1.826 | Acc: 33.134% (11133/33600)\n",
      "Loss: 1.826 | Acc: 33.145% (11158/33664)\n",
      "Loss: 1.826 | Acc: 33.148% (11180/33728)\n",
      "Loss: 1.826 | Acc: 33.156% (11204/33792)\n",
      "Loss: 1.826 | Acc: 33.155% (11225/33856)\n",
      "Loss: 1.825 | Acc: 33.157% (11247/33920)\n",
      "Loss: 1.825 | Acc: 33.180% (11276/33984)\n",
      "Loss: 1.824 | Acc: 33.186% (11299/34048)\n",
      "Loss: 1.824 | Acc: 33.197% (11324/34112)\n",
      "Loss: 1.823 | Acc: 33.208% (11349/34176)\n",
      "Loss: 1.822 | Acc: 33.236% (11380/34240)\n",
      "Loss: 1.822 | Acc: 33.250% (11406/34304)\n",
      "Loss: 1.822 | Acc: 33.258% (11430/34368)\n",
      "Loss: 1.821 | Acc: 33.274% (11457/34432)\n",
      "Loss: 1.821 | Acc: 33.288% (11483/34496)\n",
      "Loss: 1.821 | Acc: 33.299% (11508/34560)\n",
      "Loss: 1.820 | Acc: 33.312% (11534/34624)\n",
      "Loss: 1.819 | Acc: 33.331% (11562/34688)\n",
      "Loss: 1.819 | Acc: 33.345% (11588/34752)\n",
      "Loss: 1.818 | Acc: 33.355% (11613/34816)\n",
      "Loss: 1.818 | Acc: 33.383% (11644/34880)\n",
      "Loss: 1.817 | Acc: 33.411% (11675/34944)\n",
      "Loss: 1.816 | Acc: 33.444% (11708/35008)\n",
      "Loss: 1.816 | Acc: 33.468% (11738/35072)\n",
      "Loss: 1.815 | Acc: 33.490% (11767/35136)\n",
      "Loss: 1.815 | Acc: 33.506% (11794/35200)\n",
      "Loss: 1.815 | Acc: 33.507% (11816/35264)\n",
      "Loss: 1.815 | Acc: 33.514% (11840/35328)\n",
      "Loss: 1.814 | Acc: 33.513% (11861/35392)\n",
      "Loss: 1.814 | Acc: 33.532% (11889/35456)\n",
      "Loss: 1.813 | Acc: 33.556% (11919/35520)\n",
      "Loss: 1.813 | Acc: 33.583% (11950/35584)\n",
      "Loss: 1.812 | Acc: 33.598% (11977/35648)\n",
      "Loss: 1.812 | Acc: 33.619% (12006/35712)\n",
      "Loss: 1.811 | Acc: 33.626% (12030/35776)\n",
      "Loss: 1.812 | Acc: 33.622% (12050/35840)\n",
      "Loss: 1.812 | Acc: 33.631% (12075/35904)\n",
      "Loss: 1.811 | Acc: 33.660% (12107/35968)\n",
      "Loss: 1.811 | Acc: 33.656% (12127/36032)\n",
      "Loss: 1.810 | Acc: 33.666% (12152/36096)\n",
      "Loss: 1.810 | Acc: 33.684% (12180/36160)\n",
      "Loss: 1.810 | Acc: 33.685% (12202/36224)\n",
      "Loss: 1.809 | Acc: 33.714% (12234/36288)\n",
      "Loss: 1.809 | Acc: 33.715% (12256/36352)\n",
      "Loss: 1.809 | Acc: 33.719% (12279/36416)\n",
      "Loss: 1.809 | Acc: 33.725% (12303/36480)\n",
      "Loss: 1.808 | Acc: 33.748% (12333/36544)\n",
      "Loss: 1.808 | Acc: 33.771% (12363/36608)\n",
      "Loss: 1.807 | Acc: 33.786% (12390/36672)\n",
      "Loss: 1.807 | Acc: 33.801% (12417/36736)\n",
      "Loss: 1.807 | Acc: 33.815% (12444/36800)\n",
      "Loss: 1.806 | Acc: 33.824% (12469/36864)\n",
      "Loss: 1.806 | Acc: 33.820% (12489/36928)\n",
      "Loss: 1.806 | Acc: 33.824% (12512/36992)\n",
      "Loss: 1.806 | Acc: 33.835% (12538/37056)\n",
      "Loss: 1.805 | Acc: 33.860% (12569/37120)\n",
      "Loss: 1.805 | Acc: 33.872% (12595/37184)\n",
      "Loss: 1.805 | Acc: 33.889% (12623/37248)\n",
      "Loss: 1.805 | Acc: 33.901% (12649/37312)\n",
      "Loss: 1.804 | Acc: 33.920% (12678/37376)\n",
      "Loss: 1.804 | Acc: 33.924% (12701/37440)\n",
      "Loss: 1.804 | Acc: 33.946% (12731/37504)\n",
      "Loss: 1.803 | Acc: 33.978% (12765/37568)\n",
      "Loss: 1.803 | Acc: 33.992% (12792/37632)\n",
      "Loss: 1.802 | Acc: 34.022% (12825/37696)\n",
      "Loss: 1.801 | Acc: 34.033% (12851/37760)\n",
      "Loss: 1.800 | Acc: 34.052% (12880/37824)\n",
      "Loss: 1.800 | Acc: 34.069% (12908/37888)\n",
      "Loss: 1.800 | Acc: 34.061% (12927/37952)\n",
      "Loss: 1.800 | Acc: 34.075% (12954/38016)\n",
      "Loss: 1.800 | Acc: 34.078% (12977/38080)\n",
      "Loss: 1.800 | Acc: 34.079% (12999/38144)\n",
      "Loss: 1.800 | Acc: 34.092% (13026/38208)\n",
      "Loss: 1.799 | Acc: 34.095% (13049/38272)\n",
      "Loss: 1.799 | Acc: 34.112% (13077/38336)\n",
      "Loss: 1.798 | Acc: 34.117% (13101/38400)\n",
      "Loss: 1.798 | Acc: 34.105% (13118/38464)\n",
      "Loss: 1.798 | Acc: 34.102% (13139/38528)\n",
      "Loss: 1.798 | Acc: 34.118% (13167/38592)\n",
      "Loss: 1.797 | Acc: 34.137% (13196/38656)\n",
      "Loss: 1.797 | Acc: 34.153% (13224/38720)\n",
      "Loss: 1.797 | Acc: 34.164% (13250/38784)\n",
      "Loss: 1.796 | Acc: 34.172% (13275/38848)\n",
      "Loss: 1.796 | Acc: 34.190% (13304/38912)\n",
      "Loss: 1.796 | Acc: 34.198% (13329/38976)\n",
      "Loss: 1.796 | Acc: 34.219% (13359/39040)\n",
      "Loss: 1.795 | Acc: 34.229% (13385/39104)\n",
      "Loss: 1.795 | Acc: 34.229% (13407/39168)\n",
      "Loss: 1.795 | Acc: 34.255% (13439/39232)\n",
      "Loss: 1.795 | Acc: 34.255% (13461/39296)\n",
      "Loss: 1.794 | Acc: 34.286% (13495/39360)\n",
      "Loss: 1.794 | Acc: 34.284% (13516/39424)\n",
      "Loss: 1.794 | Acc: 34.296% (13543/39488)\n",
      "Loss: 1.793 | Acc: 34.297% (13565/39552)\n",
      "Loss: 1.793 | Acc: 34.312% (13593/39616)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.793 | Acc: 34.322% (13619/39680)\n",
      "Loss: 1.792 | Acc: 34.327% (13643/39744)\n",
      "Loss: 1.792 | Acc: 34.330% (13666/39808)\n",
      "Loss: 1.792 | Acc: 34.355% (13698/39872)\n",
      "Loss: 1.791 | Acc: 34.372% (13727/39936)\n",
      "Loss: 1.791 | Acc: 34.400% (13760/40000)\n",
      "Loss: 1.790 | Acc: 34.417% (13789/40064)\n",
      "Loss: 1.790 | Acc: 34.442% (13821/40128)\n",
      "Loss: 1.789 | Acc: 34.462% (13851/40192)\n",
      "Loss: 1.789 | Acc: 34.477% (13879/40256)\n",
      "Loss: 1.788 | Acc: 34.492% (13907/40320)\n",
      "Loss: 1.789 | Acc: 34.494% (13930/40384)\n",
      "Loss: 1.788 | Acc: 34.496% (13953/40448)\n",
      "Loss: 1.788 | Acc: 34.516% (13983/40512)\n",
      "Loss: 1.787 | Acc: 34.525% (14009/40576)\n",
      "Loss: 1.787 | Acc: 34.535% (14035/40640)\n",
      "Loss: 1.787 | Acc: 34.537% (14058/40704)\n",
      "Loss: 1.787 | Acc: 34.542% (14082/40768)\n",
      "Loss: 1.787 | Acc: 34.566% (14114/40832)\n",
      "Loss: 1.786 | Acc: 34.585% (14144/40896)\n",
      "Loss: 1.786 | Acc: 34.597% (14171/40960)\n",
      "Loss: 1.786 | Acc: 34.604% (14196/41024)\n",
      "Loss: 1.786 | Acc: 34.614% (14222/41088)\n",
      "Loss: 1.785 | Acc: 34.630% (14251/41152)\n",
      "Loss: 1.784 | Acc: 34.673% (14291/41216)\n",
      "Loss: 1.784 | Acc: 34.690% (14320/41280)\n",
      "Loss: 1.784 | Acc: 34.704% (14348/41344)\n",
      "Loss: 1.784 | Acc: 34.708% (14372/41408)\n",
      "Loss: 1.783 | Acc: 34.727% (14402/41472)\n",
      "Loss: 1.783 | Acc: 34.739% (14429/41536)\n",
      "Loss: 1.783 | Acc: 34.748% (14455/41600)\n",
      "Loss: 1.782 | Acc: 34.749% (14478/41664)\n",
      "Loss: 1.782 | Acc: 34.758% (14504/41728)\n",
      "Loss: 1.782 | Acc: 34.763% (14528/41792)\n",
      "Loss: 1.781 | Acc: 34.767% (14552/41856)\n",
      "Loss: 1.781 | Acc: 34.785% (14582/41920)\n",
      "Loss: 1.780 | Acc: 34.804% (14612/41984)\n",
      "Loss: 1.780 | Acc: 34.813% (14638/42048)\n",
      "Loss: 1.780 | Acc: 34.829% (14667/42112)\n",
      "Loss: 1.779 | Acc: 34.840% (14694/42176)\n",
      "Loss: 1.779 | Acc: 34.851% (14721/42240)\n",
      "Loss: 1.779 | Acc: 34.860% (14747/42304)\n",
      "Loss: 1.779 | Acc: 34.852% (14766/42368)\n",
      "Loss: 1.779 | Acc: 34.863% (14793/42432)\n",
      "Loss: 1.778 | Acc: 34.879% (14822/42496)\n",
      "Loss: 1.778 | Acc: 34.899% (14853/42560)\n",
      "Loss: 1.778 | Acc: 34.901% (14876/42624)\n",
      "Loss: 1.777 | Acc: 34.916% (14905/42688)\n",
      "Loss: 1.777 | Acc: 34.918% (14928/42752)\n",
      "Loss: 1.777 | Acc: 34.919% (14951/42816)\n",
      "Loss: 1.777 | Acc: 34.928% (14977/42880)\n",
      "Loss: 1.777 | Acc: 34.922% (14997/42944)\n",
      "Loss: 1.777 | Acc: 34.933% (15024/43008)\n",
      "Loss: 1.777 | Acc: 34.932% (15046/43072)\n",
      "Loss: 1.776 | Acc: 34.931% (15068/43136)\n",
      "Loss: 1.776 | Acc: 34.951% (15099/43200)\n",
      "Loss: 1.776 | Acc: 34.944% (15118/43264)\n",
      "Loss: 1.776 | Acc: 34.959% (15147/43328)\n",
      "Loss: 1.775 | Acc: 34.983% (15180/43392)\n",
      "Loss: 1.775 | Acc: 34.983% (15202/43456)\n",
      "Loss: 1.775 | Acc: 34.989% (15227/43520)\n",
      "Loss: 1.775 | Acc: 34.976% (15244/43584)\n",
      "Loss: 1.774 | Acc: 35.012% (15282/43648)\n",
      "Loss: 1.774 | Acc: 35.018% (15307/43712)\n",
      "Loss: 1.775 | Acc: 35.012% (15327/43776)\n",
      "Loss: 1.775 | Acc: 35.025% (15355/43840)\n",
      "Loss: 1.774 | Acc: 35.045% (15386/43904)\n",
      "Loss: 1.774 | Acc: 35.044% (15408/43968)\n",
      "Loss: 1.774 | Acc: 35.061% (15438/44032)\n",
      "Loss: 1.773 | Acc: 35.069% (15464/44096)\n",
      "Loss: 1.773 | Acc: 35.086% (15494/44160)\n",
      "Loss: 1.773 | Acc: 35.090% (15518/44224)\n",
      "Loss: 1.773 | Acc: 35.095% (15543/44288)\n",
      "Loss: 1.773 | Acc: 35.099% (15567/44352)\n",
      "Loss: 1.773 | Acc: 35.102% (15591/44416)\n",
      "Loss: 1.772 | Acc: 35.115% (15619/44480)\n",
      "Loss: 1.772 | Acc: 35.120% (15644/44544)\n",
      "Loss: 1.771 | Acc: 35.128% (15670/44608)\n",
      "Loss: 1.771 | Acc: 35.147% (15701/44672)\n",
      "Loss: 1.770 | Acc: 35.171% (15734/44736)\n",
      "Loss: 1.770 | Acc: 35.163% (15753/44800)\n",
      "Loss: 1.770 | Acc: 35.182% (15784/44864)\n",
      "Loss: 1.769 | Acc: 35.199% (15814/44928)\n",
      "Loss: 1.769 | Acc: 35.193% (15834/44992)\n",
      "Loss: 1.769 | Acc: 35.198% (15859/45056)\n",
      "Loss: 1.769 | Acc: 35.206% (15885/45120)\n",
      "Loss: 1.769 | Acc: 35.220% (15914/45184)\n",
      "Loss: 1.768 | Acc: 35.244% (15947/45248)\n",
      "Loss: 1.768 | Acc: 35.251% (15973/45312)\n",
      "Loss: 1.768 | Acc: 35.259% (15999/45376)\n",
      "Loss: 1.767 | Acc: 35.271% (16027/45440)\n",
      "Loss: 1.767 | Acc: 35.285% (16056/45504)\n",
      "Loss: 1.767 | Acc: 35.286% (16079/45568)\n",
      "Loss: 1.767 | Acc: 35.300% (16108/45632)\n",
      "Loss: 1.767 | Acc: 35.314% (16137/45696)\n",
      "Loss: 1.766 | Acc: 35.321% (16163/45760)\n",
      "Loss: 1.766 | Acc: 35.329% (16189/45824)\n",
      "Loss: 1.766 | Acc: 35.332% (16213/45888)\n",
      "Loss: 1.766 | Acc: 35.348% (16243/45952)\n",
      "Loss: 1.765 | Acc: 35.357% (16270/46016)\n",
      "Loss: 1.765 | Acc: 35.354% (16291/46080)\n",
      "Loss: 1.765 | Acc: 35.363% (16318/46144)\n",
      "Loss: 1.765 | Acc: 35.364% (16341/46208)\n",
      "Loss: 1.764 | Acc: 35.371% (16367/46272)\n",
      "Loss: 1.764 | Acc: 35.396% (16401/46336)\n",
      "Loss: 1.763 | Acc: 35.409% (16430/46400)\n",
      "Loss: 1.763 | Acc: 35.427% (16461/46464)\n",
      "Loss: 1.763 | Acc: 35.432% (16486/46528)\n",
      "Loss: 1.763 | Acc: 35.435% (16510/46592)\n",
      "Loss: 1.763 | Acc: 35.455% (16542/46656)\n",
      "Loss: 1.762 | Acc: 35.456% (16565/46720)\n",
      "Loss: 1.762 | Acc: 35.478% (16598/46784)\n",
      "Loss: 1.762 | Acc: 35.481% (16622/46848)\n",
      "Loss: 1.762 | Acc: 35.486% (16647/46912)\n",
      "Loss: 1.762 | Acc: 35.482% (16668/46976)\n",
      "Loss: 1.762 | Acc: 35.474% (16687/47040)\n",
      "Loss: 1.762 | Acc: 35.473% (16709/47104)\n",
      "Loss: 1.761 | Acc: 35.486% (16738/47168)\n",
      "Loss: 1.761 | Acc: 35.482% (16759/47232)\n",
      "Loss: 1.761 | Acc: 35.481% (16781/47296)\n",
      "Loss: 1.761 | Acc: 35.490% (16808/47360)\n",
      "Loss: 1.760 | Acc: 35.514% (16842/47424)\n",
      "Loss: 1.760 | Acc: 35.518% (16867/47488)\n",
      "Loss: 1.760 | Acc: 35.536% (16898/47552)\n",
      "Loss: 1.760 | Acc: 35.538% (16922/47616)\n",
      "Loss: 1.759 | Acc: 35.552% (16951/47680)\n",
      "Loss: 1.759 | Acc: 35.565% (16980/47744)\n",
      "Loss: 1.758 | Acc: 35.569% (17005/47808)\n",
      "Loss: 1.758 | Acc: 35.584% (17035/47872)\n",
      "Loss: 1.758 | Acc: 35.579% (17055/47936)\n",
      "Loss: 1.757 | Acc: 35.596% (17086/48000)\n",
      "Loss: 1.757 | Acc: 35.586% (17104/48064)\n",
      "Loss: 1.757 | Acc: 35.611% (17139/48128)\n",
      "Loss: 1.757 | Acc: 35.632% (17172/48192)\n",
      "Loss: 1.756 | Acc: 35.643% (17200/48256)\n",
      "Loss: 1.756 | Acc: 35.673% (17237/48320)\n",
      "Loss: 1.755 | Acc: 35.683% (17265/48384)\n",
      "Loss: 1.755 | Acc: 35.706% (17299/48448)\n",
      "Loss: 1.754 | Acc: 35.721% (17329/48512)\n",
      "Loss: 1.754 | Acc: 35.740% (17361/48576)\n",
      "Loss: 1.754 | Acc: 35.757% (17392/48640)\n",
      "Loss: 1.753 | Acc: 35.765% (17419/48704)\n",
      "Loss: 1.753 | Acc: 35.773% (17446/48768)\n",
      "Loss: 1.753 | Acc: 35.780% (17472/48832)\n",
      "Loss: 1.753 | Acc: 35.786% (17498/48896)\n",
      "Loss: 1.753 | Acc: 35.803% (17529/48960)\n",
      "Loss: 1.752 | Acc: 35.814% (17549/49000)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 35.81428571428572\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.476 | Acc: 42.188% (27/64)\n",
      "Loss: 1.470 | Acc: 42.969% (55/128)\n",
      "Loss: 1.516 | Acc: 42.188% (81/192)\n",
      "Loss: 1.583 | Acc: 39.453% (101/256)\n",
      "Loss: 1.527 | Acc: 41.562% (133/320)\n",
      "Loss: 1.561 | Acc: 40.885% (157/384)\n",
      "Loss: 1.569 | Acc: 40.848% (183/448)\n",
      "Loss: 1.565 | Acc: 41.211% (211/512)\n",
      "Loss: 1.549 | Acc: 42.188% (243/576)\n",
      "Loss: 1.542 | Acc: 42.500% (272/640)\n",
      "Loss: 1.555 | Acc: 42.045% (296/704)\n",
      "Loss: 1.577 | Acc: 41.927% (322/768)\n",
      "Loss: 1.571 | Acc: 41.707% (347/832)\n",
      "Loss: 1.561 | Acc: 41.629% (373/896)\n",
      "Loss: 1.559 | Acc: 41.875% (402/960)\n",
      "Loss: 1.549 | Acc: 42.871% (439/1024)\n",
      "Loss: 1.556 | Acc: 42.647% (464/1088)\n",
      "Loss: 1.556 | Acc: 42.795% (493/1152)\n",
      "Loss: 1.553 | Acc: 43.421% (528/1216)\n",
      "Loss: 1.558 | Acc: 43.203% (553/1280)\n",
      "Loss: 1.561 | Acc: 43.006% (578/1344)\n",
      "Loss: 1.559 | Acc: 42.969% (605/1408)\n",
      "Loss: 1.548 | Acc: 43.342% (638/1472)\n",
      "Loss: 1.553 | Acc: 43.294% (665/1536)\n",
      "Loss: 1.552 | Acc: 43.125% (690/1600)\n",
      "Loss: 1.557 | Acc: 42.969% (715/1664)\n",
      "Loss: 1.558 | Acc: 43.229% (747/1728)\n",
      "Loss: 1.556 | Acc: 43.248% (775/1792)\n",
      "Loss: 1.552 | Acc: 43.588% (809/1856)\n",
      "Loss: 1.553 | Acc: 43.542% (836/1920)\n",
      "Loss: 1.556 | Acc: 43.498% (863/1984)\n",
      "Loss: 1.558 | Acc: 43.555% (892/2048)\n",
      "Loss: 1.553 | Acc: 43.845% (926/2112)\n",
      "Loss: 1.553 | Acc: 44.026% (958/2176)\n",
      "Loss: 1.555 | Acc: 43.973% (985/2240)\n",
      "Loss: 1.553 | Acc: 44.054% (1015/2304)\n",
      "Loss: 1.555 | Acc: 43.834% (1038/2368)\n",
      "Loss: 1.556 | Acc: 43.791% (1065/2432)\n",
      "Loss: 1.551 | Acc: 43.910% (1096/2496)\n",
      "Loss: 1.561 | Acc: 43.711% (1119/2560)\n",
      "Loss: 1.566 | Acc: 43.559% (1143/2624)\n",
      "Loss: 1.565 | Acc: 43.638% (1173/2688)\n",
      "Loss: 1.563 | Acc: 43.641% (1201/2752)\n",
      "Loss: 1.565 | Acc: 43.501% (1225/2816)\n",
      "Loss: 1.565 | Acc: 43.507% (1253/2880)\n",
      "Loss: 1.564 | Acc: 43.716% (1287/2944)\n",
      "Loss: 1.566 | Acc: 43.551% (1310/3008)\n",
      "Loss: 1.566 | Acc: 43.587% (1339/3072)\n",
      "Loss: 1.563 | Acc: 43.718% (1371/3136)\n",
      "Loss: 1.561 | Acc: 43.781% (1401/3200)\n",
      "Loss: 1.560 | Acc: 43.781% (1429/3264)\n",
      "Loss: 1.563 | Acc: 43.630% (1452/3328)\n",
      "Loss: 1.562 | Acc: 43.691% (1482/3392)\n",
      "Loss: 1.565 | Acc: 43.605% (1507/3456)\n",
      "Loss: 1.565 | Acc: 43.494% (1531/3520)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.562 | Acc: 43.694% (1566/3584)\n",
      "Loss: 1.561 | Acc: 43.750% (1596/3648)\n",
      "Loss: 1.557 | Acc: 43.912% (1630/3712)\n",
      "Loss: 1.558 | Acc: 43.829% (1655/3776)\n",
      "Loss: 1.557 | Acc: 43.854% (1684/3840)\n",
      "Loss: 1.555 | Acc: 43.878% (1713/3904)\n",
      "Loss: 1.554 | Acc: 44.052% (1748/3968)\n",
      "Loss: 1.557 | Acc: 43.973% (1773/4032)\n",
      "Loss: 1.557 | Acc: 43.823% (1795/4096)\n",
      "Loss: 1.559 | Acc: 43.918% (1827/4160)\n",
      "Loss: 1.558 | Acc: 43.987% (1858/4224)\n",
      "Loss: 1.559 | Acc: 43.890% (1882/4288)\n",
      "Loss: 1.556 | Acc: 43.934% (1912/4352)\n",
      "Loss: 1.554 | Acc: 43.999% (1943/4416)\n",
      "Loss: 1.554 | Acc: 43.996% (1971/4480)\n",
      "Loss: 1.552 | Acc: 44.036% (2001/4544)\n",
      "Loss: 1.554 | Acc: 44.054% (2030/4608)\n",
      "Loss: 1.552 | Acc: 44.157% (2063/4672)\n",
      "Loss: 1.550 | Acc: 44.151% (2091/4736)\n",
      "Loss: 1.551 | Acc: 44.125% (2118/4800)\n",
      "Loss: 1.548 | Acc: 44.223% (2151/4864)\n",
      "Loss: 1.546 | Acc: 44.318% (2184/4928)\n",
      "Loss: 1.546 | Acc: 44.231% (2208/4992)\n",
      "Loss: 1.545 | Acc: 44.324% (2241/5056)\n",
      "Loss: 1.545 | Acc: 44.277% (2267/5120)\n",
      "Loss: 1.545 | Acc: 44.271% (2295/5184)\n",
      "Loss: 1.545 | Acc: 44.207% (2320/5248)\n",
      "Loss: 1.545 | Acc: 44.202% (2348/5312)\n",
      "Loss: 1.548 | Acc: 44.122% (2372/5376)\n",
      "Loss: 1.549 | Acc: 44.062% (2397/5440)\n",
      "Loss: 1.550 | Acc: 44.059% (2425/5504)\n",
      "Loss: 1.551 | Acc: 44.019% (2451/5568)\n",
      "Loss: 1.553 | Acc: 44.052% (2481/5632)\n",
      "Loss: 1.553 | Acc: 44.136% (2514/5696)\n",
      "Loss: 1.551 | Acc: 44.219% (2547/5760)\n",
      "Loss: 1.548 | Acc: 44.282% (2579/5824)\n",
      "Loss: 1.550 | Acc: 44.175% (2601/5888)\n",
      "Loss: 1.552 | Acc: 44.204% (2631/5952)\n",
      "Loss: 1.553 | Acc: 44.132% (2655/6016)\n",
      "Loss: 1.554 | Acc: 44.145% (2684/6080)\n",
      "Loss: 1.552 | Acc: 44.222% (2717/6144)\n",
      "Loss: 1.554 | Acc: 44.153% (2741/6208)\n",
      "Loss: 1.555 | Acc: 44.133% (2768/6272)\n",
      "Loss: 1.553 | Acc: 44.255% (2804/6336)\n",
      "Loss: 1.552 | Acc: 44.266% (2833/6400)\n",
      "Loss: 1.554 | Acc: 44.183% (2856/6464)\n",
      "Loss: 1.554 | Acc: 44.179% (2884/6528)\n",
      "Loss: 1.555 | Acc: 44.129% (2909/6592)\n",
      "Loss: 1.554 | Acc: 44.066% (2933/6656)\n",
      "Loss: 1.553 | Acc: 44.107% (2964/6720)\n",
      "Loss: 1.551 | Acc: 44.207% (2999/6784)\n",
      "Loss: 1.550 | Acc: 44.276% (3032/6848)\n",
      "Loss: 1.553 | Acc: 44.170% (3053/6912)\n",
      "Loss: 1.554 | Acc: 44.123% (3078/6976)\n",
      "Loss: 1.555 | Acc: 44.119% (3106/7040)\n",
      "Loss: 1.556 | Acc: 44.046% (3129/7104)\n",
      "Loss: 1.555 | Acc: 44.099% (3161/7168)\n",
      "Loss: 1.556 | Acc: 44.027% (3184/7232)\n",
      "Loss: 1.553 | Acc: 44.065% (3215/7296)\n",
      "Loss: 1.552 | Acc: 44.130% (3248/7360)\n",
      "Loss: 1.553 | Acc: 44.073% (3272/7424)\n",
      "Loss: 1.550 | Acc: 44.124% (3304/7488)\n",
      "Loss: 1.548 | Acc: 44.240% (3341/7552)\n",
      "Loss: 1.549 | Acc: 44.210% (3367/7616)\n",
      "Loss: 1.548 | Acc: 44.271% (3400/7680)\n",
      "Loss: 1.546 | Acc: 44.357% (3435/7744)\n",
      "Loss: 1.544 | Acc: 44.454% (3471/7808)\n",
      "Loss: 1.545 | Acc: 44.372% (3493/7872)\n",
      "Loss: 1.544 | Acc: 44.380% (3522/7936)\n",
      "Loss: 1.545 | Acc: 44.325% (3546/8000)\n",
      "Loss: 1.547 | Acc: 44.296% (3572/8064)\n",
      "Loss: 1.547 | Acc: 44.341% (3604/8128)\n",
      "Loss: 1.546 | Acc: 44.385% (3636/8192)\n",
      "Loss: 1.547 | Acc: 44.344% (3661/8256)\n",
      "Loss: 1.547 | Acc: 44.351% (3690/8320)\n",
      "Loss: 1.548 | Acc: 44.263% (3711/8384)\n",
      "Loss: 1.548 | Acc: 44.223% (3736/8448)\n",
      "Loss: 1.549 | Acc: 44.196% (3762/8512)\n",
      "Loss: 1.550 | Acc: 44.181% (3789/8576)\n",
      "Loss: 1.549 | Acc: 44.190% (3818/8640)\n",
      "Loss: 1.550 | Acc: 44.175% (3845/8704)\n",
      "Loss: 1.551 | Acc: 44.058% (3863/8768)\n",
      "Loss: 1.552 | Acc: 44.022% (3888/8832)\n",
      "Loss: 1.551 | Acc: 44.065% (3920/8896)\n",
      "Loss: 1.551 | Acc: 44.062% (3948/8960)\n",
      "Loss: 1.551 | Acc: 44.094% (3979/9024)\n",
      "Loss: 1.552 | Acc: 44.069% (4005/9088)\n",
      "Loss: 1.551 | Acc: 44.078% (4034/9152)\n",
      "Loss: 1.549 | Acc: 44.173% (4071/9216)\n",
      "Loss: 1.547 | Acc: 44.224% (4104/9280)\n",
      "Loss: 1.547 | Acc: 44.221% (4132/9344)\n",
      "Loss: 1.548 | Acc: 44.228% (4161/9408)\n",
      "Loss: 1.549 | Acc: 44.162% (4183/9472)\n",
      "Loss: 1.549 | Acc: 44.243% (4219/9536)\n",
      "Loss: 1.548 | Acc: 44.260% (4249/9600)\n",
      "Loss: 1.549 | Acc: 44.247% (4276/9664)\n",
      "Loss: 1.551 | Acc: 44.223% (4302/9728)\n",
      "Loss: 1.551 | Acc: 44.158% (4324/9792)\n",
      "Loss: 1.550 | Acc: 44.196% (4356/9856)\n",
      "Loss: 1.550 | Acc: 44.163% (4381/9920)\n",
      "Loss: 1.550 | Acc: 44.111% (4404/9984)\n",
      "Loss: 1.549 | Acc: 44.130% (4413/10000)\n",
      "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 44.13\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.518 | Acc: 51.562% (33/64)\n",
      "Loss: 1.446 | Acc: 48.438% (62/128)\n",
      "Loss: 1.512 | Acc: 44.271% (85/192)\n",
      "Loss: 1.525 | Acc: 43.359% (111/256)\n",
      "Loss: 1.580 | Acc: 42.500% (136/320)\n",
      "Loss: 1.539 | Acc: 44.010% (169/384)\n",
      "Loss: 1.540 | Acc: 43.527% (195/448)\n",
      "Loss: 1.509 | Acc: 44.727% (229/512)\n",
      "Loss: 1.526 | Acc: 44.444% (256/576)\n",
      "Loss: 1.523 | Acc: 44.375% (284/640)\n",
      "Loss: 1.535 | Acc: 44.886% (316/704)\n",
      "Loss: 1.541 | Acc: 44.271% (340/768)\n",
      "Loss: 1.544 | Acc: 43.389% (361/832)\n",
      "Loss: 1.531 | Acc: 43.638% (391/896)\n",
      "Loss: 1.530 | Acc: 44.167% (424/960)\n",
      "Loss: 1.528 | Acc: 44.531% (456/1024)\n",
      "Loss: 1.531 | Acc: 44.026% (479/1088)\n",
      "Loss: 1.528 | Acc: 44.010% (507/1152)\n",
      "Loss: 1.526 | Acc: 44.572% (542/1216)\n",
      "Loss: 1.517 | Acc: 44.766% (573/1280)\n",
      "Loss: 1.517 | Acc: 44.643% (600/1344)\n",
      "Loss: 1.518 | Acc: 44.318% (624/1408)\n",
      "Loss: 1.514 | Acc: 44.565% (656/1472)\n",
      "Loss: 1.509 | Acc: 44.727% (687/1536)\n",
      "Loss: 1.505 | Acc: 44.625% (714/1600)\n",
      "Loss: 1.511 | Acc: 44.712% (744/1664)\n",
      "Loss: 1.521 | Acc: 44.502% (769/1728)\n",
      "Loss: 1.525 | Acc: 44.866% (804/1792)\n",
      "Loss: 1.523 | Acc: 45.097% (837/1856)\n",
      "Loss: 1.521 | Acc: 45.052% (865/1920)\n",
      "Loss: 1.525 | Acc: 44.758% (888/1984)\n",
      "Loss: 1.532 | Acc: 44.629% (914/2048)\n",
      "Loss: 1.535 | Acc: 44.413% (938/2112)\n",
      "Loss: 1.537 | Acc: 44.256% (963/2176)\n",
      "Loss: 1.537 | Acc: 44.196% (990/2240)\n",
      "Loss: 1.545 | Acc: 43.924% (1012/2304)\n",
      "Loss: 1.542 | Acc: 44.003% (1042/2368)\n",
      "Loss: 1.540 | Acc: 44.202% (1075/2432)\n",
      "Loss: 1.546 | Acc: 44.151% (1102/2496)\n",
      "Loss: 1.548 | Acc: 43.906% (1124/2560)\n",
      "Loss: 1.542 | Acc: 44.322% (1163/2624)\n",
      "Loss: 1.540 | Acc: 44.457% (1195/2688)\n",
      "Loss: 1.540 | Acc: 44.259% (1218/2752)\n",
      "Loss: 1.541 | Acc: 44.141% (1243/2816)\n",
      "Loss: 1.538 | Acc: 44.167% (1272/2880)\n",
      "Loss: 1.536 | Acc: 44.192% (1301/2944)\n",
      "Loss: 1.533 | Acc: 44.182% (1329/3008)\n",
      "Loss: 1.536 | Acc: 44.271% (1360/3072)\n",
      "Loss: 1.533 | Acc: 44.420% (1393/3136)\n",
      "Loss: 1.532 | Acc: 44.531% (1425/3200)\n",
      "Loss: 1.532 | Acc: 44.608% (1456/3264)\n",
      "Loss: 1.534 | Acc: 44.561% (1483/3328)\n",
      "Loss: 1.533 | Acc: 44.634% (1514/3392)\n",
      "Loss: 1.533 | Acc: 44.618% (1542/3456)\n",
      "Loss: 1.532 | Acc: 44.716% (1574/3520)\n",
      "Loss: 1.533 | Acc: 44.699% (1602/3584)\n",
      "Loss: 1.536 | Acc: 44.627% (1628/3648)\n",
      "Loss: 1.537 | Acc: 44.747% (1661/3712)\n",
      "Loss: 1.534 | Acc: 44.783% (1691/3776)\n",
      "Loss: 1.534 | Acc: 44.661% (1715/3840)\n",
      "Loss: 1.534 | Acc: 44.595% (1741/3904)\n",
      "Loss: 1.531 | Acc: 44.733% (1775/3968)\n",
      "Loss: 1.528 | Acc: 44.940% (1812/4032)\n",
      "Loss: 1.529 | Acc: 44.873% (1838/4096)\n",
      "Loss: 1.528 | Acc: 44.880% (1867/4160)\n",
      "Loss: 1.529 | Acc: 44.744% (1890/4224)\n",
      "Loss: 1.528 | Acc: 44.776% (1920/4288)\n",
      "Loss: 1.529 | Acc: 44.738% (1947/4352)\n",
      "Loss: 1.526 | Acc: 44.905% (1983/4416)\n",
      "Loss: 1.526 | Acc: 44.844% (2009/4480)\n",
      "Loss: 1.530 | Acc: 44.630% (2028/4544)\n",
      "Loss: 1.530 | Acc: 44.705% (2060/4608)\n",
      "Loss: 1.528 | Acc: 44.735% (2090/4672)\n",
      "Loss: 1.528 | Acc: 44.721% (2118/4736)\n",
      "Loss: 1.526 | Acc: 44.667% (2144/4800)\n",
      "Loss: 1.526 | Acc: 44.716% (2175/4864)\n",
      "Loss: 1.523 | Acc: 44.825% (2209/4928)\n",
      "Loss: 1.522 | Acc: 44.812% (2237/4992)\n",
      "Loss: 1.523 | Acc: 44.858% (2268/5056)\n",
      "Loss: 1.520 | Acc: 44.941% (2301/5120)\n",
      "Loss: 1.520 | Acc: 44.907% (2328/5184)\n",
      "Loss: 1.522 | Acc: 44.893% (2356/5248)\n",
      "Loss: 1.522 | Acc: 44.880% (2384/5312)\n",
      "Loss: 1.524 | Acc: 44.885% (2413/5376)\n",
      "Loss: 1.526 | Acc: 44.779% (2436/5440)\n",
      "Loss: 1.525 | Acc: 44.804% (2466/5504)\n",
      "Loss: 1.524 | Acc: 44.881% (2499/5568)\n",
      "Loss: 1.522 | Acc: 44.904% (2529/5632)\n",
      "Loss: 1.524 | Acc: 44.926% (2559/5696)\n",
      "Loss: 1.523 | Acc: 44.983% (2591/5760)\n",
      "Loss: 1.524 | Acc: 44.935% (2617/5824)\n",
      "Loss: 1.523 | Acc: 44.939% (2646/5888)\n",
      "Loss: 1.522 | Acc: 45.044% (2681/5952)\n",
      "Loss: 1.523 | Acc: 45.063% (2711/6016)\n",
      "Loss: 1.522 | Acc: 45.132% (2744/6080)\n",
      "Loss: 1.520 | Acc: 45.231% (2779/6144)\n",
      "Loss: 1.521 | Acc: 45.184% (2805/6208)\n",
      "Loss: 1.521 | Acc: 45.217% (2836/6272)\n",
      "Loss: 1.521 | Acc: 45.218% (2865/6336)\n",
      "Loss: 1.520 | Acc: 45.297% (2899/6400)\n",
      "Loss: 1.520 | Acc: 45.312% (2929/6464)\n",
      "Loss: 1.519 | Acc: 45.312% (2958/6528)\n",
      "Loss: 1.520 | Acc: 45.130% (2975/6592)\n",
      "Loss: 1.521 | Acc: 45.042% (2998/6656)\n",
      "Loss: 1.521 | Acc: 45.030% (3026/6720)\n",
      "Loss: 1.521 | Acc: 45.062% (3057/6784)\n",
      "Loss: 1.519 | Acc: 45.152% (3092/6848)\n",
      "Loss: 1.520 | Acc: 45.197% (3124/6912)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.521 | Acc: 45.183% (3152/6976)\n",
      "Loss: 1.523 | Acc: 45.142% (3178/7040)\n",
      "Loss: 1.522 | Acc: 45.130% (3206/7104)\n",
      "Loss: 1.522 | Acc: 45.159% (3237/7168)\n",
      "Loss: 1.521 | Acc: 45.133% (3264/7232)\n",
      "Loss: 1.523 | Acc: 45.134% (3293/7296)\n",
      "Loss: 1.524 | Acc: 45.136% (3322/7360)\n",
      "Loss: 1.526 | Acc: 45.070% (3346/7424)\n",
      "Loss: 1.524 | Acc: 45.126% (3379/7488)\n",
      "Loss: 1.525 | Acc: 45.087% (3405/7552)\n",
      "Loss: 1.525 | Acc: 45.089% (3434/7616)\n",
      "Loss: 1.523 | Acc: 45.078% (3462/7680)\n",
      "Loss: 1.523 | Acc: 45.145% (3496/7744)\n",
      "Loss: 1.522 | Acc: 45.172% (3527/7808)\n",
      "Loss: 1.523 | Acc: 45.135% (3553/7872)\n",
      "Loss: 1.523 | Acc: 45.048% (3575/7936)\n",
      "Loss: 1.525 | Acc: 44.987% (3599/8000)\n",
      "Loss: 1.525 | Acc: 44.928% (3623/8064)\n",
      "Loss: 1.525 | Acc: 44.906% (3650/8128)\n",
      "Loss: 1.524 | Acc: 44.910% (3679/8192)\n",
      "Loss: 1.523 | Acc: 44.901% (3707/8256)\n",
      "Loss: 1.524 | Acc: 44.808% (3728/8320)\n",
      "Loss: 1.523 | Acc: 44.812% (3757/8384)\n",
      "Loss: 1.524 | Acc: 44.756% (3781/8448)\n",
      "Loss: 1.523 | Acc: 44.807% (3814/8512)\n",
      "Loss: 1.522 | Acc: 44.823% (3844/8576)\n",
      "Loss: 1.522 | Acc: 44.826% (3873/8640)\n",
      "Loss: 1.522 | Acc: 44.773% (3897/8704)\n",
      "Loss: 1.522 | Acc: 44.799% (3928/8768)\n",
      "Loss: 1.522 | Acc: 44.758% (3953/8832)\n",
      "Loss: 1.522 | Acc: 44.694% (3976/8896)\n",
      "Loss: 1.524 | Acc: 44.621% (3998/8960)\n",
      "Loss: 1.526 | Acc: 44.559% (4021/9024)\n",
      "Loss: 1.524 | Acc: 44.652% (4058/9088)\n",
      "Loss: 1.523 | Acc: 44.712% (4092/9152)\n",
      "Loss: 1.524 | Acc: 44.694% (4119/9216)\n",
      "Loss: 1.524 | Acc: 44.709% (4149/9280)\n",
      "Loss: 1.524 | Acc: 44.702% (4177/9344)\n",
      "Loss: 1.524 | Acc: 44.696% (4205/9408)\n",
      "Loss: 1.524 | Acc: 44.711% (4235/9472)\n",
      "Loss: 1.525 | Acc: 44.662% (4259/9536)\n",
      "Loss: 1.525 | Acc: 44.562% (4278/9600)\n",
      "Loss: 1.527 | Acc: 44.505% (4301/9664)\n",
      "Loss: 1.528 | Acc: 44.459% (4325/9728)\n",
      "Loss: 1.528 | Acc: 44.465% (4354/9792)\n",
      "Loss: 1.527 | Acc: 44.501% (4386/9856)\n",
      "Loss: 1.527 | Acc: 44.526% (4417/9920)\n",
      "Loss: 1.527 | Acc: 44.511% (4444/9984)\n",
      "Loss: 1.526 | Acc: 44.596% (4481/10048)\n",
      "Loss: 1.524 | Acc: 44.660% (4516/10112)\n",
      "Loss: 1.524 | Acc: 44.664% (4545/10176)\n",
      "Loss: 1.526 | Acc: 44.688% (4576/10240)\n",
      "Loss: 1.528 | Acc: 44.614% (4597/10304)\n",
      "Loss: 1.527 | Acc: 44.608% (4625/10368)\n",
      "Loss: 1.527 | Acc: 44.584% (4651/10432)\n",
      "Loss: 1.526 | Acc: 44.579% (4679/10496)\n",
      "Loss: 1.527 | Acc: 44.574% (4707/10560)\n",
      "Loss: 1.527 | Acc: 44.559% (4734/10624)\n",
      "Loss: 1.526 | Acc: 44.545% (4761/10688)\n",
      "Loss: 1.525 | Acc: 44.587% (4794/10752)\n",
      "Loss: 1.524 | Acc: 44.619% (4826/10816)\n",
      "Loss: 1.525 | Acc: 44.614% (4854/10880)\n",
      "Loss: 1.525 | Acc: 44.545% (4875/10944)\n",
      "Loss: 1.524 | Acc: 44.568% (4906/11008)\n",
      "Loss: 1.523 | Acc: 44.536% (4931/11072)\n",
      "Loss: 1.524 | Acc: 44.531% (4959/11136)\n",
      "Loss: 1.523 | Acc: 44.616% (4997/11200)\n",
      "Loss: 1.522 | Acc: 44.656% (5030/11264)\n",
      "Loss: 1.522 | Acc: 44.677% (5061/11328)\n",
      "Loss: 1.520 | Acc: 44.742% (5097/11392)\n",
      "Loss: 1.519 | Acc: 44.719% (5123/11456)\n",
      "Loss: 1.520 | Acc: 44.670% (5146/11520)\n",
      "Loss: 1.519 | Acc: 44.682% (5176/11584)\n",
      "Loss: 1.518 | Acc: 44.746% (5212/11648)\n",
      "Loss: 1.517 | Acc: 44.749% (5241/11712)\n",
      "Loss: 1.516 | Acc: 44.828% (5279/11776)\n",
      "Loss: 1.516 | Acc: 44.882% (5314/11840)\n",
      "Loss: 1.514 | Acc: 44.934% (5349/11904)\n",
      "Loss: 1.513 | Acc: 44.945% (5379/11968)\n",
      "Loss: 1.513 | Acc: 44.947% (5408/12032)\n",
      "Loss: 1.512 | Acc: 44.965% (5439/12096)\n",
      "Loss: 1.511 | Acc: 45.008% (5473/12160)\n",
      "Loss: 1.511 | Acc: 44.993% (5500/12224)\n",
      "Loss: 1.510 | Acc: 45.020% (5532/12288)\n",
      "Loss: 1.510 | Acc: 45.013% (5560/12352)\n",
      "Loss: 1.510 | Acc: 44.950% (5581/12416)\n",
      "Loss: 1.510 | Acc: 44.952% (5610/12480)\n",
      "Loss: 1.511 | Acc: 44.938% (5637/12544)\n",
      "Loss: 1.511 | Acc: 44.916% (5663/12608)\n",
      "Loss: 1.511 | Acc: 44.926% (5693/12672)\n",
      "Loss: 1.511 | Acc: 44.912% (5720/12736)\n",
      "Loss: 1.510 | Acc: 44.930% (5751/12800)\n",
      "Loss: 1.511 | Acc: 44.939% (5781/12864)\n",
      "Loss: 1.511 | Acc: 44.910% (5806/12928)\n",
      "Loss: 1.511 | Acc: 44.928% (5837/12992)\n",
      "Loss: 1.511 | Acc: 44.899% (5862/13056)\n",
      "Loss: 1.510 | Acc: 44.962% (5899/13120)\n",
      "Loss: 1.511 | Acc: 44.903% (5920/13184)\n",
      "Loss: 1.511 | Acc: 44.890% (5947/13248)\n",
      "Loss: 1.510 | Acc: 44.907% (5978/13312)\n",
      "Loss: 1.511 | Acc: 44.849% (5999/13376)\n",
      "Loss: 1.511 | Acc: 44.821% (6024/13440)\n",
      "Loss: 1.512 | Acc: 44.794% (6049/13504)\n",
      "Loss: 1.512 | Acc: 44.767% (6074/13568)\n",
      "Loss: 1.512 | Acc: 44.762% (6102/13632)\n",
      "Loss: 1.511 | Acc: 44.809% (6137/13696)\n",
      "Loss: 1.511 | Acc: 44.782% (6162/13760)\n",
      "Loss: 1.513 | Acc: 44.748% (6186/13824)\n",
      "Loss: 1.513 | Acc: 44.751% (6215/13888)\n",
      "Loss: 1.513 | Acc: 44.682% (6234/13952)\n",
      "Loss: 1.514 | Acc: 44.670% (6261/14016)\n",
      "Loss: 1.513 | Acc: 44.730% (6298/14080)\n",
      "Loss: 1.512 | Acc: 44.740% (6328/14144)\n",
      "Loss: 1.511 | Acc: 44.792% (6364/14208)\n",
      "Loss: 1.512 | Acc: 44.780% (6391/14272)\n",
      "Loss: 1.512 | Acc: 44.775% (6419/14336)\n",
      "Loss: 1.512 | Acc: 44.792% (6450/14400)\n",
      "Loss: 1.512 | Acc: 44.759% (6474/14464)\n",
      "Loss: 1.512 | Acc: 44.776% (6505/14528)\n",
      "Loss: 1.512 | Acc: 44.757% (6531/14592)\n",
      "Loss: 1.512 | Acc: 44.726% (6555/14656)\n",
      "Loss: 1.511 | Acc: 44.735% (6585/14720)\n",
      "Loss: 1.511 | Acc: 44.744% (6615/14784)\n",
      "Loss: 1.510 | Acc: 44.747% (6644/14848)\n",
      "Loss: 1.510 | Acc: 44.736% (6671/14912)\n",
      "Loss: 1.510 | Acc: 44.732% (6699/14976)\n",
      "Loss: 1.510 | Acc: 44.701% (6723/15040)\n",
      "Loss: 1.510 | Acc: 44.703% (6752/15104)\n",
      "Loss: 1.509 | Acc: 44.706% (6781/15168)\n",
      "Loss: 1.509 | Acc: 44.715% (6811/15232)\n",
      "Loss: 1.509 | Acc: 44.691% (6836/15296)\n",
      "Loss: 1.509 | Acc: 44.714% (6868/15360)\n",
      "Loss: 1.509 | Acc: 44.742% (6901/15424)\n",
      "Loss: 1.510 | Acc: 44.712% (6925/15488)\n",
      "Loss: 1.510 | Acc: 44.715% (6954/15552)\n",
      "Loss: 1.509 | Acc: 44.743% (6987/15616)\n",
      "Loss: 1.509 | Acc: 44.751% (7017/15680)\n",
      "Loss: 1.508 | Acc: 44.792% (7052/15744)\n",
      "Loss: 1.508 | Acc: 44.806% (7083/15808)\n",
      "Loss: 1.508 | Acc: 44.783% (7108/15872)\n",
      "Loss: 1.509 | Acc: 44.767% (7134/15936)\n",
      "Loss: 1.509 | Acc: 44.769% (7163/16000)\n",
      "Loss: 1.509 | Acc: 44.802% (7197/16064)\n",
      "Loss: 1.510 | Acc: 44.748% (7217/16128)\n",
      "Loss: 1.510 | Acc: 44.744% (7245/16192)\n",
      "Loss: 1.509 | Acc: 44.747% (7274/16256)\n",
      "Loss: 1.510 | Acc: 44.718% (7298/16320)\n",
      "Loss: 1.509 | Acc: 44.769% (7335/16384)\n",
      "Loss: 1.508 | Acc: 44.790% (7367/16448)\n",
      "Loss: 1.507 | Acc: 44.858% (7407/16512)\n",
      "Loss: 1.506 | Acc: 44.878% (7439/16576)\n",
      "Loss: 1.506 | Acc: 44.910% (7473/16640)\n",
      "Loss: 1.506 | Acc: 44.935% (7506/16704)\n",
      "Loss: 1.506 | Acc: 44.913% (7531/16768)\n",
      "Loss: 1.506 | Acc: 44.938% (7564/16832)\n",
      "Loss: 1.505 | Acc: 44.951% (7595/16896)\n",
      "Loss: 1.505 | Acc: 44.976% (7628/16960)\n",
      "Loss: 1.505 | Acc: 44.978% (7657/17024)\n",
      "Loss: 1.505 | Acc: 44.967% (7684/17088)\n",
      "Loss: 1.505 | Acc: 44.939% (7708/17152)\n",
      "Loss: 1.505 | Acc: 44.970% (7742/17216)\n",
      "Loss: 1.504 | Acc: 44.959% (7769/17280)\n",
      "Loss: 1.504 | Acc: 44.949% (7796/17344)\n",
      "Loss: 1.504 | Acc: 44.985% (7831/17408)\n",
      "Loss: 1.503 | Acc: 44.986% (7860/17472)\n",
      "Loss: 1.503 | Acc: 44.970% (7886/17536)\n",
      "Loss: 1.504 | Acc: 44.960% (7913/17600)\n",
      "Loss: 1.504 | Acc: 44.956% (7941/17664)\n",
      "Loss: 1.503 | Acc: 44.985% (7975/17728)\n",
      "Loss: 1.503 | Acc: 44.981% (8003/17792)\n",
      "Loss: 1.502 | Acc: 44.993% (8034/17856)\n",
      "Loss: 1.502 | Acc: 44.978% (8060/17920)\n",
      "Loss: 1.502 | Acc: 45.012% (8095/17984)\n",
      "Loss: 1.501 | Acc: 45.069% (8134/18048)\n",
      "Loss: 1.501 | Acc: 45.075% (8164/18112)\n",
      "Loss: 1.500 | Acc: 45.092% (8196/18176)\n",
      "Loss: 1.500 | Acc: 45.088% (8224/18240)\n",
      "Loss: 1.500 | Acc: 45.121% (8259/18304)\n",
      "Loss: 1.499 | Acc: 45.138% (8291/18368)\n",
      "Loss: 1.499 | Acc: 45.161% (8324/18432)\n",
      "Loss: 1.500 | Acc: 45.139% (8349/18496)\n",
      "Loss: 1.499 | Acc: 45.167% (8383/18560)\n",
      "Loss: 1.500 | Acc: 45.162% (8411/18624)\n",
      "Loss: 1.500 | Acc: 45.141% (8436/18688)\n",
      "Loss: 1.500 | Acc: 45.142% (8465/18752)\n",
      "Loss: 1.500 | Acc: 45.142% (8494/18816)\n",
      "Loss: 1.499 | Acc: 45.127% (8520/18880)\n",
      "Loss: 1.499 | Acc: 45.101% (8544/18944)\n",
      "Loss: 1.500 | Acc: 45.081% (8569/19008)\n",
      "Loss: 1.500 | Acc: 45.092% (8600/19072)\n",
      "Loss: 1.500 | Acc: 45.098% (8630/19136)\n",
      "Loss: 1.500 | Acc: 45.099% (8659/19200)\n",
      "Loss: 1.501 | Acc: 45.110% (8690/19264)\n",
      "Loss: 1.501 | Acc: 45.095% (8716/19328)\n",
      "Loss: 1.501 | Acc: 45.091% (8744/19392)\n",
      "Loss: 1.500 | Acc: 45.117% (8778/19456)\n",
      "Loss: 1.501 | Acc: 45.133% (8810/19520)\n",
      "Loss: 1.501 | Acc: 45.108% (8834/19584)\n",
      "Loss: 1.501 | Acc: 45.089% (8859/19648)\n",
      "Loss: 1.501 | Acc: 45.089% (8888/19712)\n",
      "Loss: 1.501 | Acc: 45.090% (8917/19776)\n",
      "Loss: 1.501 | Acc: 45.116% (8951/19840)\n",
      "Loss: 1.500 | Acc: 45.132% (8983/19904)\n",
      "Loss: 1.499 | Acc: 45.162% (9018/19968)\n",
      "Loss: 1.499 | Acc: 45.148% (9044/20032)\n",
      "Loss: 1.500 | Acc: 45.138% (9071/20096)\n",
      "Loss: 1.500 | Acc: 45.114% (9095/20160)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.501 | Acc: 45.125% (9126/20224)\n",
      "Loss: 1.500 | Acc: 45.145% (9159/20288)\n",
      "Loss: 1.499 | Acc: 45.170% (9193/20352)\n",
      "Loss: 1.500 | Acc: 45.161% (9220/20416)\n",
      "Loss: 1.499 | Acc: 45.156% (9248/20480)\n",
      "Loss: 1.499 | Acc: 45.181% (9282/20544)\n",
      "Loss: 1.498 | Acc: 45.191% (9313/20608)\n",
      "Loss: 1.498 | Acc: 45.177% (9339/20672)\n",
      "Loss: 1.498 | Acc: 45.153% (9363/20736)\n",
      "Loss: 1.497 | Acc: 45.144% (9390/20800)\n",
      "Loss: 1.497 | Acc: 45.178% (9426/20864)\n",
      "Loss: 1.498 | Acc: 45.179% (9455/20928)\n",
      "Loss: 1.498 | Acc: 45.184% (9485/20992)\n",
      "Loss: 1.498 | Acc: 45.180% (9513/21056)\n",
      "Loss: 1.498 | Acc: 45.189% (9544/21120)\n",
      "Loss: 1.498 | Acc: 45.171% (9569/21184)\n",
      "Loss: 1.498 | Acc: 45.171% (9598/21248)\n",
      "Loss: 1.498 | Acc: 45.167% (9626/21312)\n",
      "Loss: 1.499 | Acc: 45.153% (9652/21376)\n",
      "Loss: 1.499 | Acc: 45.149% (9680/21440)\n",
      "Loss: 1.499 | Acc: 45.159% (9711/21504)\n",
      "Loss: 1.498 | Acc: 45.192% (9747/21568)\n",
      "Loss: 1.498 | Acc: 45.229% (9784/21632)\n",
      "Loss: 1.498 | Acc: 45.225% (9812/21696)\n",
      "Loss: 1.497 | Acc: 45.257% (9848/21760)\n",
      "Loss: 1.497 | Acc: 45.248% (9875/21824)\n",
      "Loss: 1.496 | Acc: 45.281% (9911/21888)\n",
      "Loss: 1.496 | Acc: 45.285% (9941/21952)\n",
      "Loss: 1.496 | Acc: 45.258% (9964/22016)\n",
      "Loss: 1.496 | Acc: 45.263% (9994/22080)\n",
      "Loss: 1.495 | Acc: 45.272% (10025/22144)\n",
      "Loss: 1.495 | Acc: 45.258% (10051/22208)\n",
      "Loss: 1.496 | Acc: 45.241% (10076/22272)\n",
      "Loss: 1.496 | Acc: 45.205% (10097/22336)\n",
      "Loss: 1.495 | Acc: 45.246% (10135/22400)\n",
      "Loss: 1.495 | Acc: 45.259% (10167/22464)\n",
      "Loss: 1.495 | Acc: 45.233% (10190/22528)\n",
      "Loss: 1.495 | Acc: 45.255% (10224/22592)\n",
      "Loss: 1.495 | Acc: 45.282% (10259/22656)\n",
      "Loss: 1.494 | Acc: 45.321% (10297/22720)\n",
      "Loss: 1.494 | Acc: 45.299% (10321/22784)\n",
      "Loss: 1.493 | Acc: 45.312% (10353/22848)\n",
      "Loss: 1.494 | Acc: 45.299% (10379/22912)\n",
      "Loss: 1.494 | Acc: 45.304% (10409/22976)\n",
      "Loss: 1.494 | Acc: 45.317% (10441/23040)\n",
      "Loss: 1.494 | Acc: 45.304% (10467/23104)\n",
      "Loss: 1.494 | Acc: 45.325% (10501/23168)\n",
      "Loss: 1.493 | Acc: 45.351% (10536/23232)\n",
      "Loss: 1.492 | Acc: 45.347% (10564/23296)\n",
      "Loss: 1.493 | Acc: 45.334% (10590/23360)\n",
      "Loss: 1.493 | Acc: 45.325% (10617/23424)\n",
      "Loss: 1.493 | Acc: 45.312% (10643/23488)\n",
      "Loss: 1.494 | Acc: 45.296% (10668/23552)\n",
      "Loss: 1.493 | Acc: 45.296% (10697/23616)\n",
      "Loss: 1.494 | Acc: 45.283% (10723/23680)\n",
      "Loss: 1.494 | Acc: 45.287% (10753/23744)\n",
      "Loss: 1.494 | Acc: 45.296% (10784/23808)\n",
      "Loss: 1.494 | Acc: 45.292% (10812/23872)\n",
      "Loss: 1.495 | Acc: 45.287% (10840/23936)\n",
      "Loss: 1.494 | Acc: 45.304% (10873/24000)\n",
      "Loss: 1.494 | Acc: 45.300% (10901/24064)\n",
      "Loss: 1.494 | Acc: 45.292% (10928/24128)\n",
      "Loss: 1.494 | Acc: 45.288% (10956/24192)\n",
      "Loss: 1.494 | Acc: 45.296% (10987/24256)\n",
      "Loss: 1.493 | Acc: 45.300% (11017/24320)\n",
      "Loss: 1.493 | Acc: 45.300% (11046/24384)\n",
      "Loss: 1.493 | Acc: 45.312% (11078/24448)\n",
      "Loss: 1.493 | Acc: 45.317% (11108/24512)\n",
      "Loss: 1.493 | Acc: 45.288% (11130/24576)\n",
      "Loss: 1.493 | Acc: 45.296% (11161/24640)\n",
      "Loss: 1.493 | Acc: 45.308% (11193/24704)\n",
      "Loss: 1.494 | Acc: 45.288% (11217/24768)\n",
      "Loss: 1.494 | Acc: 45.260% (11239/24832)\n",
      "Loss: 1.494 | Acc: 45.260% (11268/24896)\n",
      "Loss: 1.494 | Acc: 45.272% (11300/24960)\n",
      "Loss: 1.494 | Acc: 45.257% (11325/25024)\n",
      "Loss: 1.493 | Acc: 45.289% (11362/25088)\n",
      "Loss: 1.493 | Acc: 45.305% (11395/25152)\n",
      "Loss: 1.493 | Acc: 45.312% (11426/25216)\n",
      "Loss: 1.493 | Acc: 45.309% (11454/25280)\n",
      "Loss: 1.493 | Acc: 45.316% (11485/25344)\n",
      "Loss: 1.493 | Acc: 45.305% (11511/25408)\n",
      "Loss: 1.493 | Acc: 45.289% (11536/25472)\n",
      "Loss: 1.493 | Acc: 45.297% (11567/25536)\n",
      "Loss: 1.493 | Acc: 45.273% (11590/25600)\n",
      "Loss: 1.493 | Acc: 45.281% (11621/25664)\n",
      "Loss: 1.493 | Acc: 45.266% (11646/25728)\n",
      "Loss: 1.493 | Acc: 45.285% (11680/25792)\n",
      "Loss: 1.492 | Acc: 45.289% (11710/25856)\n",
      "Loss: 1.492 | Acc: 45.297% (11741/25920)\n",
      "Loss: 1.492 | Acc: 45.301% (11771/25984)\n",
      "Loss: 1.492 | Acc: 45.316% (11804/26048)\n",
      "Loss: 1.492 | Acc: 45.309% (11831/26112)\n",
      "Loss: 1.492 | Acc: 45.297% (11857/26176)\n",
      "Loss: 1.491 | Acc: 45.301% (11887/26240)\n",
      "Loss: 1.491 | Acc: 45.290% (11913/26304)\n",
      "Loss: 1.491 | Acc: 45.301% (11945/26368)\n",
      "Loss: 1.491 | Acc: 45.278% (11968/26432)\n",
      "Loss: 1.491 | Acc: 45.286% (11999/26496)\n",
      "Loss: 1.490 | Acc: 45.312% (12035/26560)\n",
      "Loss: 1.490 | Acc: 45.335% (12070/26624)\n",
      "Loss: 1.490 | Acc: 45.327% (12097/26688)\n",
      "Loss: 1.490 | Acc: 45.339% (12129/26752)\n",
      "Loss: 1.490 | Acc: 45.376% (12168/26816)\n",
      "Loss: 1.489 | Acc: 45.402% (12204/26880)\n",
      "Loss: 1.489 | Acc: 45.409% (12235/26944)\n",
      "Loss: 1.489 | Acc: 45.420% (12267/27008)\n",
      "Loss: 1.489 | Acc: 45.423% (12297/27072)\n",
      "Loss: 1.489 | Acc: 45.438% (12330/27136)\n",
      "Loss: 1.489 | Acc: 45.408% (12351/27200)\n",
      "Loss: 1.489 | Acc: 45.426% (12385/27264)\n",
      "Loss: 1.488 | Acc: 45.437% (12417/27328)\n",
      "Loss: 1.488 | Acc: 45.459% (12452/27392)\n",
      "Loss: 1.488 | Acc: 45.469% (12484/27456)\n",
      "Loss: 1.488 | Acc: 45.480% (12516/27520)\n",
      "Loss: 1.487 | Acc: 45.490% (12548/27584)\n",
      "Loss: 1.487 | Acc: 45.511% (12583/27648)\n",
      "Loss: 1.487 | Acc: 45.515% (12613/27712)\n",
      "Loss: 1.487 | Acc: 45.511% (12641/27776)\n",
      "Loss: 1.488 | Acc: 45.485% (12663/27840)\n",
      "Loss: 1.488 | Acc: 45.481% (12691/27904)\n",
      "Loss: 1.487 | Acc: 45.523% (12732/27968)\n",
      "Loss: 1.487 | Acc: 45.530% (12763/28032)\n",
      "Loss: 1.487 | Acc: 45.537% (12794/28096)\n",
      "Loss: 1.486 | Acc: 45.575% (12834/28160)\n",
      "Loss: 1.486 | Acc: 45.582% (12865/28224)\n",
      "Loss: 1.486 | Acc: 45.599% (12899/28288)\n",
      "Loss: 1.486 | Acc: 45.609% (12931/28352)\n",
      "Loss: 1.485 | Acc: 45.615% (12962/28416)\n",
      "Loss: 1.485 | Acc: 45.614% (12991/28480)\n",
      "Loss: 1.485 | Acc: 45.603% (13017/28544)\n",
      "Loss: 1.485 | Acc: 45.610% (13048/28608)\n",
      "Loss: 1.484 | Acc: 45.619% (13080/28672)\n",
      "Loss: 1.484 | Acc: 45.633% (13113/28736)\n",
      "Loss: 1.484 | Acc: 45.635% (13143/28800)\n",
      "Loss: 1.484 | Acc: 45.621% (13168/28864)\n",
      "Loss: 1.484 | Acc: 45.634% (13201/28928)\n",
      "Loss: 1.484 | Acc: 45.654% (13236/28992)\n",
      "Loss: 1.484 | Acc: 45.643% (13262/29056)\n",
      "Loss: 1.484 | Acc: 45.642% (13291/29120)\n",
      "Loss: 1.484 | Acc: 45.641% (13320/29184)\n",
      "Loss: 1.484 | Acc: 45.630% (13346/29248)\n",
      "Loss: 1.484 | Acc: 45.647% (13380/29312)\n",
      "Loss: 1.484 | Acc: 45.650% (13410/29376)\n",
      "Loss: 1.484 | Acc: 45.642% (13437/29440)\n",
      "Loss: 1.484 | Acc: 45.665% (13473/29504)\n",
      "Loss: 1.484 | Acc: 45.671% (13504/29568)\n",
      "Loss: 1.483 | Acc: 45.690% (13539/29632)\n",
      "Loss: 1.483 | Acc: 45.696% (13570/29696)\n",
      "Loss: 1.483 | Acc: 45.692% (13598/29760)\n",
      "Loss: 1.483 | Acc: 45.701% (13630/29824)\n",
      "Loss: 1.483 | Acc: 45.727% (13667/29888)\n",
      "Loss: 1.482 | Acc: 45.747% (13702/29952)\n",
      "Loss: 1.483 | Acc: 45.746% (13731/30016)\n",
      "Loss: 1.483 | Acc: 45.721% (13753/30080)\n",
      "Loss: 1.483 | Acc: 45.737% (13787/30144)\n",
      "Loss: 1.482 | Acc: 45.763% (13824/30208)\n",
      "Loss: 1.481 | Acc: 45.825% (13872/30272)\n",
      "Loss: 1.481 | Acc: 45.837% (13905/30336)\n",
      "Loss: 1.481 | Acc: 45.836% (13934/30400)\n",
      "Loss: 1.480 | Acc: 45.861% (13971/30464)\n",
      "Loss: 1.480 | Acc: 45.860% (14000/30528)\n",
      "Loss: 1.480 | Acc: 45.849% (14026/30592)\n",
      "Loss: 1.480 | Acc: 45.854% (14057/30656)\n",
      "Loss: 1.479 | Acc: 45.866% (14090/30720)\n",
      "Loss: 1.479 | Acc: 45.865% (14119/30784)\n",
      "Loss: 1.479 | Acc: 45.854% (14145/30848)\n",
      "Loss: 1.480 | Acc: 45.843% (14171/30912)\n",
      "Loss: 1.480 | Acc: 45.823% (14194/30976)\n",
      "Loss: 1.481 | Acc: 45.799% (14216/31040)\n",
      "Loss: 1.481 | Acc: 45.811% (14249/31104)\n",
      "Loss: 1.481 | Acc: 45.816% (14280/31168)\n",
      "Loss: 1.481 | Acc: 45.818% (14310/31232)\n",
      "Loss: 1.480 | Acc: 45.830% (14343/31296)\n",
      "Loss: 1.480 | Acc: 45.855% (14380/31360)\n",
      "Loss: 1.480 | Acc: 45.876% (14416/31424)\n",
      "Loss: 1.480 | Acc: 45.871% (14444/31488)\n",
      "Loss: 1.480 | Acc: 45.889% (14479/31552)\n",
      "Loss: 1.480 | Acc: 45.891% (14509/31616)\n",
      "Loss: 1.480 | Acc: 45.893% (14539/31680)\n",
      "Loss: 1.479 | Acc: 45.914% (14575/31744)\n",
      "Loss: 1.480 | Acc: 45.904% (14601/31808)\n",
      "Loss: 1.480 | Acc: 45.924% (14637/31872)\n",
      "Loss: 1.479 | Acc: 45.942% (14672/31936)\n",
      "Loss: 1.479 | Acc: 45.947% (14703/32000)\n",
      "Loss: 1.479 | Acc: 45.946% (14732/32064)\n",
      "Loss: 1.479 | Acc: 45.947% (14762/32128)\n",
      "Loss: 1.478 | Acc: 45.974% (14800/32192)\n",
      "Loss: 1.478 | Acc: 45.976% (14830/32256)\n",
      "Loss: 1.478 | Acc: 45.975% (14859/32320)\n",
      "Loss: 1.478 | Acc: 45.979% (14890/32384)\n",
      "Loss: 1.478 | Acc: 45.954% (14911/32448)\n",
      "Loss: 1.478 | Acc: 45.937% (14935/32512)\n",
      "Loss: 1.478 | Acc: 45.945% (14967/32576)\n",
      "Loss: 1.478 | Acc: 45.941% (14995/32640)\n",
      "Loss: 1.478 | Acc: 45.927% (15020/32704)\n",
      "Loss: 1.478 | Acc: 45.929% (15050/32768)\n",
      "Loss: 1.479 | Acc: 45.919% (15076/32832)\n",
      "Loss: 1.479 | Acc: 45.930% (15109/32896)\n",
      "Loss: 1.478 | Acc: 45.941% (15142/32960)\n",
      "Loss: 1.479 | Acc: 45.933% (15169/33024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.478 | Acc: 45.944% (15202/33088)\n",
      "Loss: 1.478 | Acc: 45.952% (15234/33152)\n",
      "Loss: 1.478 | Acc: 45.960% (15266/33216)\n",
      "Loss: 1.478 | Acc: 45.974% (15300/33280)\n",
      "Loss: 1.478 | Acc: 45.978% (15331/33344)\n",
      "Loss: 1.477 | Acc: 45.986% (15363/33408)\n",
      "Loss: 1.477 | Acc: 45.988% (15393/33472)\n",
      "Loss: 1.477 | Acc: 45.980% (15420/33536)\n",
      "Loss: 1.477 | Acc: 45.973% (15447/33600)\n",
      "Loss: 1.477 | Acc: 45.963% (15473/33664)\n",
      "Loss: 1.477 | Acc: 45.971% (15505/33728)\n",
      "Loss: 1.477 | Acc: 45.969% (15534/33792)\n",
      "Loss: 1.476 | Acc: 45.986% (15569/33856)\n",
      "Loss: 1.476 | Acc: 45.994% (15601/33920)\n",
      "Loss: 1.476 | Acc: 45.989% (15629/33984)\n",
      "Loss: 1.476 | Acc: 46.003% (15663/34048)\n",
      "Loss: 1.476 | Acc: 46.010% (15695/34112)\n",
      "Loss: 1.476 | Acc: 46.003% (15722/34176)\n",
      "Loss: 1.476 | Acc: 46.005% (15752/34240)\n",
      "Loss: 1.476 | Acc: 46.006% (15782/34304)\n",
      "Loss: 1.477 | Acc: 45.993% (15807/34368)\n",
      "Loss: 1.477 | Acc: 45.978% (15831/34432)\n",
      "Loss: 1.477 | Acc: 45.971% (15858/34496)\n",
      "Loss: 1.477 | Acc: 45.975% (15889/34560)\n",
      "Loss: 1.478 | Acc: 45.965% (15915/34624)\n",
      "Loss: 1.478 | Acc: 45.958% (15942/34688)\n",
      "Loss: 1.478 | Acc: 45.963% (15973/34752)\n",
      "Loss: 1.477 | Acc: 45.964% (16003/34816)\n",
      "Loss: 1.477 | Acc: 45.958% (16030/34880)\n",
      "Loss: 1.477 | Acc: 45.985% (16069/34944)\n",
      "Loss: 1.477 | Acc: 45.981% (16097/35008)\n",
      "Loss: 1.476 | Acc: 45.985% (16128/35072)\n",
      "Loss: 1.477 | Acc: 45.967% (16151/35136)\n",
      "Loss: 1.477 | Acc: 45.966% (16180/35200)\n",
      "Loss: 1.477 | Acc: 45.968% (16210/35264)\n",
      "Loss: 1.477 | Acc: 45.964% (16238/35328)\n",
      "Loss: 1.477 | Acc: 45.968% (16269/35392)\n",
      "Loss: 1.476 | Acc: 45.972% (16300/35456)\n",
      "Loss: 1.476 | Acc: 45.985% (16334/35520)\n",
      "Loss: 1.476 | Acc: 45.987% (16364/35584)\n",
      "Loss: 1.476 | Acc: 45.994% (16396/35648)\n",
      "Loss: 1.475 | Acc: 46.018% (16434/35712)\n",
      "Loss: 1.476 | Acc: 46.000% (16457/35776)\n",
      "Loss: 1.476 | Acc: 45.988% (16482/35840)\n",
      "Loss: 1.476 | Acc: 45.975% (16507/35904)\n",
      "Loss: 1.476 | Acc: 45.977% (16537/35968)\n",
      "Loss: 1.476 | Acc: 45.959% (16560/36032)\n",
      "Loss: 1.476 | Acc: 45.955% (16588/36096)\n",
      "Loss: 1.476 | Acc: 45.971% (16623/36160)\n",
      "Loss: 1.476 | Acc: 45.958% (16648/36224)\n",
      "Loss: 1.476 | Acc: 45.968% (16681/36288)\n",
      "Loss: 1.476 | Acc: 45.975% (16713/36352)\n",
      "Loss: 1.475 | Acc: 45.977% (16743/36416)\n",
      "Loss: 1.475 | Acc: 45.987% (16776/36480)\n",
      "Loss: 1.475 | Acc: 46.002% (16811/36544)\n",
      "Loss: 1.475 | Acc: 46.001% (16840/36608)\n",
      "Loss: 1.475 | Acc: 46.005% (16871/36672)\n",
      "Loss: 1.474 | Acc: 46.004% (16900/36736)\n",
      "Loss: 1.474 | Acc: 45.997% (16927/36800)\n",
      "Loss: 1.475 | Acc: 45.980% (16950/36864)\n",
      "Loss: 1.475 | Acc: 45.984% (16981/36928)\n",
      "Loss: 1.475 | Acc: 45.983% (17010/36992)\n",
      "Loss: 1.475 | Acc: 45.998% (17045/37056)\n",
      "Loss: 1.475 | Acc: 45.997% (17074/37120)\n",
      "Loss: 1.475 | Acc: 46.004% (17106/37184)\n",
      "Loss: 1.475 | Acc: 45.997% (17133/37248)\n",
      "Loss: 1.475 | Acc: 45.988% (17159/37312)\n",
      "Loss: 1.475 | Acc: 45.973% (17183/37376)\n",
      "Loss: 1.475 | Acc: 45.970% (17211/37440)\n",
      "Loss: 1.475 | Acc: 45.976% (17243/37504)\n",
      "Loss: 1.475 | Acc: 45.986% (17276/37568)\n",
      "Loss: 1.474 | Acc: 46.003% (17312/37632)\n",
      "Loss: 1.474 | Acc: 46.010% (17344/37696)\n",
      "Loss: 1.474 | Acc: 46.009% (17373/37760)\n",
      "Loss: 1.474 | Acc: 46.024% (17408/37824)\n",
      "Loss: 1.474 | Acc: 46.028% (17439/37888)\n",
      "Loss: 1.474 | Acc: 46.005% (17460/37952)\n",
      "Loss: 1.474 | Acc: 45.999% (17487/38016)\n",
      "Loss: 1.474 | Acc: 46.006% (17519/38080)\n",
      "Loss: 1.474 | Acc: 46.007% (17549/38144)\n",
      "Loss: 1.474 | Acc: 46.006% (17578/38208)\n",
      "Loss: 1.473 | Acc: 46.031% (17617/38272)\n",
      "Loss: 1.473 | Acc: 46.045% (17652/38336)\n",
      "Loss: 1.473 | Acc: 46.031% (17676/38400)\n",
      "Loss: 1.473 | Acc: 46.040% (17709/38464)\n",
      "Loss: 1.473 | Acc: 46.026% (17733/38528)\n",
      "Loss: 1.474 | Acc: 46.030% (17764/38592)\n",
      "Loss: 1.474 | Acc: 46.029% (17793/38656)\n",
      "Loss: 1.473 | Acc: 46.036% (17825/38720)\n",
      "Loss: 1.473 | Acc: 46.034% (17854/38784)\n",
      "Loss: 1.473 | Acc: 46.044% (17887/38848)\n",
      "Loss: 1.473 | Acc: 46.040% (17915/38912)\n",
      "Loss: 1.473 | Acc: 46.031% (17941/38976)\n",
      "Loss: 1.473 | Acc: 46.035% (17972/39040)\n",
      "Loss: 1.473 | Acc: 46.031% (18000/39104)\n",
      "Loss: 1.473 | Acc: 46.027% (18028/39168)\n",
      "Loss: 1.473 | Acc: 46.031% (18059/39232)\n",
      "Loss: 1.473 | Acc: 46.033% (18089/39296)\n",
      "Loss: 1.473 | Acc: 46.049% (18125/39360)\n",
      "Loss: 1.473 | Acc: 46.053% (18156/39424)\n",
      "Loss: 1.472 | Acc: 46.070% (18192/39488)\n",
      "Loss: 1.472 | Acc: 46.068% (18221/39552)\n",
      "Loss: 1.472 | Acc: 46.075% (18253/39616)\n",
      "Loss: 1.472 | Acc: 46.069% (18280/39680)\n",
      "Loss: 1.472 | Acc: 46.062% (18307/39744)\n",
      "Loss: 1.472 | Acc: 46.074% (18341/39808)\n",
      "Loss: 1.472 | Acc: 46.085% (18375/39872)\n",
      "Loss: 1.472 | Acc: 46.096% (18409/39936)\n",
      "Loss: 1.472 | Acc: 46.080% (18432/40000)\n",
      "Loss: 1.472 | Acc: 46.069% (18457/40064)\n",
      "Loss: 1.472 | Acc: 46.060% (18483/40128)\n",
      "Loss: 1.473 | Acc: 46.054% (18510/40192)\n",
      "Loss: 1.472 | Acc: 46.068% (18545/40256)\n",
      "Loss: 1.472 | Acc: 46.057% (18570/40320)\n",
      "Loss: 1.472 | Acc: 46.060% (18601/40384)\n",
      "Loss: 1.472 | Acc: 46.064% (18632/40448)\n",
      "Loss: 1.472 | Acc: 46.063% (18661/40512)\n",
      "Loss: 1.472 | Acc: 46.057% (18688/40576)\n",
      "Loss: 1.472 | Acc: 46.073% (18724/40640)\n",
      "Loss: 1.472 | Acc: 46.079% (18756/40704)\n",
      "Loss: 1.472 | Acc: 46.085% (18788/40768)\n",
      "Loss: 1.472 | Acc: 46.074% (18813/40832)\n",
      "Loss: 1.472 | Acc: 46.085% (18847/40896)\n",
      "Loss: 1.471 | Acc: 46.091% (18879/40960)\n",
      "Loss: 1.471 | Acc: 46.093% (18909/41024)\n",
      "Loss: 1.471 | Acc: 46.096% (18940/41088)\n",
      "Loss: 1.471 | Acc: 46.110% (18975/41152)\n",
      "Loss: 1.471 | Acc: 46.103% (19002/41216)\n",
      "Loss: 1.470 | Acc: 46.105% (19032/41280)\n",
      "Loss: 1.471 | Acc: 46.103% (19061/41344)\n",
      "Loss: 1.471 | Acc: 46.088% (19084/41408)\n",
      "Loss: 1.471 | Acc: 46.094% (19116/41472)\n",
      "Loss: 1.471 | Acc: 46.102% (19149/41536)\n",
      "Loss: 1.471 | Acc: 46.111% (19182/41600)\n",
      "Loss: 1.471 | Acc: 46.093% (19204/41664)\n",
      "Loss: 1.470 | Acc: 46.101% (19237/41728)\n",
      "Loss: 1.470 | Acc: 46.114% (19272/41792)\n",
      "Loss: 1.470 | Acc: 46.106% (19298/41856)\n",
      "Loss: 1.470 | Acc: 46.104% (19327/41920)\n",
      "Loss: 1.470 | Acc: 46.094% (19352/41984)\n",
      "Loss: 1.470 | Acc: 46.100% (19384/42048)\n",
      "Loss: 1.470 | Acc: 46.103% (19415/42112)\n",
      "Loss: 1.470 | Acc: 46.109% (19447/42176)\n",
      "Loss: 1.469 | Acc: 46.117% (19480/42240)\n",
      "Loss: 1.469 | Acc: 46.126% (19513/42304)\n",
      "Loss: 1.469 | Acc: 46.134% (19546/42368)\n",
      "Loss: 1.469 | Acc: 46.135% (19576/42432)\n",
      "Loss: 1.469 | Acc: 46.146% (19610/42496)\n",
      "Loss: 1.469 | Acc: 46.135% (19635/42560)\n",
      "Loss: 1.469 | Acc: 46.136% (19665/42624)\n",
      "Loss: 1.469 | Acc: 46.142% (19697/42688)\n",
      "Loss: 1.469 | Acc: 46.143% (19727/42752)\n",
      "Loss: 1.469 | Acc: 46.142% (19756/42816)\n",
      "Loss: 1.469 | Acc: 46.150% (19789/42880)\n",
      "Loss: 1.469 | Acc: 46.144% (19816/42944)\n",
      "Loss: 1.469 | Acc: 46.138% (19843/43008)\n",
      "Loss: 1.469 | Acc: 46.144% (19875/43072)\n",
      "Loss: 1.469 | Acc: 46.138% (19902/43136)\n",
      "Loss: 1.469 | Acc: 46.150% (19937/43200)\n",
      "Loss: 1.468 | Acc: 46.140% (19962/43264)\n",
      "Loss: 1.468 | Acc: 46.155% (19998/43328)\n",
      "Loss: 1.468 | Acc: 46.165% (20032/43392)\n",
      "Loss: 1.468 | Acc: 46.182% (20069/43456)\n",
      "Loss: 1.468 | Acc: 46.188% (20101/43520)\n",
      "Loss: 1.468 | Acc: 46.189% (20131/43584)\n",
      "Loss: 1.468 | Acc: 46.192% (20162/43648)\n",
      "Loss: 1.468 | Acc: 46.198% (20194/43712)\n",
      "Loss: 1.467 | Acc: 46.199% (20224/43776)\n",
      "Loss: 1.467 | Acc: 46.218% (20262/43840)\n",
      "Loss: 1.467 | Acc: 46.235% (20299/43904)\n",
      "Loss: 1.467 | Acc: 46.231% (20327/43968)\n",
      "Loss: 1.467 | Acc: 46.237% (20359/44032)\n",
      "Loss: 1.467 | Acc: 46.251% (20395/44096)\n",
      "Loss: 1.467 | Acc: 46.259% (20428/44160)\n",
      "Loss: 1.466 | Acc: 46.267% (20461/44224)\n",
      "Loss: 1.466 | Acc: 46.268% (20491/44288)\n",
      "Loss: 1.467 | Acc: 46.257% (20516/44352)\n",
      "Loss: 1.467 | Acc: 46.254% (20544/44416)\n",
      "Loss: 1.466 | Acc: 46.263% (20578/44480)\n",
      "Loss: 1.466 | Acc: 46.258% (20605/44544)\n",
      "Loss: 1.466 | Acc: 46.270% (20640/44608)\n",
      "Loss: 1.466 | Acc: 46.264% (20667/44672)\n",
      "Loss: 1.466 | Acc: 46.274% (20701/44736)\n",
      "Loss: 1.466 | Acc: 46.283% (20735/44800)\n",
      "Loss: 1.466 | Acc: 46.280% (20763/44864)\n",
      "Loss: 1.466 | Acc: 46.278% (20792/44928)\n",
      "Loss: 1.466 | Acc: 46.282% (20823/44992)\n",
      "Loss: 1.466 | Acc: 46.282% (20853/45056)\n",
      "Loss: 1.466 | Acc: 46.285% (20884/45120)\n",
      "Loss: 1.466 | Acc: 46.273% (20908/45184)\n",
      "Loss: 1.466 | Acc: 46.298% (20949/45248)\n",
      "Loss: 1.466 | Acc: 46.301% (20980/45312)\n",
      "Loss: 1.466 | Acc: 46.300% (21009/45376)\n",
      "Loss: 1.466 | Acc: 46.303% (21040/45440)\n",
      "Loss: 1.466 | Acc: 46.312% (21074/45504)\n",
      "Loss: 1.465 | Acc: 46.326% (21110/45568)\n",
      "Loss: 1.465 | Acc: 46.334% (21143/45632)\n",
      "Loss: 1.465 | Acc: 46.330% (21171/45696)\n",
      "Loss: 1.465 | Acc: 46.329% (21200/45760)\n",
      "Loss: 1.465 | Acc: 46.321% (21226/45824)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.465 | Acc: 46.311% (21251/45888)\n",
      "Loss: 1.465 | Acc: 46.327% (21288/45952)\n",
      "Loss: 1.465 | Acc: 46.325% (21317/46016)\n",
      "Loss: 1.465 | Acc: 46.335% (21351/46080)\n",
      "Loss: 1.465 | Acc: 46.335% (21381/46144)\n",
      "Loss: 1.465 | Acc: 46.347% (21416/46208)\n",
      "Loss: 1.465 | Acc: 46.346% (21445/46272)\n",
      "Loss: 1.465 | Acc: 46.338% (21471/46336)\n",
      "Loss: 1.465 | Acc: 46.336% (21500/46400)\n",
      "Loss: 1.465 | Acc: 46.346% (21534/46464)\n",
      "Loss: 1.465 | Acc: 46.357% (21569/46528)\n",
      "Loss: 1.464 | Acc: 46.362% (21601/46592)\n",
      "Loss: 1.464 | Acc: 46.378% (21638/46656)\n",
      "Loss: 1.464 | Acc: 46.376% (21667/46720)\n",
      "Loss: 1.464 | Acc: 46.373% (21695/46784)\n",
      "Loss: 1.464 | Acc: 46.371% (21724/46848)\n",
      "Loss: 1.464 | Acc: 46.380% (21758/46912)\n",
      "Loss: 1.464 | Acc: 46.383% (21789/46976)\n",
      "Loss: 1.463 | Acc: 46.390% (21822/47040)\n",
      "Loss: 1.463 | Acc: 46.389% (21851/47104)\n",
      "Loss: 1.463 | Acc: 46.387% (21880/47168)\n",
      "Loss: 1.463 | Acc: 46.380% (21906/47232)\n",
      "Loss: 1.463 | Acc: 46.384% (21938/47296)\n",
      "Loss: 1.464 | Acc: 46.375% (21963/47360)\n",
      "Loss: 1.464 | Acc: 46.358% (21985/47424)\n",
      "Loss: 1.464 | Acc: 46.361% (22016/47488)\n",
      "Loss: 1.464 | Acc: 46.377% (22053/47552)\n",
      "Loss: 1.464 | Acc: 46.377% (22083/47616)\n",
      "Loss: 1.464 | Acc: 46.370% (22109/47680)\n",
      "Loss: 1.463 | Acc: 46.370% (22139/47744)\n",
      "Loss: 1.463 | Acc: 46.386% (22176/47808)\n",
      "Loss: 1.463 | Acc: 46.395% (22210/47872)\n",
      "Loss: 1.463 | Acc: 46.395% (22240/47936)\n",
      "Loss: 1.463 | Acc: 46.404% (22274/48000)\n",
      "Loss: 1.463 | Acc: 46.403% (22303/48064)\n",
      "Loss: 1.463 | Acc: 46.407% (22335/48128)\n",
      "Loss: 1.463 | Acc: 46.404% (22363/48192)\n",
      "Loss: 1.463 | Acc: 46.409% (22395/48256)\n",
      "Loss: 1.463 | Acc: 46.407% (22424/48320)\n",
      "Loss: 1.463 | Acc: 46.412% (22456/48384)\n",
      "Loss: 1.463 | Acc: 46.413% (22486/48448)\n",
      "Loss: 1.463 | Acc: 46.417% (22518/48512)\n",
      "Loss: 1.463 | Acc: 46.424% (22551/48576)\n",
      "Loss: 1.463 | Acc: 46.429% (22583/48640)\n",
      "Loss: 1.463 | Acc: 46.429% (22613/48704)\n",
      "Loss: 1.462 | Acc: 46.434% (22645/48768)\n",
      "Loss: 1.463 | Acc: 46.437% (22676/48832)\n",
      "Loss: 1.462 | Acc: 46.450% (22712/48896)\n",
      "Loss: 1.462 | Acc: 46.452% (22743/48960)\n",
      "Loss: 1.463 | Acc: 46.453% (22762/49000)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 46.453061224489794\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.335 | Acc: 50.000% (32/64)\n",
      "Loss: 1.317 | Acc: 50.000% (64/128)\n",
      "Loss: 1.379 | Acc: 48.958% (94/192)\n",
      "Loss: 1.488 | Acc: 47.656% (122/256)\n",
      "Loss: 1.454 | Acc: 48.438% (155/320)\n",
      "Loss: 1.460 | Acc: 46.875% (180/384)\n",
      "Loss: 1.489 | Acc: 45.312% (203/448)\n",
      "Loss: 1.472 | Acc: 45.508% (233/512)\n",
      "Loss: 1.454 | Acc: 46.354% (267/576)\n",
      "Loss: 1.433 | Acc: 47.344% (303/640)\n",
      "Loss: 1.447 | Acc: 46.875% (330/704)\n",
      "Loss: 1.455 | Acc: 47.266% (363/768)\n",
      "Loss: 1.455 | Acc: 47.115% (392/832)\n",
      "Loss: 1.451 | Acc: 47.545% (426/896)\n",
      "Loss: 1.443 | Acc: 47.500% (456/960)\n",
      "Loss: 1.430 | Acc: 48.438% (496/1024)\n",
      "Loss: 1.437 | Acc: 48.162% (524/1088)\n",
      "Loss: 1.438 | Acc: 48.177% (555/1152)\n",
      "Loss: 1.442 | Acc: 48.602% (591/1216)\n",
      "Loss: 1.448 | Acc: 48.203% (617/1280)\n",
      "Loss: 1.444 | Acc: 48.065% (646/1344)\n",
      "Loss: 1.443 | Acc: 47.869% (674/1408)\n",
      "Loss: 1.439 | Acc: 48.166% (709/1472)\n",
      "Loss: 1.442 | Acc: 47.982% (737/1536)\n",
      "Loss: 1.438 | Acc: 47.875% (766/1600)\n",
      "Loss: 1.436 | Acc: 47.837% (796/1664)\n",
      "Loss: 1.431 | Acc: 48.322% (835/1728)\n",
      "Loss: 1.428 | Acc: 48.493% (869/1792)\n",
      "Loss: 1.426 | Acc: 48.599% (902/1856)\n",
      "Loss: 1.428 | Acc: 48.802% (937/1920)\n",
      "Loss: 1.429 | Acc: 48.690% (966/1984)\n",
      "Loss: 1.426 | Acc: 49.072% (1005/2048)\n",
      "Loss: 1.422 | Acc: 49.148% (1038/2112)\n",
      "Loss: 1.423 | Acc: 48.943% (1065/2176)\n",
      "Loss: 1.422 | Acc: 49.062% (1099/2240)\n",
      "Loss: 1.421 | Acc: 49.045% (1130/2304)\n",
      "Loss: 1.424 | Acc: 48.944% (1159/2368)\n",
      "Loss: 1.424 | Acc: 48.931% (1190/2432)\n",
      "Loss: 1.422 | Acc: 48.958% (1222/2496)\n",
      "Loss: 1.432 | Acc: 48.711% (1247/2560)\n",
      "Loss: 1.438 | Acc: 48.514% (1273/2624)\n",
      "Loss: 1.439 | Acc: 48.624% (1307/2688)\n",
      "Loss: 1.438 | Acc: 48.474% (1334/2752)\n",
      "Loss: 1.439 | Acc: 48.331% (1361/2816)\n",
      "Loss: 1.438 | Acc: 48.299% (1391/2880)\n",
      "Loss: 1.437 | Acc: 48.438% (1426/2944)\n",
      "Loss: 1.439 | Acc: 48.271% (1452/3008)\n",
      "Loss: 1.438 | Acc: 48.145% (1479/3072)\n",
      "Loss: 1.435 | Acc: 48.151% (1510/3136)\n",
      "Loss: 1.435 | Acc: 48.219% (1543/3200)\n",
      "Loss: 1.435 | Acc: 48.254% (1575/3264)\n",
      "Loss: 1.436 | Acc: 48.257% (1606/3328)\n",
      "Loss: 1.436 | Acc: 48.202% (1635/3392)\n",
      "Loss: 1.437 | Acc: 48.177% (1665/3456)\n",
      "Loss: 1.435 | Acc: 48.239% (1698/3520)\n",
      "Loss: 1.434 | Acc: 48.465% (1737/3584)\n",
      "Loss: 1.434 | Acc: 48.465% (1768/3648)\n",
      "Loss: 1.430 | Acc: 48.545% (1802/3712)\n",
      "Loss: 1.431 | Acc: 48.438% (1829/3776)\n",
      "Loss: 1.431 | Acc: 48.490% (1862/3840)\n",
      "Loss: 1.429 | Acc: 48.540% (1895/3904)\n",
      "Loss: 1.428 | Acc: 48.513% (1925/3968)\n",
      "Loss: 1.431 | Acc: 48.438% (1953/4032)\n",
      "Loss: 1.432 | Acc: 48.364% (1981/4096)\n",
      "Loss: 1.435 | Acc: 48.293% (2009/4160)\n",
      "Loss: 1.436 | Acc: 48.414% (2045/4224)\n",
      "Loss: 1.434 | Acc: 48.507% (2080/4288)\n",
      "Loss: 1.432 | Acc: 48.598% (2115/4352)\n",
      "Loss: 1.429 | Acc: 48.822% (2156/4416)\n",
      "Loss: 1.428 | Acc: 48.839% (2188/4480)\n",
      "Loss: 1.427 | Acc: 48.856% (2220/4544)\n",
      "Loss: 1.430 | Acc: 48.698% (2244/4608)\n",
      "Loss: 1.427 | Acc: 48.887% (2284/4672)\n",
      "Loss: 1.424 | Acc: 48.986% (2320/4736)\n",
      "Loss: 1.426 | Acc: 48.958% (2350/4800)\n",
      "Loss: 1.425 | Acc: 48.931% (2380/4864)\n",
      "Loss: 1.423 | Acc: 48.925% (2411/4928)\n",
      "Loss: 1.424 | Acc: 48.938% (2443/4992)\n",
      "Loss: 1.422 | Acc: 49.031% (2479/5056)\n",
      "Loss: 1.422 | Acc: 49.023% (2510/5120)\n",
      "Loss: 1.421 | Acc: 49.074% (2544/5184)\n",
      "Loss: 1.420 | Acc: 49.066% (2575/5248)\n",
      "Loss: 1.418 | Acc: 49.059% (2606/5312)\n",
      "Loss: 1.419 | Acc: 49.033% (2636/5376)\n",
      "Loss: 1.419 | Acc: 48.989% (2665/5440)\n",
      "Loss: 1.419 | Acc: 49.055% (2700/5504)\n",
      "Loss: 1.422 | Acc: 48.886% (2722/5568)\n",
      "Loss: 1.425 | Acc: 48.846% (2751/5632)\n",
      "Loss: 1.423 | Acc: 48.912% (2786/5696)\n",
      "Loss: 1.422 | Acc: 48.906% (2817/5760)\n",
      "Loss: 1.420 | Acc: 49.004% (2854/5824)\n",
      "Loss: 1.422 | Acc: 48.913% (2880/5888)\n",
      "Loss: 1.423 | Acc: 48.874% (2909/5952)\n",
      "Loss: 1.423 | Acc: 48.870% (2940/6016)\n",
      "Loss: 1.425 | Acc: 48.799% (2967/6080)\n",
      "Loss: 1.423 | Acc: 48.779% (2997/6144)\n",
      "Loss: 1.424 | Acc: 48.776% (3028/6208)\n",
      "Loss: 1.426 | Acc: 48.740% (3057/6272)\n",
      "Loss: 1.424 | Acc: 48.848% (3095/6336)\n",
      "Loss: 1.423 | Acc: 48.812% (3124/6400)\n",
      "Loss: 1.427 | Acc: 48.623% (3143/6464)\n",
      "Loss: 1.427 | Acc: 48.545% (3169/6528)\n",
      "Loss: 1.428 | Acc: 48.589% (3203/6592)\n",
      "Loss: 1.428 | Acc: 48.528% (3230/6656)\n",
      "Loss: 1.429 | Acc: 48.512% (3260/6720)\n",
      "Loss: 1.427 | Acc: 48.629% (3299/6784)\n",
      "Loss: 1.424 | Acc: 48.671% (3333/6848)\n",
      "Loss: 1.427 | Acc: 48.626% (3361/6912)\n",
      "Loss: 1.430 | Acc: 48.552% (3387/6976)\n",
      "Loss: 1.432 | Acc: 48.466% (3412/7040)\n",
      "Loss: 1.432 | Acc: 48.409% (3439/7104)\n",
      "Loss: 1.432 | Acc: 48.410% (3470/7168)\n",
      "Loss: 1.434 | Acc: 48.327% (3495/7232)\n",
      "Loss: 1.432 | Acc: 48.424% (3533/7296)\n",
      "Loss: 1.430 | Acc: 48.383% (3561/7360)\n",
      "Loss: 1.429 | Acc: 48.343% (3589/7424)\n",
      "Loss: 1.427 | Acc: 48.357% (3621/7488)\n",
      "Loss: 1.427 | Acc: 48.371% (3653/7552)\n",
      "Loss: 1.427 | Acc: 48.385% (3685/7616)\n",
      "Loss: 1.427 | Acc: 48.398% (3717/7680)\n",
      "Loss: 1.424 | Acc: 48.515% (3757/7744)\n",
      "Loss: 1.423 | Acc: 48.578% (3793/7808)\n",
      "Loss: 1.423 | Acc: 48.526% (3820/7872)\n",
      "Loss: 1.423 | Acc: 48.488% (3848/7936)\n",
      "Loss: 1.424 | Acc: 48.438% (3875/8000)\n",
      "Loss: 1.425 | Acc: 48.388% (3902/8064)\n",
      "Loss: 1.425 | Acc: 48.413% (3935/8128)\n",
      "Loss: 1.424 | Acc: 48.450% (3969/8192)\n",
      "Loss: 1.424 | Acc: 48.365% (3993/8256)\n",
      "Loss: 1.426 | Acc: 48.317% (4020/8320)\n",
      "Loss: 1.426 | Acc: 48.271% (4047/8384)\n",
      "Loss: 1.426 | Acc: 48.236% (4075/8448)\n",
      "Loss: 1.427 | Acc: 48.214% (4104/8512)\n",
      "Loss: 1.427 | Acc: 48.204% (4134/8576)\n",
      "Loss: 1.427 | Acc: 48.252% (4169/8640)\n",
      "Loss: 1.429 | Acc: 48.162% (4192/8704)\n",
      "Loss: 1.429 | Acc: 48.141% (4221/8768)\n",
      "Loss: 1.429 | Acc: 48.132% (4251/8832)\n",
      "Loss: 1.428 | Acc: 48.213% (4289/8896)\n",
      "Loss: 1.428 | Acc: 48.181% (4317/8960)\n",
      "Loss: 1.429 | Acc: 48.160% (4346/9024)\n",
      "Loss: 1.429 | Acc: 48.162% (4377/9088)\n",
      "Loss: 1.429 | Acc: 48.142% (4406/9152)\n",
      "Loss: 1.427 | Acc: 48.231% (4445/9216)\n",
      "Loss: 1.426 | Acc: 48.297% (4482/9280)\n",
      "Loss: 1.427 | Acc: 48.298% (4513/9344)\n",
      "Loss: 1.427 | Acc: 48.289% (4543/9408)\n",
      "Loss: 1.429 | Acc: 48.216% (4567/9472)\n",
      "Loss: 1.430 | Acc: 48.238% (4600/9536)\n",
      "Loss: 1.428 | Acc: 48.260% (4633/9600)\n",
      "Loss: 1.428 | Acc: 48.262% (4664/9664)\n",
      "Loss: 1.429 | Acc: 48.232% (4692/9728)\n",
      "Loss: 1.431 | Acc: 48.182% (4718/9792)\n",
      "Loss: 1.431 | Acc: 48.184% (4749/9856)\n",
      "Loss: 1.433 | Acc: 48.165% (4778/9920)\n",
      "Loss: 1.433 | Acc: 48.127% (4805/9984)\n",
      "Loss: 1.432 | Acc: 48.120% (4812/10000)\n",
      "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 48.12\n",
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.353 | Acc: 51.562% (33/64)\n",
      "Loss: 1.247 | Acc: 58.594% (75/128)\n",
      "Loss: 1.293 | Acc: 55.208% (106/192)\n",
      "Loss: 1.261 | Acc: 54.297% (139/256)\n",
      "Loss: 1.325 | Acc: 52.188% (167/320)\n",
      "Loss: 1.287 | Acc: 52.865% (203/384)\n",
      "Loss: 1.296 | Acc: 52.232% (234/448)\n",
      "Loss: 1.303 | Acc: 51.562% (264/512)\n",
      "Loss: 1.310 | Acc: 51.562% (297/576)\n",
      "Loss: 1.305 | Acc: 52.188% (334/640)\n",
      "Loss: 1.292 | Acc: 53.125% (374/704)\n",
      "Loss: 1.278 | Acc: 53.776% (413/768)\n",
      "Loss: 1.279 | Acc: 54.207% (451/832)\n",
      "Loss: 1.281 | Acc: 54.241% (486/896)\n",
      "Loss: 1.287 | Acc: 53.958% (518/960)\n",
      "Loss: 1.279 | Acc: 54.102% (554/1024)\n",
      "Loss: 1.284 | Acc: 53.860% (586/1088)\n",
      "Loss: 1.286 | Acc: 53.385% (615/1152)\n",
      "Loss: 1.291 | Acc: 53.043% (645/1216)\n",
      "Loss: 1.283 | Acc: 53.516% (685/1280)\n",
      "Loss: 1.282 | Acc: 53.869% (724/1344)\n",
      "Loss: 1.289 | Acc: 53.196% (749/1408)\n",
      "Loss: 1.291 | Acc: 53.125% (782/1472)\n",
      "Loss: 1.298 | Acc: 52.995% (814/1536)\n",
      "Loss: 1.291 | Acc: 53.312% (853/1600)\n",
      "Loss: 1.293 | Acc: 53.125% (884/1664)\n",
      "Loss: 1.301 | Acc: 52.836% (913/1728)\n",
      "Loss: 1.305 | Acc: 52.790% (946/1792)\n",
      "Loss: 1.311 | Acc: 52.317% (971/1856)\n",
      "Loss: 1.319 | Acc: 52.031% (999/1920)\n",
      "Loss: 1.318 | Acc: 51.966% (1031/1984)\n",
      "Loss: 1.320 | Acc: 51.855% (1062/2048)\n",
      "Loss: 1.322 | Acc: 51.562% (1089/2112)\n",
      "Loss: 1.324 | Acc: 51.517% (1121/2176)\n",
      "Loss: 1.330 | Acc: 51.250% (1148/2240)\n",
      "Loss: 1.334 | Acc: 51.085% (1177/2304)\n",
      "Loss: 1.332 | Acc: 51.182% (1212/2368)\n",
      "Loss: 1.329 | Acc: 51.398% (1250/2432)\n",
      "Loss: 1.330 | Acc: 51.402% (1283/2496)\n",
      "Loss: 1.330 | Acc: 51.289% (1313/2560)\n",
      "Loss: 1.335 | Acc: 51.067% (1340/2624)\n",
      "Loss: 1.334 | Acc: 51.228% (1377/2688)\n",
      "Loss: 1.338 | Acc: 51.090% (1406/2752)\n",
      "Loss: 1.340 | Acc: 50.994% (1436/2816)\n",
      "Loss: 1.340 | Acc: 50.972% (1468/2880)\n",
      "Loss: 1.341 | Acc: 50.849% (1497/2944)\n",
      "Loss: 1.344 | Acc: 50.565% (1521/3008)\n",
      "Loss: 1.347 | Acc: 50.391% (1548/3072)\n",
      "Loss: 1.345 | Acc: 50.542% (1585/3136)\n",
      "Loss: 1.341 | Acc: 50.469% (1615/3200)\n",
      "Loss: 1.344 | Acc: 50.398% (1645/3264)\n",
      "Loss: 1.342 | Acc: 50.421% (1678/3328)\n",
      "Loss: 1.346 | Acc: 50.265% (1705/3392)\n",
      "Loss: 1.347 | Acc: 50.347% (1740/3456)\n",
      "Loss: 1.346 | Acc: 50.341% (1772/3520)\n",
      "Loss: 1.344 | Acc: 50.446% (1808/3584)\n",
      "Loss: 1.346 | Acc: 50.493% (1842/3648)\n",
      "Loss: 1.345 | Acc: 50.404% (1871/3712)\n",
      "Loss: 1.349 | Acc: 50.291% (1899/3776)\n",
      "Loss: 1.348 | Acc: 50.391% (1935/3840)\n",
      "Loss: 1.347 | Acc: 50.359% (1966/3904)\n",
      "Loss: 1.347 | Acc: 50.252% (1994/3968)\n",
      "Loss: 1.348 | Acc: 50.223% (2025/4032)\n",
      "Loss: 1.348 | Acc: 50.269% (2059/4096)\n",
      "Loss: 1.347 | Acc: 50.337% (2094/4160)\n",
      "Loss: 1.349 | Acc: 50.260% (2123/4224)\n",
      "Loss: 1.349 | Acc: 50.257% (2155/4288)\n",
      "Loss: 1.351 | Acc: 50.184% (2184/4352)\n",
      "Loss: 1.355 | Acc: 50.159% (2215/4416)\n",
      "Loss: 1.353 | Acc: 50.134% (2246/4480)\n",
      "Loss: 1.353 | Acc: 50.088% (2276/4544)\n",
      "Loss: 1.357 | Acc: 49.978% (2303/4608)\n",
      "Loss: 1.354 | Acc: 50.064% (2339/4672)\n",
      "Loss: 1.354 | Acc: 50.106% (2373/4736)\n",
      "Loss: 1.354 | Acc: 50.083% (2404/4800)\n",
      "Loss: 1.354 | Acc: 50.185% (2441/4864)\n",
      "Loss: 1.355 | Acc: 50.142% (2471/4928)\n",
      "Loss: 1.357 | Acc: 50.080% (2500/4992)\n",
      "Loss: 1.356 | Acc: 50.040% (2530/5056)\n",
      "Loss: 1.357 | Acc: 50.059% (2563/5120)\n",
      "Loss: 1.357 | Acc: 50.077% (2596/5184)\n",
      "Loss: 1.357 | Acc: 50.114% (2630/5248)\n",
      "Loss: 1.358 | Acc: 49.981% (2655/5312)\n",
      "Loss: 1.357 | Acc: 49.981% (2687/5376)\n",
      "Loss: 1.357 | Acc: 49.963% (2718/5440)\n",
      "Loss: 1.356 | Acc: 50.018% (2753/5504)\n",
      "Loss: 1.356 | Acc: 50.054% (2787/5568)\n",
      "Loss: 1.356 | Acc: 50.071% (2820/5632)\n",
      "Loss: 1.357 | Acc: 50.070% (2852/5696)\n",
      "Loss: 1.357 | Acc: 50.174% (2890/5760)\n",
      "Loss: 1.356 | Acc: 50.155% (2921/5824)\n",
      "Loss: 1.355 | Acc: 50.136% (2952/5888)\n",
      "Loss: 1.356 | Acc: 50.101% (2982/5952)\n",
      "Loss: 1.355 | Acc: 50.083% (3013/6016)\n",
      "Loss: 1.355 | Acc: 50.115% (3047/6080)\n",
      "Loss: 1.355 | Acc: 50.179% (3083/6144)\n",
      "Loss: 1.354 | Acc: 50.209% (3117/6208)\n",
      "Loss: 1.351 | Acc: 50.367% (3159/6272)\n",
      "Loss: 1.352 | Acc: 50.284% (3186/6336)\n",
      "Loss: 1.352 | Acc: 50.375% (3224/6400)\n",
      "Loss: 1.350 | Acc: 50.387% (3257/6464)\n",
      "Loss: 1.348 | Acc: 50.444% (3293/6528)\n",
      "Loss: 1.348 | Acc: 50.455% (3326/6592)\n",
      "Loss: 1.347 | Acc: 50.466% (3359/6656)\n",
      "Loss: 1.346 | Acc: 50.536% (3396/6720)\n",
      "Loss: 1.345 | Acc: 50.501% (3426/6784)\n",
      "Loss: 1.345 | Acc: 50.482% (3457/6848)\n",
      "Loss: 1.345 | Acc: 50.477% (3489/6912)\n",
      "Loss: 1.343 | Acc: 50.588% (3529/6976)\n",
      "Loss: 1.344 | Acc: 50.540% (3558/7040)\n",
      "Loss: 1.344 | Acc: 50.465% (3585/7104)\n",
      "Loss: 1.342 | Acc: 50.488% (3619/7168)\n",
      "Loss: 1.342 | Acc: 50.470% (3650/7232)\n",
      "Loss: 1.342 | Acc: 50.466% (3682/7296)\n",
      "Loss: 1.342 | Acc: 50.435% (3712/7360)\n",
      "Loss: 1.341 | Acc: 50.498% (3749/7424)\n",
      "Loss: 1.342 | Acc: 50.494% (3781/7488)\n",
      "Loss: 1.341 | Acc: 50.516% (3815/7552)\n",
      "Loss: 1.343 | Acc: 50.407% (3839/7616)\n",
      "Loss: 1.342 | Acc: 50.456% (3875/7680)\n",
      "Loss: 1.341 | Acc: 50.504% (3911/7744)\n",
      "Loss: 1.340 | Acc: 50.551% (3947/7808)\n",
      "Loss: 1.340 | Acc: 50.572% (3981/7872)\n",
      "Loss: 1.338 | Acc: 50.617% (4017/7936)\n",
      "Loss: 1.341 | Acc: 50.500% (4040/8000)\n",
      "Loss: 1.342 | Acc: 50.508% (4073/8064)\n",
      "Loss: 1.343 | Acc: 50.480% (4103/8128)\n",
      "Loss: 1.342 | Acc: 50.500% (4137/8192)\n",
      "Loss: 1.342 | Acc: 50.484% (4168/8256)\n",
      "Loss: 1.342 | Acc: 50.445% (4197/8320)\n",
      "Loss: 1.343 | Acc: 50.406% (4226/8384)\n",
      "Loss: 1.342 | Acc: 50.414% (4259/8448)\n",
      "Loss: 1.341 | Acc: 50.423% (4292/8512)\n",
      "Loss: 1.341 | Acc: 50.361% (4319/8576)\n",
      "Loss: 1.341 | Acc: 50.370% (4352/8640)\n",
      "Loss: 1.343 | Acc: 50.253% (4374/8704)\n",
      "Loss: 1.344 | Acc: 50.194% (4401/8768)\n",
      "Loss: 1.341 | Acc: 50.283% (4441/8832)\n",
      "Loss: 1.341 | Acc: 50.259% (4471/8896)\n",
      "Loss: 1.342 | Acc: 50.234% (4501/8960)\n",
      "Loss: 1.343 | Acc: 50.255% (4535/9024)\n",
      "Loss: 1.343 | Acc: 50.220% (4564/9088)\n",
      "Loss: 1.343 | Acc: 50.208% (4595/9152)\n",
      "Loss: 1.342 | Acc: 50.260% (4632/9216)\n",
      "Loss: 1.343 | Acc: 50.248% (4663/9280)\n",
      "Loss: 1.344 | Acc: 50.257% (4696/9344)\n",
      "Loss: 1.344 | Acc: 50.266% (4729/9408)\n",
      "Loss: 1.343 | Acc: 50.338% (4768/9472)\n",
      "Loss: 1.344 | Acc: 50.283% (4795/9536)\n",
      "Loss: 1.344 | Acc: 50.271% (4826/9600)\n",
      "Loss: 1.344 | Acc: 50.248% (4856/9664)\n",
      "Loss: 1.344 | Acc: 50.278% (4891/9728)\n",
      "Loss: 1.344 | Acc: 50.245% (4920/9792)\n",
      "Loss: 1.344 | Acc: 50.264% (4954/9856)\n",
      "Loss: 1.344 | Acc: 50.282% (4988/9920)\n",
      "Loss: 1.344 | Acc: 50.280% (5020/9984)\n",
      "Loss: 1.344 | Acc: 50.289% (5053/10048)\n",
      "Loss: 1.343 | Acc: 50.297% (5086/10112)\n",
      "Loss: 1.343 | Acc: 50.314% (5120/10176)\n",
      "Loss: 1.344 | Acc: 50.264% (5147/10240)\n",
      "Loss: 1.345 | Acc: 50.233% (5176/10304)\n",
      "Loss: 1.346 | Acc: 50.203% (5205/10368)\n",
      "Loss: 1.345 | Acc: 50.220% (5239/10432)\n",
      "Loss: 1.345 | Acc: 50.219% (5271/10496)\n",
      "Loss: 1.345 | Acc: 50.227% (5304/10560)\n",
      "Loss: 1.345 | Acc: 50.254% (5339/10624)\n",
      "Loss: 1.345 | Acc: 50.262% (5372/10688)\n",
      "Loss: 1.345 | Acc: 50.242% (5402/10752)\n",
      "Loss: 1.346 | Acc: 50.213% (5431/10816)\n",
      "Loss: 1.346 | Acc: 50.239% (5466/10880)\n",
      "Loss: 1.347 | Acc: 50.238% (5498/10944)\n",
      "Loss: 1.348 | Acc: 50.145% (5520/11008)\n",
      "Loss: 1.348 | Acc: 50.163% (5554/11072)\n",
      "Loss: 1.348 | Acc: 50.090% (5578/11136)\n",
      "Loss: 1.348 | Acc: 50.134% (5615/11200)\n",
      "Loss: 1.347 | Acc: 50.169% (5651/11264)\n",
      "Loss: 1.347 | Acc: 50.168% (5683/11328)\n",
      "Loss: 1.347 | Acc: 50.184% (5717/11392)\n",
      "Loss: 1.346 | Acc: 50.253% (5757/11456)\n",
      "Loss: 1.346 | Acc: 50.191% (5782/11520)\n",
      "Loss: 1.347 | Acc: 50.173% (5812/11584)\n",
      "Loss: 1.348 | Acc: 50.163% (5843/11648)\n",
      "Loss: 1.348 | Acc: 50.137% (5872/11712)\n",
      "Loss: 1.348 | Acc: 50.136% (5904/11776)\n",
      "Loss: 1.349 | Acc: 50.144% (5937/11840)\n",
      "Loss: 1.348 | Acc: 50.160% (5971/11904)\n",
      "Loss: 1.347 | Acc: 50.192% (6007/11968)\n",
      "Loss: 1.347 | Acc: 50.249% (6046/12032)\n",
      "Loss: 1.347 | Acc: 50.256% (6079/12096)\n",
      "Loss: 1.348 | Acc: 50.247% (6110/12160)\n",
      "Loss: 1.349 | Acc: 50.213% (6138/12224)\n",
      "Loss: 1.349 | Acc: 50.163% (6164/12288)\n",
      "Loss: 1.349 | Acc: 50.194% (6200/12352)\n",
      "Loss: 1.349 | Acc: 50.169% (6229/12416)\n",
      "Loss: 1.350 | Acc: 50.200% (6265/12480)\n",
      "Loss: 1.351 | Acc: 50.199% (6297/12544)\n",
      "Loss: 1.350 | Acc: 50.238% (6334/12608)\n",
      "Loss: 1.349 | Acc: 50.276% (6371/12672)\n",
      "Loss: 1.349 | Acc: 50.306% (6407/12736)\n",
      "Loss: 1.348 | Acc: 50.312% (6440/12800)\n",
      "Loss: 1.348 | Acc: 50.342% (6476/12864)\n",
      "Loss: 1.349 | Acc: 50.309% (6504/12928)\n",
      "Loss: 1.349 | Acc: 50.323% (6538/12992)\n",
      "Loss: 1.348 | Acc: 50.299% (6567/13056)\n",
      "Loss: 1.348 | Acc: 50.335% (6604/13120)\n",
      "Loss: 1.347 | Acc: 50.341% (6637/13184)\n",
      "Loss: 1.347 | Acc: 50.377% (6674/13248)\n",
      "Loss: 1.346 | Acc: 50.443% (6715/13312)\n",
      "Loss: 1.347 | Acc: 50.426% (6745/13376)\n",
      "Loss: 1.347 | Acc: 50.432% (6778/13440)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.348 | Acc: 50.430% (6810/13504)\n",
      "Loss: 1.348 | Acc: 50.450% (6845/13568)\n",
      "Loss: 1.347 | Acc: 50.455% (6878/13632)\n",
      "Loss: 1.348 | Acc: 50.445% (6909/13696)\n",
      "Loss: 1.346 | Acc: 50.501% (6949/13760)\n",
      "Loss: 1.346 | Acc: 50.506% (6982/13824)\n",
      "Loss: 1.346 | Acc: 50.518% (7016/13888)\n",
      "Loss: 1.345 | Acc: 50.538% (7051/13952)\n",
      "Loss: 1.344 | Acc: 50.557% (7086/14016)\n",
      "Loss: 1.345 | Acc: 50.575% (7121/14080)\n",
      "Loss: 1.344 | Acc: 50.559% (7151/14144)\n",
      "Loss: 1.344 | Acc: 50.591% (7188/14208)\n",
      "Loss: 1.343 | Acc: 50.610% (7223/14272)\n",
      "Loss: 1.343 | Acc: 50.614% (7256/14336)\n",
      "Loss: 1.343 | Acc: 50.653% (7294/14400)\n",
      "Loss: 1.343 | Acc: 50.643% (7325/14464)\n",
      "Loss: 1.344 | Acc: 50.619% (7354/14528)\n",
      "Loss: 1.344 | Acc: 50.630% (7388/14592)\n",
      "Loss: 1.345 | Acc: 50.600% (7416/14656)\n",
      "Loss: 1.345 | Acc: 50.564% (7443/14720)\n",
      "Loss: 1.346 | Acc: 50.561% (7475/14784)\n",
      "Loss: 1.346 | Acc: 50.552% (7506/14848)\n",
      "Loss: 1.346 | Acc: 50.530% (7535/14912)\n",
      "Loss: 1.346 | Acc: 50.541% (7569/14976)\n",
      "Loss: 1.346 | Acc: 50.519% (7598/15040)\n",
      "Loss: 1.347 | Acc: 50.490% (7626/15104)\n",
      "Loss: 1.348 | Acc: 50.461% (7654/15168)\n",
      "Loss: 1.350 | Acc: 50.414% (7679/15232)\n",
      "Loss: 1.350 | Acc: 50.431% (7714/15296)\n",
      "Loss: 1.350 | Acc: 50.456% (7750/15360)\n",
      "Loss: 1.350 | Acc: 50.499% (7789/15424)\n",
      "Loss: 1.351 | Acc: 50.497% (7821/15488)\n",
      "Loss: 1.351 | Acc: 50.495% (7853/15552)\n",
      "Loss: 1.352 | Acc: 50.493% (7885/15616)\n",
      "Loss: 1.351 | Acc: 50.497% (7918/15680)\n",
      "Loss: 1.350 | Acc: 50.527% (7955/15744)\n",
      "Loss: 1.350 | Acc: 50.563% (7993/15808)\n",
      "Loss: 1.350 | Acc: 50.573% (8027/15872)\n",
      "Loss: 1.351 | Acc: 50.552% (8056/15936)\n",
      "Loss: 1.351 | Acc: 50.506% (8081/16000)\n",
      "Loss: 1.352 | Acc: 50.517% (8115/16064)\n",
      "Loss: 1.351 | Acc: 50.502% (8145/16128)\n",
      "Loss: 1.352 | Acc: 50.488% (8175/16192)\n",
      "Loss: 1.352 | Acc: 50.461% (8203/16256)\n",
      "Loss: 1.352 | Acc: 50.490% (8240/16320)\n",
      "Loss: 1.351 | Acc: 50.494% (8273/16384)\n",
      "Loss: 1.353 | Acc: 50.474% (8302/16448)\n",
      "Loss: 1.354 | Acc: 50.412% (8324/16512)\n",
      "Loss: 1.355 | Acc: 50.362% (8348/16576)\n",
      "Loss: 1.355 | Acc: 50.355% (8379/16640)\n",
      "Loss: 1.355 | Acc: 50.305% (8403/16704)\n",
      "Loss: 1.355 | Acc: 50.286% (8432/16768)\n",
      "Loss: 1.355 | Acc: 50.267% (8461/16832)\n",
      "Loss: 1.356 | Acc: 50.225% (8486/16896)\n",
      "Loss: 1.356 | Acc: 50.200% (8514/16960)\n",
      "Loss: 1.357 | Acc: 50.159% (8539/17024)\n",
      "Loss: 1.357 | Acc: 50.158% (8571/17088)\n",
      "Loss: 1.357 | Acc: 50.163% (8604/17152)\n",
      "Loss: 1.357 | Acc: 50.157% (8635/17216)\n",
      "Loss: 1.357 | Acc: 50.162% (8668/17280)\n",
      "Loss: 1.356 | Acc: 50.173% (8702/17344)\n",
      "Loss: 1.356 | Acc: 50.149% (8730/17408)\n",
      "Loss: 1.356 | Acc: 50.149% (8762/17472)\n",
      "Loss: 1.356 | Acc: 50.171% (8798/17536)\n",
      "Loss: 1.355 | Acc: 50.216% (8838/17600)\n",
      "Loss: 1.355 | Acc: 50.192% (8866/17664)\n",
      "Loss: 1.356 | Acc: 50.209% (8901/17728)\n",
      "Loss: 1.355 | Acc: 50.253% (8941/17792)\n",
      "Loss: 1.354 | Acc: 50.319% (8985/17856)\n",
      "Loss: 1.353 | Acc: 50.312% (9016/17920)\n",
      "Loss: 1.354 | Acc: 50.272% (9041/17984)\n",
      "Loss: 1.354 | Acc: 50.277% (9074/18048)\n",
      "Loss: 1.354 | Acc: 50.254% (9102/18112)\n",
      "Loss: 1.354 | Acc: 50.297% (9142/18176)\n",
      "Loss: 1.354 | Acc: 50.280% (9171/18240)\n",
      "Loss: 1.354 | Acc: 50.273% (9202/18304)\n",
      "Loss: 1.353 | Acc: 50.289% (9237/18368)\n",
      "Loss: 1.354 | Acc: 50.288% (9269/18432)\n",
      "Loss: 1.354 | Acc: 50.292% (9302/18496)\n",
      "Loss: 1.353 | Acc: 50.307% (9337/18560)\n",
      "Loss: 1.353 | Acc: 50.285% (9365/18624)\n",
      "Loss: 1.354 | Acc: 50.284% (9397/18688)\n",
      "Loss: 1.354 | Acc: 50.256% (9424/18752)\n",
      "Loss: 1.354 | Acc: 50.271% (9459/18816)\n",
      "Loss: 1.354 | Acc: 50.270% (9491/18880)\n",
      "Loss: 1.354 | Acc: 50.269% (9523/18944)\n",
      "Loss: 1.355 | Acc: 50.242% (9550/19008)\n",
      "Loss: 1.355 | Acc: 50.241% (9582/19072)\n",
      "Loss: 1.355 | Acc: 50.261% (9618/19136)\n",
      "Loss: 1.355 | Acc: 50.266% (9651/19200)\n",
      "Loss: 1.354 | Acc: 50.265% (9683/19264)\n",
      "Loss: 1.355 | Acc: 50.279% (9718/19328)\n",
      "Loss: 1.354 | Acc: 50.284% (9751/19392)\n",
      "Loss: 1.354 | Acc: 50.293% (9785/19456)\n",
      "Loss: 1.354 | Acc: 50.292% (9817/19520)\n",
      "Loss: 1.354 | Acc: 50.337% (9858/19584)\n",
      "Loss: 1.354 | Acc: 50.346% (9892/19648)\n",
      "Loss: 1.354 | Acc: 50.335% (9922/19712)\n",
      "Loss: 1.355 | Acc: 50.329% (9953/19776)\n",
      "Loss: 1.355 | Acc: 50.328% (9985/19840)\n",
      "Loss: 1.356 | Acc: 50.296% (10011/19904)\n",
      "Loss: 1.356 | Acc: 50.295% (10043/19968)\n",
      "Loss: 1.355 | Acc: 50.319% (10080/20032)\n",
      "Loss: 1.356 | Acc: 50.318% (10112/20096)\n",
      "Loss: 1.356 | Acc: 50.308% (10142/20160)\n",
      "Loss: 1.355 | Acc: 50.316% (10176/20224)\n",
      "Loss: 1.356 | Acc: 50.325% (10210/20288)\n",
      "Loss: 1.356 | Acc: 50.319% (10241/20352)\n",
      "Loss: 1.356 | Acc: 50.358% (10281/20416)\n",
      "Loss: 1.355 | Acc: 50.366% (10315/20480)\n",
      "Loss: 1.356 | Acc: 50.321% (10338/20544)\n",
      "Loss: 1.357 | Acc: 50.306% (10367/20608)\n",
      "Loss: 1.357 | Acc: 50.290% (10396/20672)\n",
      "Loss: 1.357 | Acc: 50.256% (10421/20736)\n",
      "Loss: 1.357 | Acc: 50.264% (10455/20800)\n",
      "Loss: 1.357 | Acc: 50.268% (10488/20864)\n",
      "Loss: 1.356 | Acc: 50.315% (10530/20928)\n",
      "Loss: 1.356 | Acc: 50.319% (10563/20992)\n",
      "Loss: 1.357 | Acc: 50.313% (10594/21056)\n",
      "Loss: 1.357 | Acc: 50.317% (10627/21120)\n",
      "Loss: 1.356 | Acc: 50.330% (10662/21184)\n",
      "Loss: 1.358 | Acc: 50.278% (10683/21248)\n",
      "Loss: 1.357 | Acc: 50.310% (10722/21312)\n",
      "Loss: 1.357 | Acc: 50.313% (10755/21376)\n",
      "Loss: 1.357 | Acc: 50.326% (10790/21440)\n",
      "Loss: 1.356 | Acc: 50.321% (10821/21504)\n",
      "Loss: 1.357 | Acc: 50.292% (10847/21568)\n",
      "Loss: 1.358 | Acc: 50.263% (10873/21632)\n",
      "Loss: 1.358 | Acc: 50.258% (10904/21696)\n",
      "Loss: 1.358 | Acc: 50.257% (10936/21760)\n",
      "Loss: 1.357 | Acc: 50.275% (10972/21824)\n",
      "Loss: 1.357 | Acc: 50.260% (11001/21888)\n",
      "Loss: 1.357 | Acc: 50.241% (11029/21952)\n",
      "Loss: 1.357 | Acc: 50.250% (11063/22016)\n",
      "Loss: 1.358 | Acc: 50.240% (11093/22080)\n",
      "Loss: 1.358 | Acc: 50.221% (11121/22144)\n",
      "Loss: 1.358 | Acc: 50.225% (11154/22208)\n",
      "Loss: 1.358 | Acc: 50.242% (11190/22272)\n",
      "Loss: 1.358 | Acc: 50.251% (11224/22336)\n",
      "Loss: 1.358 | Acc: 50.246% (11255/22400)\n",
      "Loss: 1.359 | Acc: 50.209% (11279/22464)\n",
      "Loss: 1.359 | Acc: 50.226% (11315/22528)\n",
      "Loss: 1.358 | Acc: 50.230% (11348/22592)\n",
      "Loss: 1.358 | Acc: 50.238% (11382/22656)\n",
      "Loss: 1.358 | Acc: 50.233% (11413/22720)\n",
      "Loss: 1.358 | Acc: 50.228% (11444/22784)\n",
      "Loss: 1.358 | Acc: 50.197% (11469/22848)\n",
      "Loss: 1.358 | Acc: 50.196% (11501/22912)\n",
      "Loss: 1.358 | Acc: 50.222% (11539/22976)\n",
      "Loss: 1.358 | Acc: 50.217% (11570/23040)\n",
      "Loss: 1.359 | Acc: 50.203% (11599/23104)\n",
      "Loss: 1.358 | Acc: 50.207% (11632/23168)\n",
      "Loss: 1.358 | Acc: 50.194% (11661/23232)\n",
      "Loss: 1.358 | Acc: 50.223% (11700/23296)\n",
      "Loss: 1.358 | Acc: 50.205% (11728/23360)\n",
      "Loss: 1.358 | Acc: 50.205% (11760/23424)\n",
      "Loss: 1.359 | Acc: 50.183% (11787/23488)\n",
      "Loss: 1.359 | Acc: 50.191% (11821/23552)\n",
      "Loss: 1.359 | Acc: 50.178% (11850/23616)\n",
      "Loss: 1.359 | Acc: 50.173% (11881/23680)\n",
      "Loss: 1.358 | Acc: 50.185% (11916/23744)\n",
      "Loss: 1.358 | Acc: 50.214% (11955/23808)\n",
      "Loss: 1.358 | Acc: 50.222% (11989/23872)\n",
      "Loss: 1.358 | Acc: 50.226% (12022/23936)\n",
      "Loss: 1.358 | Acc: 50.217% (12052/24000)\n",
      "Loss: 1.359 | Acc: 50.183% (12076/24064)\n",
      "Loss: 1.359 | Acc: 50.182% (12108/24128)\n",
      "Loss: 1.359 | Acc: 50.190% (12142/24192)\n",
      "Loss: 1.359 | Acc: 50.186% (12173/24256)\n",
      "Loss: 1.360 | Acc: 50.169% (12201/24320)\n",
      "Loss: 1.359 | Acc: 50.205% (12242/24384)\n",
      "Loss: 1.359 | Acc: 50.225% (12279/24448)\n",
      "Loss: 1.359 | Acc: 50.224% (12311/24512)\n",
      "Loss: 1.358 | Acc: 50.244% (12348/24576)\n",
      "Loss: 1.358 | Acc: 50.239% (12379/24640)\n",
      "Loss: 1.358 | Acc: 50.215% (12405/24704)\n",
      "Loss: 1.358 | Acc: 50.202% (12434/24768)\n",
      "Loss: 1.358 | Acc: 50.209% (12468/24832)\n",
      "Loss: 1.358 | Acc: 50.245% (12509/24896)\n",
      "Loss: 1.357 | Acc: 50.248% (12542/24960)\n",
      "Loss: 1.357 | Acc: 50.276% (12581/25024)\n",
      "Loss: 1.356 | Acc: 50.307% (12621/25088)\n",
      "Loss: 1.356 | Acc: 50.318% (12656/25152)\n",
      "Loss: 1.356 | Acc: 50.321% (12689/25216)\n",
      "Loss: 1.355 | Acc: 50.336% (12725/25280)\n",
      "Loss: 1.354 | Acc: 50.367% (12765/25344)\n",
      "Loss: 1.354 | Acc: 50.362% (12796/25408)\n",
      "Loss: 1.354 | Acc: 50.381% (12833/25472)\n",
      "Loss: 1.353 | Acc: 50.384% (12866/25536)\n",
      "Loss: 1.354 | Acc: 50.391% (12900/25600)\n",
      "Loss: 1.354 | Acc: 50.401% (12935/25664)\n",
      "Loss: 1.353 | Acc: 50.412% (12970/25728)\n",
      "Loss: 1.353 | Acc: 50.395% (12998/25792)\n",
      "Loss: 1.354 | Acc: 50.394% (13030/25856)\n",
      "Loss: 1.354 | Acc: 50.394% (13062/25920)\n",
      "Loss: 1.354 | Acc: 50.404% (13097/25984)\n",
      "Loss: 1.354 | Acc: 50.395% (13127/26048)\n",
      "Loss: 1.354 | Acc: 50.410% (13163/26112)\n",
      "Loss: 1.354 | Acc: 50.397% (13192/26176)\n",
      "Loss: 1.354 | Acc: 50.400% (13225/26240)\n",
      "Loss: 1.354 | Acc: 50.411% (13260/26304)\n",
      "Loss: 1.354 | Acc: 50.440% (13300/26368)\n",
      "Loss: 1.354 | Acc: 50.454% (13336/26432)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.354 | Acc: 50.445% (13366/26496)\n",
      "Loss: 1.354 | Acc: 50.448% (13399/26560)\n",
      "Loss: 1.354 | Acc: 50.454% (13433/26624)\n",
      "Loss: 1.353 | Acc: 50.468% (13469/26688)\n",
      "Loss: 1.354 | Acc: 50.467% (13501/26752)\n",
      "Loss: 1.354 | Acc: 50.462% (13532/26816)\n",
      "Loss: 1.354 | Acc: 50.454% (13562/26880)\n",
      "Loss: 1.354 | Acc: 50.438% (13590/26944)\n",
      "Loss: 1.354 | Acc: 50.444% (13624/27008)\n",
      "Loss: 1.354 | Acc: 50.447% (13657/27072)\n",
      "Loss: 1.354 | Acc: 50.431% (13685/27136)\n",
      "Loss: 1.354 | Acc: 50.426% (13716/27200)\n",
      "Loss: 1.354 | Acc: 50.407% (13743/27264)\n",
      "Loss: 1.354 | Acc: 50.403% (13774/27328)\n",
      "Loss: 1.354 | Acc: 50.398% (13805/27392)\n",
      "Loss: 1.355 | Acc: 50.393% (13836/27456)\n",
      "Loss: 1.355 | Acc: 50.378% (13864/27520)\n",
      "Loss: 1.356 | Acc: 50.359% (13891/27584)\n",
      "Loss: 1.356 | Acc: 50.354% (13922/27648)\n",
      "Loss: 1.356 | Acc: 50.346% (13952/27712)\n",
      "Loss: 1.356 | Acc: 50.364% (13989/27776)\n",
      "Loss: 1.356 | Acc: 50.356% (14019/27840)\n",
      "Loss: 1.356 | Acc: 50.355% (14051/27904)\n",
      "Loss: 1.357 | Acc: 50.354% (14083/27968)\n",
      "Loss: 1.357 | Acc: 50.353% (14115/28032)\n",
      "Loss: 1.357 | Acc: 50.352% (14147/28096)\n",
      "Loss: 1.356 | Acc: 50.369% (14184/28160)\n",
      "Loss: 1.356 | Acc: 50.368% (14216/28224)\n",
      "Loss: 1.357 | Acc: 50.332% (14238/28288)\n",
      "Loss: 1.357 | Acc: 50.328% (14269/28352)\n",
      "Loss: 1.357 | Acc: 50.324% (14300/28416)\n",
      "Loss: 1.357 | Acc: 50.334% (14335/28480)\n",
      "Loss: 1.358 | Acc: 50.336% (14368/28544)\n",
      "Loss: 1.358 | Acc: 50.332% (14399/28608)\n",
      "Loss: 1.358 | Acc: 50.335% (14432/28672)\n",
      "Loss: 1.357 | Acc: 50.345% (14467/28736)\n",
      "Loss: 1.357 | Acc: 50.375% (14508/28800)\n",
      "Loss: 1.356 | Acc: 50.374% (14540/28864)\n",
      "Loss: 1.356 | Acc: 50.394% (14578/28928)\n",
      "Loss: 1.356 | Acc: 50.407% (14614/28992)\n",
      "Loss: 1.356 | Acc: 50.410% (14647/29056)\n",
      "Loss: 1.356 | Acc: 50.429% (14685/29120)\n",
      "Loss: 1.356 | Acc: 50.418% (14714/29184)\n",
      "Loss: 1.356 | Acc: 50.414% (14745/29248)\n",
      "Loss: 1.356 | Acc: 50.430% (14782/29312)\n",
      "Loss: 1.356 | Acc: 50.436% (14816/29376)\n",
      "Loss: 1.356 | Acc: 50.459% (14855/29440)\n",
      "Loss: 1.355 | Acc: 50.471% (14891/29504)\n",
      "Loss: 1.355 | Acc: 50.470% (14923/29568)\n",
      "Loss: 1.355 | Acc: 50.476% (14957/29632)\n",
      "Loss: 1.356 | Acc: 50.465% (14986/29696)\n",
      "Loss: 1.355 | Acc: 50.474% (15021/29760)\n",
      "Loss: 1.355 | Acc: 50.493% (15059/29824)\n",
      "Loss: 1.355 | Acc: 50.509% (15096/29888)\n",
      "Loss: 1.355 | Acc: 50.507% (15128/29952)\n",
      "Loss: 1.355 | Acc: 50.496% (15157/30016)\n",
      "Loss: 1.355 | Acc: 50.515% (15195/30080)\n",
      "Loss: 1.354 | Acc: 50.534% (15233/30144)\n",
      "Loss: 1.355 | Acc: 50.536% (15266/30208)\n",
      "Loss: 1.354 | Acc: 50.535% (15298/30272)\n",
      "Loss: 1.354 | Acc: 50.537% (15331/30336)\n",
      "Loss: 1.354 | Acc: 50.543% (15365/30400)\n",
      "Loss: 1.354 | Acc: 50.542% (15397/30464)\n",
      "Loss: 1.354 | Acc: 50.573% (15439/30528)\n",
      "Loss: 1.354 | Acc: 50.575% (15472/30592)\n",
      "Loss: 1.354 | Acc: 50.568% (15502/30656)\n",
      "Loss: 1.354 | Acc: 50.544% (15527/30720)\n",
      "Loss: 1.355 | Acc: 50.536% (15557/30784)\n",
      "Loss: 1.354 | Acc: 50.548% (15593/30848)\n",
      "Loss: 1.355 | Acc: 50.540% (15623/30912)\n",
      "Loss: 1.355 | Acc: 50.536% (15654/30976)\n",
      "Loss: 1.355 | Acc: 50.538% (15687/31040)\n",
      "Loss: 1.355 | Acc: 50.540% (15720/31104)\n",
      "Loss: 1.355 | Acc: 50.555% (15757/31168)\n",
      "Loss: 1.354 | Acc: 50.573% (15795/31232)\n",
      "Loss: 1.355 | Acc: 50.540% (15817/31296)\n",
      "Loss: 1.355 | Acc: 50.529% (15846/31360)\n",
      "Loss: 1.355 | Acc: 50.554% (15886/31424)\n",
      "Loss: 1.355 | Acc: 50.578% (15926/31488)\n",
      "Loss: 1.355 | Acc: 50.580% (15959/31552)\n",
      "Loss: 1.355 | Acc: 50.572% (15989/31616)\n",
      "Loss: 1.355 | Acc: 50.559% (16017/31680)\n",
      "Loss: 1.355 | Acc: 50.567% (16052/31744)\n",
      "Loss: 1.355 | Acc: 50.575% (16087/31808)\n",
      "Loss: 1.355 | Acc: 50.577% (16120/31872)\n",
      "Loss: 1.355 | Acc: 50.579% (16153/31936)\n",
      "Loss: 1.354 | Acc: 50.600% (16192/32000)\n",
      "Loss: 1.355 | Acc: 50.596% (16223/32064)\n",
      "Loss: 1.354 | Acc: 50.623% (16264/32128)\n",
      "Loss: 1.354 | Acc: 50.631% (16299/32192)\n",
      "Loss: 1.354 | Acc: 50.642% (16335/32256)\n",
      "Loss: 1.353 | Acc: 50.647% (16369/32320)\n",
      "Loss: 1.353 | Acc: 50.639% (16399/32384)\n",
      "Loss: 1.353 | Acc: 50.619% (16425/32448)\n",
      "Loss: 1.353 | Acc: 50.621% (16458/32512)\n",
      "Loss: 1.353 | Acc: 50.617% (16489/32576)\n",
      "Loss: 1.353 | Acc: 50.619% (16522/32640)\n",
      "Loss: 1.353 | Acc: 50.615% (16553/32704)\n",
      "Loss: 1.353 | Acc: 50.629% (16590/32768)\n",
      "Loss: 1.354 | Acc: 50.615% (16618/32832)\n",
      "Loss: 1.354 | Acc: 50.611% (16649/32896)\n",
      "Loss: 1.354 | Acc: 50.610% (16681/32960)\n",
      "Loss: 1.354 | Acc: 50.612% (16714/33024)\n",
      "Loss: 1.354 | Acc: 50.623% (16750/33088)\n",
      "Loss: 1.354 | Acc: 50.621% (16782/33152)\n",
      "Loss: 1.354 | Acc: 50.611% (16811/33216)\n",
      "Loss: 1.354 | Acc: 50.619% (16846/33280)\n",
      "Loss: 1.354 | Acc: 50.600% (16872/33344)\n",
      "Loss: 1.353 | Acc: 50.617% (16910/33408)\n",
      "Loss: 1.353 | Acc: 50.627% (16946/33472)\n",
      "Loss: 1.353 | Acc: 50.626% (16978/33536)\n",
      "Loss: 1.353 | Acc: 50.643% (17016/33600)\n",
      "Loss: 1.353 | Acc: 50.645% (17049/33664)\n",
      "Loss: 1.353 | Acc: 50.640% (17080/33728)\n",
      "Loss: 1.353 | Acc: 50.642% (17113/33792)\n",
      "Loss: 1.353 | Acc: 50.620% (17138/33856)\n",
      "Loss: 1.353 | Acc: 50.628% (17173/33920)\n",
      "Loss: 1.353 | Acc: 50.633% (17207/33984)\n",
      "Loss: 1.353 | Acc: 50.631% (17239/34048)\n",
      "Loss: 1.353 | Acc: 50.627% (17270/34112)\n",
      "Loss: 1.353 | Acc: 50.620% (17300/34176)\n",
      "Loss: 1.353 | Acc: 50.613% (17330/34240)\n",
      "Loss: 1.353 | Acc: 50.612% (17362/34304)\n",
      "Loss: 1.353 | Acc: 50.585% (17385/34368)\n",
      "Loss: 1.353 | Acc: 50.587% (17418/34432)\n",
      "Loss: 1.354 | Acc: 50.577% (17447/34496)\n",
      "Loss: 1.354 | Acc: 50.561% (17474/34560)\n",
      "Loss: 1.354 | Acc: 50.572% (17510/34624)\n",
      "Loss: 1.354 | Acc: 50.582% (17546/34688)\n",
      "Loss: 1.354 | Acc: 50.570% (17574/34752)\n",
      "Loss: 1.355 | Acc: 50.563% (17604/34816)\n",
      "Loss: 1.355 | Acc: 50.553% (17633/34880)\n",
      "Loss: 1.355 | Acc: 50.555% (17666/34944)\n",
      "Loss: 1.355 | Acc: 50.557% (17699/35008)\n",
      "Loss: 1.355 | Acc: 50.550% (17729/35072)\n",
      "Loss: 1.355 | Acc: 50.564% (17766/35136)\n",
      "Loss: 1.354 | Acc: 50.588% (17807/35200)\n",
      "Loss: 1.355 | Acc: 50.576% (17835/35264)\n",
      "Loss: 1.355 | Acc: 50.572% (17866/35328)\n",
      "Loss: 1.355 | Acc: 50.557% (17893/35392)\n",
      "Loss: 1.355 | Acc: 50.561% (17927/35456)\n",
      "Loss: 1.355 | Acc: 50.560% (17959/35520)\n",
      "Loss: 1.355 | Acc: 50.556% (17990/35584)\n",
      "Loss: 1.355 | Acc: 50.572% (18028/35648)\n",
      "Loss: 1.355 | Acc: 50.571% (18060/35712)\n",
      "Loss: 1.355 | Acc: 50.576% (18094/35776)\n",
      "Loss: 1.355 | Acc: 50.578% (18127/35840)\n",
      "Loss: 1.354 | Acc: 50.602% (18168/35904)\n",
      "Loss: 1.354 | Acc: 50.614% (18205/35968)\n",
      "Loss: 1.354 | Acc: 50.605% (18234/36032)\n",
      "Loss: 1.354 | Acc: 50.598% (18264/36096)\n",
      "Loss: 1.355 | Acc: 50.595% (18295/36160)\n",
      "Loss: 1.355 | Acc: 50.580% (18322/36224)\n",
      "Loss: 1.354 | Acc: 50.587% (18357/36288)\n",
      "Loss: 1.354 | Acc: 50.600% (18394/36352)\n",
      "Loss: 1.354 | Acc: 50.601% (18427/36416)\n",
      "Loss: 1.354 | Acc: 50.606% (18461/36480)\n",
      "Loss: 1.355 | Acc: 50.594% (18489/36544)\n",
      "Loss: 1.354 | Acc: 50.604% (18525/36608)\n",
      "Loss: 1.355 | Acc: 50.584% (18550/36672)\n",
      "Loss: 1.355 | Acc: 50.585% (18583/36736)\n",
      "Loss: 1.355 | Acc: 50.584% (18615/36800)\n",
      "Loss: 1.355 | Acc: 50.581% (18646/36864)\n",
      "Loss: 1.355 | Acc: 50.582% (18679/36928)\n",
      "Loss: 1.355 | Acc: 50.581% (18711/36992)\n",
      "Loss: 1.355 | Acc: 50.586% (18745/37056)\n",
      "Loss: 1.355 | Acc: 50.574% (18773/37120)\n",
      "Loss: 1.355 | Acc: 50.578% (18807/37184)\n",
      "Loss: 1.355 | Acc: 50.591% (18844/37248)\n",
      "Loss: 1.355 | Acc: 50.598% (18879/37312)\n",
      "Loss: 1.355 | Acc: 50.583% (18906/37376)\n",
      "Loss: 1.355 | Acc: 50.590% (18941/37440)\n",
      "Loss: 1.356 | Acc: 50.576% (18968/37504)\n",
      "Loss: 1.355 | Acc: 50.586% (19004/37568)\n",
      "Loss: 1.355 | Acc: 50.577% (19033/37632)\n",
      "Loss: 1.355 | Acc: 50.581% (19067/37696)\n",
      "Loss: 1.355 | Acc: 50.583% (19100/37760)\n",
      "Loss: 1.355 | Acc: 50.574% (19129/37824)\n",
      "Loss: 1.356 | Acc: 50.554% (19154/37888)\n",
      "Loss: 1.356 | Acc: 50.530% (19177/37952)\n",
      "Loss: 1.356 | Acc: 50.531% (19210/38016)\n",
      "Loss: 1.356 | Acc: 50.528% (19241/38080)\n",
      "Loss: 1.356 | Acc: 50.527% (19273/38144)\n",
      "Loss: 1.356 | Acc: 50.552% (19315/38208)\n",
      "Loss: 1.356 | Acc: 50.551% (19347/38272)\n",
      "Loss: 1.356 | Acc: 50.550% (19379/38336)\n",
      "Loss: 1.356 | Acc: 50.549% (19411/38400)\n",
      "Loss: 1.356 | Acc: 50.562% (19448/38464)\n",
      "Loss: 1.356 | Acc: 50.574% (19485/38528)\n",
      "Loss: 1.356 | Acc: 50.573% (19517/38592)\n",
      "Loss: 1.356 | Acc: 50.582% (19553/38656)\n",
      "Loss: 1.356 | Acc: 50.573% (19582/38720)\n",
      "Loss: 1.355 | Acc: 50.575% (19615/38784)\n",
      "Loss: 1.356 | Acc: 50.571% (19646/38848)\n",
      "Loss: 1.355 | Acc: 50.589% (19685/38912)\n",
      "Loss: 1.355 | Acc: 50.585% (19716/38976)\n",
      "Loss: 1.355 | Acc: 50.612% (19759/39040)\n",
      "Loss: 1.355 | Acc: 50.619% (19794/39104)\n",
      "Loss: 1.355 | Acc: 50.620% (19827/39168)\n",
      "Loss: 1.355 | Acc: 50.619% (19859/39232)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.355 | Acc: 50.611% (19888/39296)\n",
      "Loss: 1.355 | Acc: 50.607% (19919/39360)\n",
      "Loss: 1.355 | Acc: 50.614% (19954/39424)\n",
      "Loss: 1.355 | Acc: 50.610% (19985/39488)\n",
      "Loss: 1.355 | Acc: 50.602% (20014/39552)\n",
      "Loss: 1.355 | Acc: 50.596% (20044/39616)\n",
      "Loss: 1.355 | Acc: 50.607% (20081/39680)\n",
      "Loss: 1.355 | Acc: 50.624% (20120/39744)\n",
      "Loss: 1.355 | Acc: 50.626% (20153/39808)\n",
      "Loss: 1.355 | Acc: 50.622% (20184/39872)\n",
      "Loss: 1.355 | Acc: 50.608% (20211/39936)\n",
      "Loss: 1.355 | Acc: 50.600% (20240/40000)\n",
      "Loss: 1.355 | Acc: 50.607% (20275/40064)\n",
      "Loss: 1.355 | Acc: 50.611% (20309/40128)\n",
      "Loss: 1.355 | Acc: 50.617% (20344/40192)\n",
      "Loss: 1.355 | Acc: 50.614% (20375/40256)\n",
      "Loss: 1.355 | Acc: 50.627% (20413/40320)\n",
      "Loss: 1.354 | Acc: 50.646% (20453/40384)\n",
      "Loss: 1.354 | Acc: 50.653% (20488/40448)\n",
      "Loss: 1.354 | Acc: 50.657% (20522/40512)\n",
      "Loss: 1.354 | Acc: 50.656% (20554/40576)\n",
      "Loss: 1.355 | Acc: 50.640% (20580/40640)\n",
      "Loss: 1.355 | Acc: 50.636% (20611/40704)\n",
      "Loss: 1.355 | Acc: 50.638% (20644/40768)\n",
      "Loss: 1.355 | Acc: 50.659% (20685/40832)\n",
      "Loss: 1.355 | Acc: 50.655% (20716/40896)\n",
      "Loss: 1.355 | Acc: 50.667% (20753/40960)\n",
      "Loss: 1.355 | Acc: 50.673% (20788/41024)\n",
      "Loss: 1.355 | Acc: 50.684% (20825/41088)\n",
      "Loss: 1.355 | Acc: 50.680% (20856/41152)\n",
      "Loss: 1.355 | Acc: 50.679% (20888/41216)\n",
      "Loss: 1.355 | Acc: 50.683% (20922/41280)\n",
      "Loss: 1.355 | Acc: 50.675% (20951/41344)\n",
      "Loss: 1.355 | Acc: 50.674% (20983/41408)\n",
      "Loss: 1.355 | Acc: 50.666% (21012/41472)\n",
      "Loss: 1.355 | Acc: 50.660% (21042/41536)\n",
      "Loss: 1.355 | Acc: 50.649% (21070/41600)\n",
      "Loss: 1.355 | Acc: 50.646% (21101/41664)\n",
      "Loss: 1.355 | Acc: 50.649% (21135/41728)\n",
      "Loss: 1.355 | Acc: 50.672% (21177/41792)\n",
      "Loss: 1.354 | Acc: 50.671% (21209/41856)\n",
      "Loss: 1.355 | Acc: 50.668% (21240/41920)\n",
      "Loss: 1.355 | Acc: 50.684% (21279/41984)\n",
      "Loss: 1.354 | Acc: 50.687% (21313/42048)\n",
      "Loss: 1.355 | Acc: 50.670% (21338/42112)\n",
      "Loss: 1.355 | Acc: 50.657% (21365/42176)\n",
      "Loss: 1.354 | Acc: 50.668% (21402/42240)\n",
      "Loss: 1.354 | Acc: 50.655% (21429/42304)\n",
      "Loss: 1.354 | Acc: 50.647% (21458/42368)\n",
      "Loss: 1.354 | Acc: 50.667% (21499/42432)\n",
      "Loss: 1.354 | Acc: 50.657% (21527/42496)\n",
      "Loss: 1.355 | Acc: 50.653% (21558/42560)\n",
      "Loss: 1.354 | Acc: 50.659% (21593/42624)\n",
      "Loss: 1.354 | Acc: 50.644% (21619/42688)\n",
      "Loss: 1.354 | Acc: 50.646% (21652/42752)\n",
      "Loss: 1.355 | Acc: 50.652% (21687/42816)\n",
      "Loss: 1.355 | Acc: 50.632% (21711/42880)\n",
      "Loss: 1.355 | Acc: 50.640% (21747/42944)\n",
      "Loss: 1.355 | Acc: 50.623% (21772/43008)\n",
      "Loss: 1.355 | Acc: 50.629% (21807/43072)\n",
      "Loss: 1.355 | Acc: 50.631% (21840/43136)\n",
      "Loss: 1.355 | Acc: 50.632% (21873/43200)\n",
      "Loss: 1.355 | Acc: 50.645% (21911/43264)\n",
      "Loss: 1.354 | Acc: 50.655% (21948/43328)\n",
      "Loss: 1.355 | Acc: 50.650% (21978/43392)\n",
      "Loss: 1.355 | Acc: 50.649% (22010/43456)\n",
      "Loss: 1.355 | Acc: 50.636% (22037/43520)\n",
      "Loss: 1.355 | Acc: 50.636% (22069/43584)\n",
      "Loss: 1.355 | Acc: 50.637% (22102/43648)\n",
      "Loss: 1.355 | Acc: 50.647% (22139/43712)\n",
      "Loss: 1.355 | Acc: 50.662% (22178/43776)\n",
      "Loss: 1.355 | Acc: 50.659% (22209/43840)\n",
      "Loss: 1.355 | Acc: 50.649% (22237/43904)\n",
      "Loss: 1.355 | Acc: 50.641% (22266/43968)\n",
      "Loss: 1.355 | Acc: 50.640% (22298/44032)\n",
      "Loss: 1.355 | Acc: 50.642% (22331/44096)\n",
      "Loss: 1.355 | Acc: 50.632% (22359/44160)\n",
      "Loss: 1.354 | Acc: 50.644% (22397/44224)\n",
      "Loss: 1.354 | Acc: 50.646% (22430/44288)\n",
      "Loss: 1.355 | Acc: 50.631% (22456/44352)\n",
      "Loss: 1.354 | Acc: 50.657% (22500/44416)\n",
      "Loss: 1.354 | Acc: 50.650% (22529/44480)\n",
      "Loss: 1.354 | Acc: 50.662% (22567/44544)\n",
      "Loss: 1.354 | Acc: 50.648% (22593/44608)\n",
      "Loss: 1.354 | Acc: 50.654% (22628/44672)\n",
      "Loss: 1.354 | Acc: 50.664% (22665/44736)\n",
      "Loss: 1.354 | Acc: 50.665% (22698/44800)\n",
      "Loss: 1.354 | Acc: 50.664% (22730/44864)\n",
      "Loss: 1.354 | Acc: 50.659% (22760/44928)\n",
      "Loss: 1.354 | Acc: 50.662% (22794/44992)\n",
      "Loss: 1.353 | Acc: 50.668% (22829/45056)\n",
      "Loss: 1.353 | Acc: 50.663% (22859/45120)\n",
      "Loss: 1.353 | Acc: 50.655% (22888/45184)\n",
      "Loss: 1.353 | Acc: 50.661% (22923/45248)\n",
      "Loss: 1.353 | Acc: 50.669% (22959/45312)\n",
      "Loss: 1.353 | Acc: 50.677% (22995/45376)\n",
      "Loss: 1.353 | Acc: 50.691% (23034/45440)\n",
      "Loss: 1.353 | Acc: 50.688% (23065/45504)\n",
      "Loss: 1.353 | Acc: 50.702% (23104/45568)\n",
      "Loss: 1.352 | Acc: 50.701% (23136/45632)\n",
      "Loss: 1.352 | Acc: 50.707% (23171/45696)\n",
      "Loss: 1.353 | Acc: 50.695% (23198/45760)\n",
      "Loss: 1.352 | Acc: 50.687% (23227/45824)\n",
      "Loss: 1.352 | Acc: 50.691% (23261/45888)\n",
      "Loss: 1.352 | Acc: 50.677% (23287/45952)\n",
      "Loss: 1.352 | Acc: 50.678% (23320/46016)\n",
      "Loss: 1.352 | Acc: 50.690% (23358/46080)\n",
      "Loss: 1.352 | Acc: 50.691% (23391/46144)\n",
      "Loss: 1.352 | Acc: 50.684% (23420/46208)\n",
      "Loss: 1.352 | Acc: 50.692% (23456/46272)\n",
      "Loss: 1.352 | Acc: 50.695% (23490/46336)\n",
      "Loss: 1.352 | Acc: 50.711% (23530/46400)\n",
      "Loss: 1.352 | Acc: 50.704% (23559/46464)\n",
      "Loss: 1.352 | Acc: 50.701% (23590/46528)\n",
      "Loss: 1.352 | Acc: 50.702% (23623/46592)\n",
      "Loss: 1.352 | Acc: 50.701% (23655/46656)\n",
      "Loss: 1.352 | Acc: 50.702% (23688/46720)\n",
      "Loss: 1.352 | Acc: 50.716% (23727/46784)\n",
      "Loss: 1.352 | Acc: 50.721% (23762/46848)\n",
      "Loss: 1.352 | Acc: 50.729% (23798/46912)\n",
      "Loss: 1.352 | Acc: 50.728% (23830/46976)\n",
      "Loss: 1.352 | Acc: 50.733% (23865/47040)\n",
      "Loss: 1.351 | Acc: 50.735% (23898/47104)\n",
      "Loss: 1.351 | Acc: 50.731% (23929/47168)\n",
      "Loss: 1.351 | Acc: 50.741% (23966/47232)\n",
      "Loss: 1.352 | Acc: 50.732% (23994/47296)\n",
      "Loss: 1.352 | Acc: 50.735% (24028/47360)\n",
      "Loss: 1.351 | Acc: 50.740% (24063/47424)\n",
      "Loss: 1.351 | Acc: 50.748% (24099/47488)\n",
      "Loss: 1.351 | Acc: 50.742% (24129/47552)\n",
      "Loss: 1.352 | Acc: 50.741% (24161/47616)\n",
      "Loss: 1.351 | Acc: 50.747% (24196/47680)\n",
      "Loss: 1.351 | Acc: 50.739% (24225/47744)\n",
      "Loss: 1.352 | Acc: 50.740% (24258/47808)\n",
      "Loss: 1.351 | Acc: 50.746% (24293/47872)\n",
      "Loss: 1.351 | Acc: 50.753% (24329/47936)\n",
      "Loss: 1.351 | Acc: 50.754% (24362/48000)\n",
      "Loss: 1.351 | Acc: 50.761% (24398/48064)\n",
      "Loss: 1.351 | Acc: 50.769% (24434/48128)\n",
      "Loss: 1.351 | Acc: 50.766% (24465/48192)\n",
      "Loss: 1.351 | Acc: 50.763% (24496/48256)\n",
      "Loss: 1.351 | Acc: 50.766% (24530/48320)\n",
      "Loss: 1.351 | Acc: 50.765% (24562/48384)\n",
      "Loss: 1.351 | Acc: 50.766% (24595/48448)\n",
      "Loss: 1.350 | Acc: 50.761% (24625/48512)\n",
      "Loss: 1.350 | Acc: 50.768% (24661/48576)\n",
      "Loss: 1.350 | Acc: 50.781% (24700/48640)\n",
      "Loss: 1.350 | Acc: 50.795% (24739/48704)\n",
      "Loss: 1.350 | Acc: 50.794% (24771/48768)\n",
      "Loss: 1.350 | Acc: 50.786% (24800/48832)\n",
      "Loss: 1.350 | Acc: 50.781% (24830/48896)\n",
      "Loss: 1.350 | Acc: 50.780% (24862/48960)\n",
      "Loss: 1.350 | Acc: 50.778% (24881/49000)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 50.77755102040816\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.380 | Acc: 48.438% (31/64)\n",
      "Loss: 1.257 | Acc: 53.906% (69/128)\n",
      "Loss: 1.310 | Acc: 51.042% (98/192)\n",
      "Loss: 1.333 | Acc: 51.953% (133/256)\n",
      "Loss: 1.278 | Acc: 54.062% (173/320)\n",
      "Loss: 1.301 | Acc: 53.125% (204/384)\n",
      "Loss: 1.302 | Acc: 53.125% (238/448)\n",
      "Loss: 1.306 | Acc: 51.758% (265/512)\n",
      "Loss: 1.291 | Acc: 52.604% (303/576)\n",
      "Loss: 1.284 | Acc: 53.125% (340/640)\n",
      "Loss: 1.307 | Acc: 52.273% (368/704)\n",
      "Loss: 1.314 | Acc: 51.953% (399/768)\n",
      "Loss: 1.312 | Acc: 52.404% (436/832)\n",
      "Loss: 1.299 | Acc: 53.013% (475/896)\n",
      "Loss: 1.289 | Acc: 53.542% (514/960)\n",
      "Loss: 1.274 | Acc: 54.492% (558/1024)\n",
      "Loss: 1.279 | Acc: 54.228% (590/1088)\n",
      "Loss: 1.276 | Acc: 54.427% (627/1152)\n",
      "Loss: 1.270 | Acc: 54.770% (666/1216)\n",
      "Loss: 1.289 | Acc: 53.750% (688/1280)\n",
      "Loss: 1.288 | Acc: 53.571% (720/1344)\n",
      "Loss: 1.284 | Acc: 53.693% (756/1408)\n",
      "Loss: 1.276 | Acc: 53.804% (792/1472)\n",
      "Loss: 1.277 | Acc: 53.971% (829/1536)\n",
      "Loss: 1.281 | Acc: 53.750% (860/1600)\n",
      "Loss: 1.285 | Acc: 53.786% (895/1664)\n",
      "Loss: 1.284 | Acc: 53.877% (931/1728)\n",
      "Loss: 1.283 | Acc: 53.962% (967/1792)\n",
      "Loss: 1.279 | Acc: 54.095% (1004/1856)\n",
      "Loss: 1.281 | Acc: 54.271% (1042/1920)\n",
      "Loss: 1.285 | Acc: 54.032% (1072/1984)\n",
      "Loss: 1.288 | Acc: 54.004% (1106/2048)\n",
      "Loss: 1.281 | Acc: 54.072% (1142/2112)\n",
      "Loss: 1.283 | Acc: 53.860% (1172/2176)\n",
      "Loss: 1.285 | Acc: 53.705% (1203/2240)\n",
      "Loss: 1.284 | Acc: 53.689% (1237/2304)\n",
      "Loss: 1.288 | Acc: 53.505% (1267/2368)\n",
      "Loss: 1.288 | Acc: 53.536% (1302/2432)\n",
      "Loss: 1.284 | Acc: 53.606% (1338/2496)\n",
      "Loss: 1.294 | Acc: 53.281% (1364/2560)\n",
      "Loss: 1.293 | Acc: 53.163% (1395/2624)\n",
      "Loss: 1.291 | Acc: 53.385% (1435/2688)\n",
      "Loss: 1.290 | Acc: 53.416% (1470/2752)\n",
      "Loss: 1.290 | Acc: 53.409% (1504/2816)\n",
      "Loss: 1.288 | Acc: 53.542% (1542/2880)\n",
      "Loss: 1.288 | Acc: 53.567% (1577/2944)\n",
      "Loss: 1.288 | Acc: 53.557% (1611/3008)\n",
      "Loss: 1.291 | Acc: 53.483% (1643/3072)\n",
      "Loss: 1.289 | Acc: 53.508% (1678/3136)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.287 | Acc: 53.625% (1716/3200)\n",
      "Loss: 1.288 | Acc: 53.615% (1750/3264)\n",
      "Loss: 1.287 | Acc: 53.606% (1784/3328)\n",
      "Loss: 1.287 | Acc: 53.626% (1819/3392)\n",
      "Loss: 1.287 | Acc: 53.617% (1853/3456)\n",
      "Loss: 1.285 | Acc: 53.636% (1888/3520)\n",
      "Loss: 1.284 | Acc: 53.767% (1927/3584)\n",
      "Loss: 1.286 | Acc: 53.755% (1961/3648)\n",
      "Loss: 1.283 | Acc: 53.852% (1999/3712)\n",
      "Loss: 1.282 | Acc: 53.840% (2033/3776)\n",
      "Loss: 1.282 | Acc: 53.854% (2068/3840)\n",
      "Loss: 1.284 | Acc: 53.765% (2099/3904)\n",
      "Loss: 1.283 | Acc: 53.831% (2136/3968)\n",
      "Loss: 1.284 | Acc: 53.770% (2168/4032)\n",
      "Loss: 1.285 | Acc: 53.809% (2204/4096)\n",
      "Loss: 1.286 | Acc: 53.798% (2238/4160)\n",
      "Loss: 1.285 | Acc: 53.741% (2270/4224)\n",
      "Loss: 1.284 | Acc: 53.778% (2306/4288)\n",
      "Loss: 1.283 | Acc: 53.814% (2342/4352)\n",
      "Loss: 1.280 | Acc: 53.963% (2383/4416)\n",
      "Loss: 1.278 | Acc: 54.040% (2421/4480)\n",
      "Loss: 1.277 | Acc: 54.071% (2457/4544)\n",
      "Loss: 1.279 | Acc: 53.993% (2488/4608)\n",
      "Loss: 1.279 | Acc: 53.981% (2522/4672)\n",
      "Loss: 1.277 | Acc: 53.991% (2557/4736)\n",
      "Loss: 1.279 | Acc: 54.021% (2593/4800)\n",
      "Loss: 1.279 | Acc: 53.968% (2625/4864)\n",
      "Loss: 1.278 | Acc: 54.058% (2664/4928)\n",
      "Loss: 1.278 | Acc: 53.986% (2695/4992)\n",
      "Loss: 1.278 | Acc: 53.975% (2729/5056)\n",
      "Loss: 1.279 | Acc: 53.887% (2759/5120)\n",
      "Loss: 1.278 | Acc: 54.070% (2803/5184)\n",
      "Loss: 1.277 | Acc: 54.021% (2835/5248)\n",
      "Loss: 1.276 | Acc: 54.029% (2870/5312)\n",
      "Loss: 1.276 | Acc: 53.999% (2903/5376)\n",
      "Loss: 1.278 | Acc: 53.879% (2931/5440)\n",
      "Loss: 1.277 | Acc: 53.943% (2969/5504)\n",
      "Loss: 1.280 | Acc: 53.861% (2999/5568)\n",
      "Loss: 1.283 | Acc: 53.817% (3031/5632)\n",
      "Loss: 1.281 | Acc: 53.950% (3073/5696)\n",
      "Loss: 1.279 | Acc: 53.941% (3107/5760)\n",
      "Loss: 1.278 | Acc: 54.001% (3145/5824)\n",
      "Loss: 1.280 | Acc: 53.923% (3175/5888)\n",
      "Loss: 1.280 | Acc: 53.881% (3207/5952)\n",
      "Loss: 1.280 | Acc: 53.890% (3242/6016)\n",
      "Loss: 1.281 | Acc: 53.816% (3272/6080)\n",
      "Loss: 1.281 | Acc: 53.792% (3305/6144)\n",
      "Loss: 1.281 | Acc: 53.737% (3336/6208)\n",
      "Loss: 1.282 | Acc: 53.699% (3368/6272)\n",
      "Loss: 1.283 | Acc: 53.709% (3403/6336)\n",
      "Loss: 1.282 | Acc: 53.688% (3436/6400)\n",
      "Loss: 1.284 | Acc: 53.620% (3466/6464)\n",
      "Loss: 1.283 | Acc: 53.585% (3498/6528)\n",
      "Loss: 1.285 | Acc: 53.535% (3529/6592)\n",
      "Loss: 1.284 | Acc: 53.561% (3565/6656)\n",
      "Loss: 1.285 | Acc: 53.557% (3599/6720)\n",
      "Loss: 1.284 | Acc: 53.641% (3639/6784)\n",
      "Loss: 1.282 | Acc: 53.665% (3675/6848)\n",
      "Loss: 1.285 | Acc: 53.602% (3705/6912)\n",
      "Loss: 1.287 | Acc: 53.627% (3741/6976)\n",
      "Loss: 1.288 | Acc: 53.594% (3773/7040)\n",
      "Loss: 1.287 | Acc: 53.632% (3810/7104)\n",
      "Loss: 1.288 | Acc: 53.627% (3844/7168)\n",
      "Loss: 1.289 | Acc: 53.554% (3873/7232)\n",
      "Loss: 1.286 | Acc: 53.646% (3914/7296)\n",
      "Loss: 1.284 | Acc: 53.723% (3954/7360)\n",
      "Loss: 1.284 | Acc: 53.677% (3985/7424)\n",
      "Loss: 1.284 | Acc: 53.699% (4021/7488)\n",
      "Loss: 1.283 | Acc: 53.694% (4055/7552)\n",
      "Loss: 1.283 | Acc: 53.768% (4095/7616)\n",
      "Loss: 1.281 | Acc: 53.789% (4131/7680)\n",
      "Loss: 1.279 | Acc: 53.990% (4181/7744)\n",
      "Loss: 1.277 | Acc: 54.073% (4222/7808)\n",
      "Loss: 1.278 | Acc: 54.014% (4252/7872)\n",
      "Loss: 1.278 | Acc: 53.994% (4285/7936)\n",
      "Loss: 1.279 | Acc: 53.950% (4316/8000)\n",
      "Loss: 1.279 | Acc: 53.931% (4349/8064)\n",
      "Loss: 1.280 | Acc: 53.949% (4385/8128)\n",
      "Loss: 1.279 | Acc: 53.955% (4420/8192)\n",
      "Loss: 1.280 | Acc: 53.937% (4453/8256)\n",
      "Loss: 1.282 | Acc: 53.846% (4480/8320)\n",
      "Loss: 1.283 | Acc: 53.805% (4511/8384)\n",
      "Loss: 1.282 | Acc: 53.800% (4545/8448)\n",
      "Loss: 1.283 | Acc: 53.748% (4575/8512)\n",
      "Loss: 1.283 | Acc: 53.790% (4613/8576)\n",
      "Loss: 1.285 | Acc: 53.750% (4644/8640)\n",
      "Loss: 1.285 | Acc: 53.722% (4676/8704)\n",
      "Loss: 1.285 | Acc: 53.718% (4710/8768)\n",
      "Loss: 1.285 | Acc: 53.725% (4745/8832)\n",
      "Loss: 1.284 | Acc: 53.777% (4784/8896)\n",
      "Loss: 1.283 | Acc: 53.772% (4818/8960)\n",
      "Loss: 1.284 | Acc: 53.757% (4851/9024)\n",
      "Loss: 1.284 | Acc: 53.708% (4881/9088)\n",
      "Loss: 1.284 | Acc: 53.704% (4915/9152)\n",
      "Loss: 1.282 | Acc: 53.776% (4956/9216)\n",
      "Loss: 1.281 | Acc: 53.772% (4990/9280)\n",
      "Loss: 1.281 | Acc: 53.692% (5017/9344)\n",
      "Loss: 1.281 | Acc: 53.656% (5048/9408)\n",
      "Loss: 1.283 | Acc: 53.600% (5077/9472)\n",
      "Loss: 1.283 | Acc: 53.649% (5116/9536)\n",
      "Loss: 1.284 | Acc: 53.646% (5150/9600)\n",
      "Loss: 1.285 | Acc: 53.642% (5184/9664)\n",
      "Loss: 1.286 | Acc: 53.618% (5216/9728)\n",
      "Loss: 1.287 | Acc: 53.574% (5246/9792)\n",
      "Loss: 1.287 | Acc: 53.561% (5279/9856)\n",
      "Loss: 1.288 | Acc: 53.508% (5308/9920)\n",
      "Loss: 1.288 | Acc: 53.496% (5341/9984)\n",
      "Loss: 1.290 | Acc: 53.440% (5344/10000)\n",
      "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 53.44\n",
      "\n",
      "Final train set accuracy is 50.77755102040816\n",
      "Final test set accuracy is 53.44\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "regularization_val = 1e-6\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims,input_dims, output_dims, num_trans_layers, num_heads, image_k, patch_k)\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate, weight_decay=regularization_val)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "test_accs=[]\n",
    "for epoch in range(3):\n",
    "    tr_acc = train(network, optimizer, loader_train)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    test_acc = evaluate(network, loader_test)\n",
    "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
    "              .format(epoch, test_acc))  \n",
    "    \n",
    "    tr_accs.append(tr_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
